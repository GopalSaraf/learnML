<!doctype html>
<html lang="en">
<head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <meta name="generator" content="pdoc 14.0.0"/>
    <title>learnML.classification API documentation</title>
            <link rel="icon" href="https://github-production-user-asset-6210df.s3.amazonaws.com/83419951/261991976-577f071d-b3d7-4a3c-8000-41e1c0275669.png"/>

    <style>/*! * Bootstrap Reboot v5.0.0 (https://getbootstrap.com/) * Copyright 2011-2021 The Bootstrap Authors * Copyright 2011-2021 Twitter, Inc. * Licensed under MIT (https://github.com/twbs/bootstrap/blob/main/LICENSE) * Forked from Normalize.css, licensed MIT (https://github.com/necolas/normalize.css/blob/master/LICENSE.md) */*,::after,::before{box-sizing:border-box}@media (prefers-reduced-motion:no-preference){:root{scroll-behavior:smooth}}body{margin:0;font-family:system-ui,-apple-system,"Segoe UI",Roboto,"Helvetica Neue",Arial,"Noto Sans","Liberation Sans",sans-serif,"Apple Color Emoji","Segoe UI Emoji","Segoe UI Symbol","Noto Color Emoji";font-size:1rem;font-weight:400;line-height:1.5;color:#212529;background-color:#fff;-webkit-text-size-adjust:100%;-webkit-tap-highlight-color:transparent}hr{margin:1rem 0;color:inherit;background-color:currentColor;border:0;opacity:.25}hr:not([size]){height:1px}h1,h2,h3,h4,h5,h6{margin-top:0;margin-bottom:.5rem;font-weight:500;line-height:1.2}h1{font-size:calc(1.375rem + 1.5vw)}@media (min-width:1200px){h1{font-size:2.5rem}}h2{font-size:calc(1.325rem + .9vw)}@media (min-width:1200px){h2{font-size:2rem}}h3{font-size:calc(1.3rem + .6vw)}@media (min-width:1200px){h3{font-size:1.75rem}}h4{font-size:calc(1.275rem + .3vw)}@media (min-width:1200px){h4{font-size:1.5rem}}h5{font-size:1.25rem}h6{font-size:1rem}p{margin-top:0;margin-bottom:1rem}abbr[data-bs-original-title],abbr[title]{-webkit-text-decoration:underline dotted;text-decoration:underline dotted;cursor:help;-webkit-text-decoration-skip-ink:none;text-decoration-skip-ink:none}address{margin-bottom:1rem;font-style:normal;line-height:inherit}ol,ul{padding-left:2rem}dl,ol,ul{margin-top:0;margin-bottom:1rem}ol ol,ol ul,ul ol,ul ul{margin-bottom:0}dt{font-weight:700}dd{margin-bottom:.5rem;margin-left:0}blockquote{margin:0 0 1rem}b,strong{font-weight:bolder}small{font-size:.875em}mark{padding:.2em;background-color:#fcf8e3}sub,sup{position:relative;font-size:.75em;line-height:0;vertical-align:baseline}sub{bottom:-.25em}sup{top:-.5em}a{color:#0d6efd;text-decoration:underline}a:hover{color:#0a58ca}a:not([href]):not([class]),a:not([href]):not([class]):hover{color:inherit;text-decoration:none}code,kbd,pre,samp{font-family:SFMono-Regular,Menlo,Monaco,Consolas,"Liberation Mono","Courier New",monospace;font-size:1em;direction:ltr;unicode-bidi:bidi-override}pre{display:block;margin-top:0;margin-bottom:1rem;overflow:auto;font-size:.875em}pre code{font-size:inherit;color:inherit;word-break:normal}code{font-size:.875em;color:#d63384;word-wrap:break-word}a>code{color:inherit}kbd{padding:.2rem .4rem;font-size:.875em;color:#fff;background-color:#212529;border-radius:.2rem}kbd kbd{padding:0;font-size:1em;font-weight:700}figure{margin:0 0 1rem}img,svg{vertical-align:middle}table{caption-side:bottom;border-collapse:collapse}caption{padding-top:.5rem;padding-bottom:.5rem;color:#6c757d;text-align:left}th{text-align:inherit;text-align:-webkit-match-parent}tbody,td,tfoot,th,thead,tr{border-color:inherit;border-style:solid;border-width:0}label{display:inline-block}button{border-radius:0}button:focus:not(:focus-visible){outline:0}button,input,optgroup,select,textarea{margin:0;font-family:inherit;font-size:inherit;line-height:inherit}button,select{text-transform:none}[role=button]{cursor:pointer}select{word-wrap:normal}select:disabled{opacity:1}[list]::-webkit-calendar-picker-indicator{display:none}[type=button],[type=reset],[type=submit],button{-webkit-appearance:button}[type=button]:not(:disabled),[type=reset]:not(:disabled),[type=submit]:not(:disabled),button:not(:disabled){cursor:pointer}::-moz-focus-inner{padding:0;border-style:none}textarea{resize:vertical}fieldset{min-width:0;padding:0;margin:0;border:0}legend{float:left;width:100%;padding:0;margin-bottom:.5rem;font-size:calc(1.275rem + .3vw);line-height:inherit}@media (min-width:1200px){legend{font-size:1.5rem}}legend+*{clear:left}::-webkit-datetime-edit-day-field,::-webkit-datetime-edit-fields-wrapper,::-webkit-datetime-edit-hour-field,::-webkit-datetime-edit-minute,::-webkit-datetime-edit-month-field,::-webkit-datetime-edit-text,::-webkit-datetime-edit-year-field{padding:0}::-webkit-inner-spin-button{height:auto}[type=search]{outline-offset:-2px;-webkit-appearance:textfield}::-webkit-search-decoration{-webkit-appearance:none}::-webkit-color-swatch-wrapper{padding:0}::file-selector-button{font:inherit}::-webkit-file-upload-button{font:inherit;-webkit-appearance:button}output{display:inline-block}iframe{border:0}summary{display:list-item;cursor:pointer}progress{vertical-align:baseline}[hidden]{display:none!important}</style>
    <style>/*! syntax-highlighting.css */pre{line-height:125%;}span.linenos{color:inherit; background-color:transparent; padding-left:5px; padding-right:20px;}.pdoc-code .hll{background-color:#ffffcc}.pdoc-code{background:#f8f8f8;}.pdoc-code .c{color:#3D7B7B; font-style:italic}.pdoc-code .err{border:1px solid #FF0000}.pdoc-code .k{color:#008000; font-weight:bold}.pdoc-code .o{color:#666666}.pdoc-code .ch{color:#3D7B7B; font-style:italic}.pdoc-code .cm{color:#3D7B7B; font-style:italic}.pdoc-code .cp{color:#9C6500}.pdoc-code .cpf{color:#3D7B7B; font-style:italic}.pdoc-code .c1{color:#3D7B7B; font-style:italic}.pdoc-code .cs{color:#3D7B7B; font-style:italic}.pdoc-code .gd{color:#A00000}.pdoc-code .ge{font-style:italic}.pdoc-code .gr{color:#E40000}.pdoc-code .gh{color:#000080; font-weight:bold}.pdoc-code .gi{color:#008400}.pdoc-code .go{color:#717171}.pdoc-code .gp{color:#000080; font-weight:bold}.pdoc-code .gs{font-weight:bold}.pdoc-code .gu{color:#800080; font-weight:bold}.pdoc-code .gt{color:#0044DD}.pdoc-code .kc{color:#008000; font-weight:bold}.pdoc-code .kd{color:#008000; font-weight:bold}.pdoc-code .kn{color:#008000; font-weight:bold}.pdoc-code .kp{color:#008000}.pdoc-code .kr{color:#008000; font-weight:bold}.pdoc-code .kt{color:#B00040}.pdoc-code .m{color:#666666}.pdoc-code .s{color:#BA2121}.pdoc-code .na{color:#687822}.pdoc-code .nb{color:#008000}.pdoc-code .nc{color:#0000FF; font-weight:bold}.pdoc-code .no{color:#880000}.pdoc-code .nd{color:#AA22FF}.pdoc-code .ni{color:#717171; font-weight:bold}.pdoc-code .ne{color:#CB3F38; font-weight:bold}.pdoc-code .nf{color:#0000FF}.pdoc-code .nl{color:#767600}.pdoc-code .nn{color:#0000FF; font-weight:bold}.pdoc-code .nt{color:#008000; font-weight:bold}.pdoc-code .nv{color:#19177C}.pdoc-code .ow{color:#AA22FF; font-weight:bold}.pdoc-code .w{color:#bbbbbb}.pdoc-code .mb{color:#666666}.pdoc-code .mf{color:#666666}.pdoc-code .mh{color:#666666}.pdoc-code .mi{color:#666666}.pdoc-code .mo{color:#666666}.pdoc-code .sa{color:#BA2121}.pdoc-code .sb{color:#BA2121}.pdoc-code .sc{color:#BA2121}.pdoc-code .dl{color:#BA2121}.pdoc-code .sd{color:#BA2121; font-style:italic}.pdoc-code .s2{color:#BA2121}.pdoc-code .se{color:#AA5D1F; font-weight:bold}.pdoc-code .sh{color:#BA2121}.pdoc-code .si{color:#A45A77; font-weight:bold}.pdoc-code .sx{color:#008000}.pdoc-code .sr{color:#A45A77}.pdoc-code .s1{color:#BA2121}.pdoc-code .ss{color:#19177C}.pdoc-code .bp{color:#008000}.pdoc-code .fm{color:#0000FF}.pdoc-code .vc{color:#19177C}.pdoc-code .vg{color:#19177C}.pdoc-code .vi{color:#19177C}.pdoc-code .vm{color:#19177C}.pdoc-code .il{color:#666666}</style>
    <style>/*! theme.css */:root{--pdoc-background:#fff;}.pdoc{--text:#212529;--muted:#6c757d;--link:#3660a5;--link-hover:#1659c5;--code:#f8f8f8;--active:#fff598;--accent:#eee;--accent2:#c1c1c1;--nav-hover:rgba(255, 255, 255, 0.5);--name:#0066BB;--def:#008800;--annotation:#007020;}</style>
    <style>/*! layout.css */html, body{width:100%;height:100%;}html, main{scroll-behavior:smooth;}body{background-color:var(--pdoc-background);}@media (max-width:769px){#navtoggle{cursor:pointer;position:absolute;width:50px;height:40px;top:1rem;right:1rem;border-color:var(--text);color:var(--text);display:flex;opacity:0.8;z-index:999;}#navtoggle:hover{opacity:1;}#togglestate + div{display:none;}#togglestate:checked + div{display:inherit;}main, header{padding:2rem 3vw;}header + main{margin-top:-3rem;}.git-button{display:none !important;}nav input[type="search"]{max-width:77%;}nav input[type="search"]:first-child{margin-top:-6px;}nav input[type="search"]:valid ~ *{display:none !important;}}@media (min-width:770px){:root{--sidebar-width:clamp(12.5rem, 28vw, 22rem);}nav{position:fixed;overflow:auto;height:100vh;width:var(--sidebar-width);}main, header{padding:3rem 2rem 3rem calc(var(--sidebar-width) + 3rem);width:calc(54rem + var(--sidebar-width));max-width:100%;}header + main{margin-top:-4rem;}#navtoggle{display:none;}}#togglestate{position:absolute;height:0;opacity:0;}nav.pdoc{--pad:clamp(0.5rem, 2vw, 1.75rem);--indent:1.5rem;background-color:var(--accent);border-right:1px solid var(--accent2);box-shadow:0 0 20px rgba(50, 50, 50, .2) inset;padding:0 0 0 var(--pad);overflow-wrap:anywhere;scrollbar-width:thin; scrollbar-color:var(--accent2) transparent }nav.pdoc::-webkit-scrollbar{width:.4rem; }nav.pdoc::-webkit-scrollbar-thumb{background-color:var(--accent2); }nav.pdoc > div{padding:var(--pad) 0;}nav.pdoc .module-list-button{display:inline-flex;align-items:center;color:var(--text);border-color:var(--muted);margin-bottom:1rem;}nav.pdoc .module-list-button:hover{border-color:var(--text);}nav.pdoc input[type=search]{display:block;outline-offset:0;width:calc(100% - var(--pad));}nav.pdoc .logo{max-width:calc(100% - var(--pad));max-height:35vh;display:block;margin:0 auto 1rem;transform:translate(calc(-.5 * var(--pad)), 0);}nav.pdoc ul{list-style:none;padding-left:0;}nav.pdoc > div > ul{margin-left:calc(0px - var(--pad));}nav.pdoc li a{padding:.2rem 0 .2rem calc(var(--pad) + var(--indent));}nav.pdoc > div > ul > li > a{padding-left:var(--pad);}nav.pdoc li{transition:all 100ms;}nav.pdoc li:hover{background-color:var(--nav-hover);}nav.pdoc a, nav.pdoc a:hover{color:var(--text);}nav.pdoc a{display:block;}nav.pdoc > h2:first-of-type{margin-top:1.5rem;}nav.pdoc .class:before{content:"class ";color:var(--muted);}nav.pdoc .function:after{content:"()";color:var(--muted);}nav.pdoc footer:before{content:"";display:block;width:calc(100% - var(--pad));border-top:solid var(--accent2) 1px;margin-top:1.5rem;padding-top:.5rem;}nav.pdoc footer{font-size:small;}</style>
    <style>/*! content.css */.pdoc{color:var(--text);box-sizing:border-box;line-height:1.5;background:none;}.pdoc .pdoc-button{cursor:pointer;display:inline-block;border:solid black 1px;border-radius:2px;font-size:.75rem;padding:calc(0.5em - 1px) 1em;transition:100ms all;}.pdoc .pdoc-alert{padding:1rem 1rem 1rem calc(1.5rem + 24px);border:1px solid transparent;border-radius:.25rem;background-repeat:no-repeat;background-position:1rem center;margin-bottom:1rem;}.pdoc .pdoc-alert > *:last-child{margin-bottom:0;}.pdoc .pdoc-alert-note {color:#084298;background-color:#cfe2ff;border-color:#b6d4fe;background-image:url("data:image/svg+xml,%3Csvg%20xmlns%3D%22http%3A//www.w3.org/2000/svg%22%20width%3D%2224%22%20height%3D%2224%22%20fill%3D%22%23084298%22%20viewBox%3D%220%200%2016%2016%22%3E%3Cpath%20d%3D%22M8%2016A8%208%200%201%200%208%200a8%208%200%200%200%200%2016zm.93-9.412-1%204.705c-.07.34.029.533.304.533.194%200%20.487-.07.686-.246l-.088.416c-.287.346-.92.598-1.465.598-.703%200-1.002-.422-.808-1.319l.738-3.468c.064-.293.006-.399-.287-.47l-.451-.081.082-.381%202.29-.287zM8%205.5a1%201%200%201%201%200-2%201%201%200%200%201%200%202z%22/%3E%3C/svg%3E");}.pdoc .pdoc-alert-warning{color:#664d03;background-color:#fff3cd;border-color:#ffecb5;background-image:url("data:image/svg+xml,%3Csvg%20xmlns%3D%22http%3A//www.w3.org/2000/svg%22%20width%3D%2224%22%20height%3D%2224%22%20fill%3D%22%23664d03%22%20viewBox%3D%220%200%2016%2016%22%3E%3Cpath%20d%3D%22M8.982%201.566a1.13%201.13%200%200%200-1.96%200L.165%2013.233c-.457.778.091%201.767.98%201.767h13.713c.889%200%201.438-.99.98-1.767L8.982%201.566zM8%205c.535%200%20.954.462.9.995l-.35%203.507a.552.552%200%200%201-1.1%200L7.1%205.995A.905.905%200%200%201%208%205zm.002%206a1%201%200%201%201%200%202%201%201%200%200%201%200-2z%22/%3E%3C/svg%3E");}.pdoc .pdoc-alert-danger{color:#842029;background-color:#f8d7da;border-color:#f5c2c7;background-image:url("data:image/svg+xml,%3Csvg%20xmlns%3D%22http%3A//www.w3.org/2000/svg%22%20width%3D%2224%22%20height%3D%2224%22%20fill%3D%22%23842029%22%20viewBox%3D%220%200%2016%2016%22%3E%3Cpath%20d%3D%22M5.52.359A.5.5%200%200%201%206%200h4a.5.5%200%200%201%20.474.658L8.694%206H12.5a.5.5%200%200%201%20.395.807l-7%209a.5.5%200%200%201-.873-.454L6.823%209.5H3.5a.5.5%200%200%201-.48-.641l2.5-8.5z%22/%3E%3C/svg%3E");}.pdoc .visually-hidden{position:absolute !important;width:1px !important;height:1px !important;padding:0 !important;margin:-1px !important;overflow:hidden !important;clip:rect(0, 0, 0, 0) !important;white-space:nowrap !important;border:0 !important;}.pdoc h1, .pdoc h2, .pdoc h3{font-weight:300;margin:.3em 0;padding:.2em 0;}.pdoc > section:not(.module-info) h1{font-size:1.5rem;font-weight:500;}.pdoc > section:not(.module-info) h2{font-size:1.4rem;font-weight:500;}.pdoc > section:not(.module-info) h3{font-size:1.3rem;font-weight:500;}.pdoc > section:not(.module-info) h4{font-size:1.2rem;}.pdoc > section:not(.module-info) h5{font-size:1.1rem;}.pdoc a{text-decoration:none;color:var(--link);}.pdoc a:hover{color:var(--link-hover);}.pdoc blockquote{margin-left:2rem;}.pdoc pre{border-top:1px solid var(--accent2);border-bottom:1px solid var(--accent2);margin-top:0;margin-bottom:1em;padding:.5rem 0 .5rem .5rem;overflow-x:auto;background-color:var(--code);}.pdoc code{color:var(--text);padding:.2em .4em;margin:0;font-size:85%;background-color:var(--code);border-radius:6px;}.pdoc a > code{color:inherit;}.pdoc pre > code{display:inline-block;font-size:inherit;background:none;border:none;padding:0;}.pdoc > section:not(.module-info){margin-bottom:1.5rem;}.pdoc .modulename{margin-top:0;font-weight:bold;}.pdoc .modulename a{color:var(--link);transition:100ms all;}.pdoc .git-button{float:right;border:solid var(--link) 1px;}.pdoc .git-button:hover{background-color:var(--link);color:var(--pdoc-background);}.view-source-toggle-state,.view-source-toggle-state ~ .pdoc-code{display:none;}.view-source-toggle-state:checked ~ .pdoc-code{display:block;}.view-source-button{display:inline-block;float:right;font-size:.75rem;line-height:1.5rem;color:var(--muted);padding:0 .4rem 0 1.3rem;cursor:pointer;text-indent:-2px;}.view-source-button > span{visibility:hidden;}.module-info .view-source-button{float:none;display:flex;justify-content:flex-end;margin:-1.2rem .4rem -.2rem 0;}.view-source-button::before{position:absolute;content:"View Source";display:list-item;list-style-type:disclosure-closed;}.view-source-toggle-state:checked ~ .attr .view-source-button::before,.view-source-toggle-state:checked ~ .view-source-button::before{list-style-type:disclosure-open;}.pdoc .docstring{margin-bottom:1.5rem;}.pdoc section:not(.module-info) .docstring{margin-left:clamp(0rem, 5vw - 2rem, 1rem);}.pdoc .docstring .pdoc-code{margin-left:1em;margin-right:1em;}.pdoc h1:target,.pdoc h2:target,.pdoc h3:target,.pdoc h4:target,.pdoc h5:target,.pdoc h6:target,.pdoc .pdoc-code > pre > span:target{background-color:var(--active);box-shadow:-1rem 0 0 0 var(--active);}.pdoc .pdoc-code > pre > span:target{display:block;}.pdoc div:target > .attr,.pdoc section:target > .attr,.pdoc dd:target > a{background-color:var(--active);}.pdoc *{scroll-margin:2rem;}.pdoc .pdoc-code .linenos{user-select:none;}.pdoc .attr:hover{filter:contrast(0.95);}.pdoc section, .pdoc .classattr{position:relative;}.pdoc .headerlink{--width:clamp(1rem, 3vw, 2rem);position:absolute;top:0;left:calc(0rem - var(--width));transition:all 100ms ease-in-out;opacity:0;}.pdoc .headerlink::before{content:"#";display:block;text-align:center;width:var(--width);height:2.3rem;line-height:2.3rem;font-size:1.5rem;}.pdoc .attr:hover ~ .headerlink,.pdoc *:target > .headerlink,.pdoc .headerlink:hover{opacity:1;}.pdoc .attr{display:block;margin:.5rem 0 .5rem;padding:.4rem .4rem .4rem 1rem;background-color:var(--accent);overflow-x:auto;}.pdoc .classattr{margin-left:2rem;}.pdoc .name{color:var(--name);font-weight:bold;}.pdoc .def{color:var(--def);font-weight:bold;}.pdoc .signature{background-color:transparent;}.pdoc .param, .pdoc .return-annotation{white-space:pre;}.pdoc .signature.multiline .param{display:block;}.pdoc .signature.condensed .param{display:inline-block;}.pdoc .annotation{color:var(--annotation);}.pdoc .view-value-toggle-state,.pdoc .view-value-toggle-state ~ .default_value{display:none;}.pdoc .view-value-toggle-state:checked ~ .default_value{display:inherit;}.pdoc .view-value-button{font-size:.5rem;vertical-align:middle;border-style:dashed;margin-top:-0.1rem;}.pdoc .view-value-button:hover{background:white;}.pdoc .view-value-button::before{content:"show";text-align:center;width:2.2em;display:inline-block;}.pdoc .view-value-toggle-state:checked ~ .view-value-button::before{content:"hide";}.pdoc .inherited{margin-left:2rem;}.pdoc .inherited dt{font-weight:700;}.pdoc .inherited dt, .pdoc .inherited dd{display:inline;margin-left:0;margin-bottom:.5rem;}.pdoc .inherited dd:not(:last-child):after{content:", ";}.pdoc .inherited .class:before{content:"class ";}.pdoc .inherited .function a:after{content:"()";}.pdoc .search-result .docstring{overflow:auto;max-height:25vh;}.pdoc .search-result.focused > .attr{background-color:var(--active);}.pdoc .attribution{margin-top:2rem;display:block;opacity:0.5;transition:all 200ms;filter:grayscale(100%);}.pdoc .attribution:hover{opacity:1;filter:grayscale(0%);}.pdoc .attribution img{margin-left:5px;height:35px;vertical-align:middle;width:70px;transition:all 200ms;}.pdoc table{display:block;width:max-content;max-width:100%;overflow:auto;margin-bottom:1rem;}.pdoc table th{font-weight:600;}.pdoc table th, .pdoc table td{padding:6px 13px;border:1px solid var(--accent2);}</style>
    <style>/*! custom.css */</style></head>
<body>
    <nav class="pdoc">
        <label id="navtoggle" for="togglestate" class="pdoc-button"><svg xmlns='http://www.w3.org/2000/svg' viewBox='0 0 30 30'><path stroke-linecap='round' stroke="currentColor" stroke-miterlimit='10' stroke-width='2' d='M4 7h22M4 15h22M4 23h22'/></svg></label>
        <input id="togglestate" type="checkbox" aria-hidden="true" tabindex="-1">
        <div>            <a class="pdoc-button module-list-button" href="../learnML.html">
<svg xmlns="http://www.w3.org/2000/svg" width="16" height="16" fill="currentColor" class="bi bi-box-arrow-in-left" viewBox="0 0 16 16">
  <path fill-rule="evenodd" d="M10 3.5a.5.5 0 0 0-.5-.5h-8a.5.5 0 0 0-.5.5v9a.5.5 0 0 0 .5.5h8a.5.5 0 0 0 .5-.5v-2a.5.5 0 0 1 1 0v2A1.5 1.5 0 0 1 9.5 14h-8A1.5 1.5 0 0 1 0 12.5v-9A1.5 1.5 0 0 1 1.5 2h8A1.5 1.5 0 0 1 11 3.5v2a.5.5 0 0 1-1 0v-2z"/>
  <path fill-rule="evenodd" d="M4.146 8.354a.5.5 0 0 1 0-.708l3-3a.5.5 0 1 1 .708.708L5.707 7.5H14.5a.5.5 0 0 1 0 1H5.707l2.147 2.146a.5.5 0 0 1-.708.708l-3-3z"/>
</svg>                &nbsp;learnML</a>

<a href="https://learnml.gopalsaraf.com/">            <img src="https://github.com/GopalSaraf/learnML/assets/83419951/d41ec63f-8cac-436c-8e92-8c86e3ede9c0" class="logo" alt="project logo"/>
</a>
            <input type="search" placeholder="Search..." role="searchbox" aria-label="search"
                   pattern=".+" required>



            <h2>API Documentation</h2>
                <ul class="memberlist">
            <li>
                    <a class="class" href="#LogisticRegression">LogisticRegression</a>
                            <ul class="memberlist">
                        <li>
                                <a class="function" href="#LogisticRegression.__init__">LogisticRegression</a>
                        </li>
                        <li>
                                <a class="function" href="#LogisticRegression.fit">fit</a>
                        </li>
                        <li>
                                <a class="function" href="#LogisticRegression.predict_proba">predict_proba</a>
                        </li>
                        <li>
                                <a class="function" href="#LogisticRegression.predict">predict</a>
                        </li>
                        <li>
                                <a class="function" href="#LogisticRegression.score">score</a>
                        </li>
                </ul>

            </li>
            <li>
                    <a class="class" href="#LinearSVC">LinearSVC</a>
                            <ul class="memberlist">
                        <li>
                                <a class="function" href="#LinearSVC.__init__">LinearSVC</a>
                        </li>
                        <li>
                                <a class="function" href="#LinearSVC.fit">fit</a>
                        </li>
                        <li>
                                <a class="function" href="#LinearSVC.predict">predict</a>
                        </li>
                        <li>
                                <a class="function" href="#LinearSVC.score">score</a>
                        </li>
                </ul>

            </li>
    </ul>



        <a class="attribution" title="pdoc: Python API documentation generator" href="https://pdoc.dev" target="_blank">
            built with <span class="visually-hidden">pdoc</span><img
                alt="pdoc logo"
                src="data:image/svg+xml,%3Csvg%20xmlns%3D%22http%3A//www.w3.org/2000/svg%22%20role%3D%22img%22%20aria-label%3D%22pdoc%20logo%22%20width%3D%22300%22%20height%3D%22150%22%20viewBox%3D%22-1%200%2060%2030%22%3E%3Ctitle%3Epdoc%3C/title%3E%3Cpath%20d%3D%22M29.621%2021.293c-.011-.273-.214-.475-.511-.481a.5.5%200%200%200-.489.503l-.044%201.393c-.097.551-.695%201.215-1.566%201.704-.577.428-1.306.486-2.193.182-1.426-.617-2.467-1.654-3.304-2.487l-.173-.172a3.43%203.43%200%200%200-.365-.306.49.49%200%200%200-.286-.196c-1.718-1.06-4.931-1.47-7.353.191l-.219.15c-1.707%201.187-3.413%202.131-4.328%201.03-.02-.027-.49-.685-.141-1.763.233-.721.546-2.408.772-4.076.042-.09.067-.187.046-.288.166-1.347.277-2.625.241-3.351%201.378-1.008%202.271-2.586%202.271-4.362%200-.976-.272-1.935-.788-2.774-.057-.094-.122-.18-.184-.268.033-.167.052-.339.052-.516%200-1.477-1.202-2.679-2.679-2.679-.791%200-1.496.352-1.987.9a6.3%206.3%200%200%200-1.001.029c-.492-.564-1.207-.929-2.012-.929-1.477%200-2.679%201.202-2.679%202.679A2.65%202.65%200%200%200%20.97%206.554c-.383.747-.595%201.572-.595%202.41%200%202.311%201.507%204.29%203.635%205.107-.037.699-.147%202.27-.423%203.294l-.137.461c-.622%202.042-2.515%208.257%201.727%2010.643%201.614.908%203.06%201.248%204.317%201.248%202.665%200%204.492-1.524%205.322-2.401%201.476-1.559%202.886-1.854%206.491.82%201.877%201.393%203.514%201.753%204.861%201.068%202.223-1.713%202.811-3.867%203.399-6.374.077-.846.056-1.469.054-1.537zm-4.835%204.313c-.054.305-.156.586-.242.629-.034-.007-.131-.022-.307-.157-.145-.111-.314-.478-.456-.908.221.121.432.25.675.355.115.039.219.051.33.081zm-2.251-1.238c-.05.33-.158.648-.252.694-.022.001-.125-.018-.307-.157-.217-.166-.488-.906-.639-1.573.358.344.754.693%201.198%201.036zm-3.887-2.337c-.006-.116-.018-.231-.041-.342.635.145%201.189.368%201.599.625.097.231.166.481.174.642-.03.049-.055.101-.067.158-.046.013-.128.026-.298.004-.278-.037-.901-.57-1.367-1.087zm-1.127-.497c.116.306.176.625.12.71-.019.014-.117.045-.345.016-.206-.027-.604-.332-.986-.695.41-.051.816-.056%201.211-.031zm-4.535%201.535c.209.22.379.47.358.598-.006.041-.088.138-.351.234-.144.055-.539-.063-.979-.259a11.66%2011.66%200%200%200%20.972-.573zm.983-.664c.359-.237.738-.418%201.126-.554.25.237.479.548.457.694-.006.042-.087.138-.351.235-.174.064-.694-.105-1.232-.375zm-3.381%201.794c-.022.145-.061.29-.149.401-.133.166-.358.248-.69.251h-.002c-.133%200-.306-.26-.45-.621.417.091.854.07%201.291-.031zm-2.066-8.077a4.78%204.78%200%200%201-.775-.584c.172-.115.505-.254.88-.378l-.105.962zm-.331%202.302a10.32%2010.32%200%200%201-.828-.502c.202-.143.576-.328.984-.49l-.156.992zm-.45%202.157l-.701-.403c.214-.115.536-.249.891-.376a11.57%2011.57%200%200%201-.19.779zm-.181%201.716c.064.398.194.702.298.893-.194-.051-.435-.162-.736-.398.061-.119.224-.3.438-.495zM8.87%204.141c0%20.152-.123.276-.276.276s-.275-.124-.275-.276.123-.276.276-.276.275.124.275.276zm-.735-.389a1.15%201.15%200%200%200-.314.783%201.16%201.16%200%200%200%201.162%201.162c.457%200%20.842-.27%201.032-.653.026.117.042.238.042.362a1.68%201.68%200%200%201-1.679%201.679%201.68%201.68%200%200%201-1.679-1.679c0-.843.626-1.535%201.436-1.654zM5.059%205.406A1.68%201.68%200%200%201%203.38%207.085a1.68%201.68%200%200%201-1.679-1.679c0-.037.009-.072.011-.109.21.3.541.508.935.508a1.16%201.16%200%200%200%201.162-1.162%201.14%201.14%200%200%200-.474-.912c.015%200%20.03-.005.045-.005.926.001%201.679.754%201.679%201.68zM3.198%204.141c0%20.152-.123.276-.276.276s-.275-.124-.275-.276.123-.276.276-.276.275.124.275.276zM1.375%208.964c0-.52.103-1.035.288-1.52.466.394%201.06.64%201.717.64%201.144%200%202.116-.725%202.499-1.738.383%201.012%201.355%201.738%202.499%201.738.867%200%201.631-.421%202.121-1.062.307.605.478%201.267.478%201.942%200%202.486-2.153%204.51-4.801%204.51s-4.801-2.023-4.801-4.51zm24.342%2019.349c-.985.498-2.267.168-3.813-.979-3.073-2.281-5.453-3.199-7.813-.705-1.315%201.391-4.163%203.365-8.423.97-3.174-1.786-2.239-6.266-1.261-9.479l.146-.492c.276-1.02.395-2.457.444-3.268a6.11%206.11%200%200%200%201.18.115%206.01%206.01%200%200%200%202.536-.562l-.006.175c-.802.215-1.848.612-2.021%201.25-.079.295.021.601.274.837.219.203.415.364.598.501-.667.304-1.243.698-1.311%201.179-.02.144-.022.507.393.787.213.144.395.26.564.365-1.285.521-1.361.96-1.381%201.126-.018.142-.011.496.427.746l.854.489c-.473.389-.971.914-.999%201.429-.018.278.095.532.316.713.675.556%201.231.721%201.653.721.059%200%20.104-.014.158-.02.207.707.641%201.64%201.513%201.64h.013c.8-.008%201.236-.345%201.462-.626.173-.216.268-.457.325-.692.424.195.93.374%201.372.374.151%200%20.294-.021.423-.068.732-.27.944-.704.993-1.021.009-.061.003-.119.002-.179.266.086.538.147.789.147.15%200%20.294-.021.423-.069.542-.2.797-.489.914-.754.237.147.478.258.704.288.106.014.205.021.296.021.356%200%20.595-.101.767-.229.438.435%201.094.992%201.656%201.067.106.014.205.021.296.021a1.56%201.56%200%200%200%20.323-.035c.17.575.453%201.289.866%201.605.358.273.665.362.914.362a.99.99%200%200%200%20.421-.093%201.03%201.03%200%200%200%20.245-.164c.168.428.39.846.68%201.068.358.273.665.362.913.362a.99.99%200%200%200%20.421-.093c.317-.148.512-.448.639-.762.251.157.495.257.726.257.127%200%20.25-.024.37-.071.427-.17.706-.617.841-1.314.022-.015.047-.022.068-.038.067-.051.133-.104.196-.159-.443%201.486-1.107%202.761-2.086%203.257zM8.66%209.925a.5.5%200%201%200-1%200c0%20.653-.818%201.205-1.787%201.205s-1.787-.552-1.787-1.205a.5.5%200%201%200-1%200c0%201.216%201.25%202.205%202.787%202.205s2.787-.989%202.787-2.205zm4.4%2015.965l-.208.097c-2.661%201.258-4.708%201.436-6.086.527-1.542-1.017-1.88-3.19-1.844-4.198a.4.4%200%200%200-.385-.414c-.242-.029-.406.164-.414.385-.046%201.249.367%203.686%202.202%204.896.708.467%201.547.7%202.51.7%201.248%200%202.706-.392%204.362-1.174l.185-.086a.4.4%200%200%200%20.205-.527c-.089-.204-.326-.291-.527-.206zM9.547%202.292c.093.077.205.114.317.114a.5.5%200%200%200%20.318-.886L8.817.397a.5.5%200%200%200-.703.068.5.5%200%200%200%20.069.703l1.364%201.124zm-7.661-.065c.086%200%20.173-.022.253-.068l1.523-.893a.5.5%200%200%200-.506-.863l-1.523.892a.5.5%200%200%200-.179.685c.094.158.261.247.432.247z%22%20transform%3D%22matrix%28-1%200%200%201%2058%200%29%22%20fill%3D%22%233bb300%22/%3E%3Cpath%20d%3D%22M.3%2021.86V10.18q0-.46.02-.68.04-.22.18-.5.28-.54%201.34-.54%201.06%200%201.42.28.38.26.44.78.76-1.04%202.38-1.04%201.64%200%203.1%201.54%201.46%201.54%201.46%203.58%200%202.04-1.46%203.58-1.44%201.54-3.08%201.54-1.64%200-2.38-.92v4.04q0%20.46-.04.68-.02.22-.18.5-.14.3-.5.42-.36.12-.98.12-.62%200-1-.12-.36-.12-.52-.4-.14-.28-.18-.5-.02-.22-.02-.68zm3.96-9.42q-.46.54-.46%201.18%200%20.64.46%201.18.48.52%201.2.52.74%200%201.24-.52.52-.52.52-1.18%200-.66-.48-1.18-.48-.54-1.26-.54-.76%200-1.22.54zm14.741-8.36q.16-.3.54-.42.38-.12%201-.12.64%200%201.02.12.38.12.52.42.16.3.18.54.04.22.04.68v11.94q0%20.46-.04.7-.02.22-.18.5-.3.54-1.7.54-1.38%200-1.54-.98-.84.96-2.34.96-1.8%200-3.28-1.56-1.48-1.58-1.48-3.66%200-2.1%201.48-3.68%201.5-1.58%203.28-1.58%201.48%200%202.3%201v-4.2q0-.46.02-.68.04-.24.18-.52zm-3.24%2010.86q.52.54%201.26.54.74%200%201.22-.54.5-.54.5-1.18%200-.66-.48-1.22-.46-.56-1.26-.56-.8%200-1.28.56-.48.54-.48%201.2%200%20.66.52%201.2zm7.833-1.2q0-2.4%201.68-3.96%201.68-1.56%203.84-1.56%202.16%200%203.82%201.56%201.66%201.54%201.66%203.94%200%201.66-.86%202.96-.86%201.28-2.1%201.9-1.22.6-2.54.6-1.32%200-2.56-.64-1.24-.66-2.1-1.92-.84-1.28-.84-2.88zm4.18%201.44q.64.48%201.3.48.66%200%201.32-.5.66-.5.66-1.48%200-.98-.62-1.46-.62-.48-1.34-.48-.72%200-1.34.5-.62.5-.62%201.48%200%20.96.64%201.46zm11.412-1.44q0%20.84.56%201.32.56.46%201.18.46.64%200%201.18-.36.56-.38.9-.38.6%200%201.46%201.06.46.58.46%201.04%200%20.76-1.1%201.42-1.14.8-2.8.8-1.86%200-3.58-1.34-.82-.64-1.34-1.7-.52-1.08-.52-2.36%200-1.3.52-2.34.52-1.06%201.34-1.7%201.66-1.32%203.54-1.32.76%200%201.48.22.72.2%201.06.4l.32.2q.36.24.56.38.52.4.52.92%200%20.5-.42%201.14-.72%201.1-1.38%201.1-.38%200-1.08-.44-.36-.34-1.04-.34-.66%200-1.24.48-.58.48-.58%201.34z%22%20fill%3D%22green%22/%3E%3C/svg%3E"/>
        </a>
</div>
    </nav>
    <main class="pdoc">
            <section class="module-info">
                    <h1 class="modulename">
<a href="./../learnML.html">learnML</a><wbr>.classification    </h1>

                
                        <input id="mod-classification-view-source" class="view-source-toggle-state" type="checkbox" aria-hidden="true" tabindex="-1">

                        <label class="view-source-button" for="mod-classification-view-source"><span>View Source</span></label>

                        <div class="pdoc-code codehilite"><pre><span></span><span id="L-1"><a href="#L-1"><span class="linenos">1</span></a><span class="kn">from</span> <span class="nn">.logistic_regression</span> <span class="kn">import</span> <span class="n">LogisticRegression</span>
</span><span id="L-2"><a href="#L-2"><span class="linenos">2</span></a><span class="kn">from</span> <span class="nn">.linear_svm</span> <span class="kn">import</span> <span class="n">LinearSVC</span>
</span><span id="L-3"><a href="#L-3"><span class="linenos">3</span></a>
</span><span id="L-4"><a href="#L-4"><span class="linenos">4</span></a><span class="n">__all__</span> <span class="o">=</span> <span class="p">[</span>
</span><span id="L-5"><a href="#L-5"><span class="linenos">5</span></a>    <span class="s2">&quot;LogisticRegression&quot;</span><span class="p">,</span>
</span><span id="L-6"><a href="#L-6"><span class="linenos">6</span></a>    <span class="s2">&quot;LinearSVC&quot;</span><span class="p">,</span>
</span><span id="L-7"><a href="#L-7"><span class="linenos">7</span></a><span class="p">]</span>
</span></pre></div>


            </section>
                <section id="LogisticRegression">
                            <input id="LogisticRegression-view-source" class="view-source-toggle-state" type="checkbox" aria-hidden="true" tabindex="-1">
<div class="attr class">
            
    <span class="def">class</span>
    <span class="name">LogisticRegression</span><wbr>(<span class="base">learnML.interfaces.iregression.IRegression</span>):

                <label class="view-source-button" for="LogisticRegression-view-source"><span>View Source</span></label>

    </div>
    <a class="headerlink" href="#LogisticRegression"></a>
            <div class="pdoc-code codehilite"><pre><span></span><span id="LogisticRegression-9"><a href="#LogisticRegression-9"><span class="linenos">  9</span></a><span class="k">class</span> <span class="nc">LogisticRegression</span><span class="p">(</span><span class="n">IRegression</span><span class="p">):</span>
</span><span id="LogisticRegression-10"><a href="#LogisticRegression-10"><span class="linenos"> 10</span></a><span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
</span><span id="LogisticRegression-11"><a href="#LogisticRegression-11"><span class="linenos"> 11</span></a><span class="sd">    # Logistic Regression Model</span>
</span><span id="LogisticRegression-12"><a href="#LogisticRegression-12"><span class="linenos"> 12</span></a>
</span><span id="LogisticRegression-13"><a href="#LogisticRegression-13"><span class="linenos"> 13</span></a><span class="sd">    Logistic Regression is a fundamental classification algorithm used to model the probability of a binary outcome. It&#39;s widely employed in machine learning for binary classification tasks and offers insights into the relationship between input features and class probabilities.</span>
</span><span id="LogisticRegression-14"><a href="#LogisticRegression-14"><span class="linenos"> 14</span></a>
</span><span id="LogisticRegression-15"><a href="#LogisticRegression-15"><span class="linenos"> 15</span></a><span class="sd">    ---</span>
</span><span id="LogisticRegression-16"><a href="#LogisticRegression-16"><span class="linenos"> 16</span></a>
</span><span id="LogisticRegression-17"><a href="#LogisticRegression-17"><span class="linenos"> 17</span></a><span class="sd">    ## Mathematical Approach</span>
</span><span id="LogisticRegression-18"><a href="#LogisticRegression-18"><span class="linenos"> 18</span></a>
</span><span id="LogisticRegression-19"><a href="#LogisticRegression-19"><span class="linenos"> 19</span></a><span class="sd">    Logistic Regression aims to predict the probability of a binary outcome by modeling it as a sigmoid function of a linear combination of input features. The equation takes the form:</span>
</span><span id="LogisticRegression-20"><a href="#LogisticRegression-20"><span class="linenos"> 20</span></a>
</span><span id="LogisticRegression-21"><a href="#LogisticRegression-21"><span class="linenos"> 21</span></a><span class="sd">    ```</span>
</span><span id="LogisticRegression-22"><a href="#LogisticRegression-22"><span class="linenos"> 22</span></a><span class="sd">    P(y=1 | X) = 1 / (1 + e^(-z))</span>
</span><span id="LogisticRegression-23"><a href="#LogisticRegression-23"><span class="linenos"> 23</span></a><span class="sd">    ```</span>
</span><span id="LogisticRegression-24"><a href="#LogisticRegression-24"><span class="linenos"> 24</span></a>
</span><span id="LogisticRegression-25"><a href="#LogisticRegression-25"><span class="linenos"> 25</span></a><span class="sd">    Where `P(y=1 | X)` is the probability of the positive class given input `X`, and `z` is the linear combination of input features, weights, and an intercept.</span>
</span><span id="LogisticRegression-26"><a href="#LogisticRegression-26"><span class="linenos"> 26</span></a>
</span><span id="LogisticRegression-27"><a href="#LogisticRegression-27"><span class="linenos"> 27</span></a><span class="sd">    ---</span>
</span><span id="LogisticRegression-28"><a href="#LogisticRegression-28"><span class="linenos"> 28</span></a>
</span><span id="LogisticRegression-29"><a href="#LogisticRegression-29"><span class="linenos"> 29</span></a><span class="sd">    ## Usage</span>
</span><span id="LogisticRegression-30"><a href="#LogisticRegression-30"><span class="linenos"> 30</span></a>
</span><span id="LogisticRegression-31"><a href="#LogisticRegression-31"><span class="linenos"> 31</span></a><span class="sd">    To use the Logistic Regression model, follow these steps:</span>
</span><span id="LogisticRegression-32"><a href="#LogisticRegression-32"><span class="linenos"> 32</span></a>
</span><span id="LogisticRegression-33"><a href="#LogisticRegression-33"><span class="linenos"> 33</span></a><span class="sd">    1. Import the `LogisticRegression` class from the appropriate module.</span>
</span><span id="LogisticRegression-34"><a href="#LogisticRegression-34"><span class="linenos"> 34</span></a><span class="sd">    2. Create an instance of the `LogisticRegression` class, specifying hyperparameters.</span>
</span><span id="LogisticRegression-35"><a href="#LogisticRegression-35"><span class="linenos"> 35</span></a><span class="sd">    3. Fit the model to your training data using the `fit` method.</span>
</span><span id="LogisticRegression-36"><a href="#LogisticRegression-36"><span class="linenos"> 36</span></a><span class="sd">    4. Make predictions on new data using the `predict` method.</span>
</span><span id="LogisticRegression-37"><a href="#LogisticRegression-37"><span class="linenos"> 37</span></a><span class="sd">    5. Evaluate the model&#39;s performance using the `score` method.</span>
</span><span id="LogisticRegression-38"><a href="#LogisticRegression-38"><span class="linenos"> 38</span></a>
</span><span id="LogisticRegression-39"><a href="#LogisticRegression-39"><span class="linenos"> 39</span></a><span class="sd">    ```python</span>
</span><span id="LogisticRegression-40"><a href="#LogisticRegression-40"><span class="linenos"> 40</span></a><span class="sd">    from learnML.classification import LogisticRegression</span>
</span><span id="LogisticRegression-41"><a href="#LogisticRegression-41"><span class="linenos"> 41</span></a>
</span><span id="LogisticRegression-42"><a href="#LogisticRegression-42"><span class="linenos"> 42</span></a><span class="sd">    # Create an instance of LogisticRegression</span>
</span><span id="LogisticRegression-43"><a href="#LogisticRegression-43"><span class="linenos"> 43</span></a><span class="sd">    model = LogisticRegression(learning_rate=0.001, n_iterations=1000)</span>
</span><span id="LogisticRegression-44"><a href="#LogisticRegression-44"><span class="linenos"> 44</span></a>
</span><span id="LogisticRegression-45"><a href="#LogisticRegression-45"><span class="linenos"> 45</span></a><span class="sd">    # Fit the model to training data</span>
</span><span id="LogisticRegression-46"><a href="#LogisticRegression-46"><span class="linenos"> 46</span></a><span class="sd">    model.fit(X_train, Y_train)</span>
</span><span id="LogisticRegression-47"><a href="#LogisticRegression-47"><span class="linenos"> 47</span></a>
</span><span id="LogisticRegression-48"><a href="#LogisticRegression-48"><span class="linenos"> 48</span></a><span class="sd">    # Make predictions on new data</span>
</span><span id="LogisticRegression-49"><a href="#LogisticRegression-49"><span class="linenos"> 49</span></a><span class="sd">    predictions = model.predict(X_test)</span>
</span><span id="LogisticRegression-50"><a href="#LogisticRegression-50"><span class="linenos"> 50</span></a>
</span><span id="LogisticRegression-51"><a href="#LogisticRegression-51"><span class="linenos"> 51</span></a><span class="sd">    # Calculate the model&#39;s score</span>
</span><span id="LogisticRegression-52"><a href="#LogisticRegression-52"><span class="linenos"> 52</span></a><span class="sd">    model_score = model.score(X_test, Y_test)</span>
</span><span id="LogisticRegression-53"><a href="#LogisticRegression-53"><span class="linenos"> 53</span></a><span class="sd">    ```</span>
</span><span id="LogisticRegression-54"><a href="#LogisticRegression-54"><span class="linenos"> 54</span></a>
</span><span id="LogisticRegression-55"><a href="#LogisticRegression-55"><span class="linenos"> 55</span></a><span class="sd">    ---</span>
</span><span id="LogisticRegression-56"><a href="#LogisticRegression-56"><span class="linenos"> 56</span></a>
</span><span id="LogisticRegression-57"><a href="#LogisticRegression-57"><span class="linenos"> 57</span></a><span class="sd">    ## Advantages</span>
</span><span id="LogisticRegression-58"><a href="#LogisticRegression-58"><span class="linenos"> 58</span></a>
</span><span id="LogisticRegression-59"><a href="#LogisticRegression-59"><span class="linenos"> 59</span></a><span class="sd">    - Simple and efficient</span>
</span><span id="LogisticRegression-60"><a href="#LogisticRegression-60"><span class="linenos"> 60</span></a><span class="sd">    - Can be updated easily with new data using stochastic gradient descent</span>
</span><span id="LogisticRegression-61"><a href="#LogisticRegression-61"><span class="linenos"> 61</span></a><span class="sd">    - Outputs have a nice probabilistic interpretation</span>
</span><span id="LogisticRegression-62"><a href="#LogisticRegression-62"><span class="linenos"> 62</span></a><span class="sd">    - Can be regularized to avoid overfitting</span>
</span><span id="LogisticRegression-63"><a href="#LogisticRegression-63"><span class="linenos"> 63</span></a><span class="sd">    - Works well with high dimensional data</span>
</span><span id="LogisticRegression-64"><a href="#LogisticRegression-64"><span class="linenos"> 64</span></a><span class="sd">    - Works well with sparse data</span>
</span><span id="LogisticRegression-65"><a href="#LogisticRegression-65"><span class="linenos"> 65</span></a>
</span><span id="LogisticRegression-66"><a href="#LogisticRegression-66"><span class="linenos"> 66</span></a><span class="sd">    ## Disadvantages</span>
</span><span id="LogisticRegression-67"><a href="#LogisticRegression-67"><span class="linenos"> 67</span></a>
</span><span id="LogisticRegression-68"><a href="#LogisticRegression-68"><span class="linenos"> 68</span></a><span class="sd">    - Not suitable for large number of features</span>
</span><span id="LogisticRegression-69"><a href="#LogisticRegression-69"><span class="linenos"> 69</span></a><span class="sd">    - Not suitable for non-linear problems</span>
</span><span id="LogisticRegression-70"><a href="#LogisticRegression-70"><span class="linenos"> 70</span></a>
</span><span id="LogisticRegression-71"><a href="#LogisticRegression-71"><span class="linenos"> 71</span></a><span class="sd">    ---</span>
</span><span id="LogisticRegression-72"><a href="#LogisticRegression-72"><span class="linenos"> 72</span></a><span class="sd">    &quot;&quot;&quot;</span>
</span><span id="LogisticRegression-73"><a href="#LogisticRegression-73"><span class="linenos"> 73</span></a>
</span><span id="LogisticRegression-74"><a href="#LogisticRegression-74"><span class="linenos"> 74</span></a>    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span>
</span><span id="LogisticRegression-75"><a href="#LogisticRegression-75"><span class="linenos"> 75</span></a>        <span class="bp">self</span><span class="p">,</span>
</span><span id="LogisticRegression-76"><a href="#LogisticRegression-76"><span class="linenos"> 76</span></a>        <span class="n">learning_rate</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">float64</span> <span class="o">=</span> <span class="mf">0.001</span><span class="p">,</span>
</span><span id="LogisticRegression-77"><a href="#LogisticRegression-77"><span class="linenos"> 77</span></a>        <span class="n">n_iterations</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">1000</span><span class="p">,</span>
</span><span id="LogisticRegression-78"><a href="#LogisticRegression-78"><span class="linenos"> 78</span></a>        <span class="n">lambda_</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">float64</span> <span class="o">=</span> <span class="mi">0</span><span class="p">,</span>
</span><span id="LogisticRegression-79"><a href="#LogisticRegression-79"><span class="linenos"> 79</span></a>        <span class="n">x_scalar</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="n">IFeatureEngineering</span><span class="p">,</span> <span class="n">List</span><span class="p">[</span><span class="n">IFeatureEngineering</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
</span><span id="LogisticRegression-80"><a href="#LogisticRegression-80"><span class="linenos"> 80</span></a>        <span class="n">debug</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
</span><span id="LogisticRegression-81"><a href="#LogisticRegression-81"><span class="linenos"> 81</span></a>        <span class="n">copy_x</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
</span><span id="LogisticRegression-82"><a href="#LogisticRegression-82"><span class="linenos"> 82</span></a>    <span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
</span><span id="LogisticRegression-83"><a href="#LogisticRegression-83"><span class="linenos"> 83</span></a><span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
</span><span id="LogisticRegression-84"><a href="#LogisticRegression-84"><span class="linenos"> 84</span></a><span class="sd">        Parameters</span>
</span><span id="LogisticRegression-85"><a href="#LogisticRegression-85"><span class="linenos"> 85</span></a><span class="sd">        ----------</span>
</span><span id="LogisticRegression-86"><a href="#LogisticRegression-86"><span class="linenos"> 86</span></a>
</span><span id="LogisticRegression-87"><a href="#LogisticRegression-87"><span class="linenos"> 87</span></a><span class="sd">        `learning_rate` : np.float64, optional</span>
</span><span id="LogisticRegression-88"><a href="#LogisticRegression-88"><span class="linenos"> 88</span></a><span class="sd">        - The learning rate, by default 0.001</span>
</span><span id="LogisticRegression-89"><a href="#LogisticRegression-89"><span class="linenos"> 89</span></a><span class="sd">        - The learning rate determines how much the weights are updated at each iteration</span>
</span><span id="LogisticRegression-90"><a href="#LogisticRegression-90"><span class="linenos"> 90</span></a><span class="sd">        - A low learning rate will take longer to converge, but a high learning rate may overshoot the optimal solution</span>
</span><span id="LogisticRegression-91"><a href="#LogisticRegression-91"><span class="linenos"> 91</span></a>
</span><span id="LogisticRegression-92"><a href="#LogisticRegression-92"><span class="linenos"> 92</span></a><span class="sd">        `n_iterations` : int, optional</span>
</span><span id="LogisticRegression-93"><a href="#LogisticRegression-93"><span class="linenos"> 93</span></a><span class="sd">        - The number of iterations, by default 1000</span>
</span><span id="LogisticRegression-94"><a href="#LogisticRegression-94"><span class="linenos"> 94</span></a><span class="sd">        - The number of iterations determines how many times the weights are updated</span>
</span><span id="LogisticRegression-95"><a href="#LogisticRegression-95"><span class="linenos"> 95</span></a><span class="sd">        - A higher number of iterations will take longer to converge, but a lower number of iterations may not be enough to converge</span>
</span><span id="LogisticRegression-96"><a href="#LogisticRegression-96"><span class="linenos"> 96</span></a>
</span><span id="LogisticRegression-97"><a href="#LogisticRegression-97"><span class="linenos"> 97</span></a><span class="sd">        `lambda_` : np.float64, optional</span>
</span><span id="LogisticRegression-98"><a href="#LogisticRegression-98"><span class="linenos"> 98</span></a><span class="sd">        - The regularization parameter, by default 0</span>
</span><span id="LogisticRegression-99"><a href="#LogisticRegression-99"><span class="linenos"> 99</span></a><span class="sd">        - The regularization parameter helps prevent overfitting by penalizing large weights</span>
</span><span id="LogisticRegression-100"><a href="#LogisticRegression-100"><span class="linenos">100</span></a><span class="sd">        - A higher regularization parameter will penalize large weights more, but a lower regularization parameter may not be enough to prevent overfitting</span>
</span><span id="LogisticRegression-101"><a href="#LogisticRegression-101"><span class="linenos">101</span></a>
</span><span id="LogisticRegression-102"><a href="#LogisticRegression-102"><span class="linenos">102</span></a><span class="sd">        `x_scalar` : Union[IFeatureEngineering, List[IFeatureEngineering]], optional</span>
</span><span id="LogisticRegression-103"><a href="#LogisticRegression-103"><span class="linenos">103</span></a><span class="sd">        - The feature engineering for the input data, by default None</span>
</span><span id="LogisticRegression-104"><a href="#LogisticRegression-104"><span class="linenos">104</span></a><span class="sd">        - If a list is provided, the feature engineering will be applied in the order provided</span>
</span><span id="LogisticRegression-105"><a href="#LogisticRegression-105"><span class="linenos">105</span></a><span class="sd">        - If a single feature engineering is provided, it will be applied to all input data</span>
</span><span id="LogisticRegression-106"><a href="#LogisticRegression-106"><span class="linenos">106</span></a>
</span><span id="LogisticRegression-107"><a href="#LogisticRegression-107"><span class="linenos">107</span></a><span class="sd">        `debug` : bool, optional</span>
</span><span id="LogisticRegression-108"><a href="#LogisticRegression-108"><span class="linenos">108</span></a><span class="sd">        - Whether to print debug messages, by default True</span>
</span><span id="LogisticRegression-109"><a href="#LogisticRegression-109"><span class="linenos">109</span></a><span class="sd">        - Debug messages include the cost at each iteration</span>
</span><span id="LogisticRegression-110"><a href="#LogisticRegression-110"><span class="linenos">110</span></a>
</span><span id="LogisticRegression-111"><a href="#LogisticRegression-111"><span class="linenos">111</span></a><span class="sd">        `copy_x` : bool, optional</span>
</span><span id="LogisticRegression-112"><a href="#LogisticRegression-112"><span class="linenos">112</span></a><span class="sd">        - Whether to copy the input array, by default True</span>
</span><span id="LogisticRegression-113"><a href="#LogisticRegression-113"><span class="linenos">113</span></a><span class="sd">        - If False, the input array will be overwritten</span>
</span><span id="LogisticRegression-114"><a href="#LogisticRegression-114"><span class="linenos">114</span></a>
</span><span id="LogisticRegression-115"><a href="#LogisticRegression-115"><span class="linenos">115</span></a><span class="sd">        ---</span>
</span><span id="LogisticRegression-116"><a href="#LogisticRegression-116"><span class="linenos">116</span></a><span class="sd">        &quot;&quot;&quot;</span>
</span><span id="LogisticRegression-117"><a href="#LogisticRegression-117"><span class="linenos">117</span></a>        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span>
</span><span id="LogisticRegression-118"><a href="#LogisticRegression-118"><span class="linenos">118</span></a>            <span class="n">learning_rate</span><span class="o">=</span><span class="n">learning_rate</span><span class="p">,</span>
</span><span id="LogisticRegression-119"><a href="#LogisticRegression-119"><span class="linenos">119</span></a>            <span class="n">n_iterations</span><span class="o">=</span><span class="n">n_iterations</span><span class="p">,</span>
</span><span id="LogisticRegression-120"><a href="#LogisticRegression-120"><span class="linenos">120</span></a>            <span class="n">debug</span><span class="o">=</span><span class="n">debug</span><span class="p">,</span>
</span><span id="LogisticRegression-121"><a href="#LogisticRegression-121"><span class="linenos">121</span></a>            <span class="n">copy_x</span><span class="o">=</span><span class="n">copy_x</span><span class="p">,</span>
</span><span id="LogisticRegression-122"><a href="#LogisticRegression-122"><span class="linenos">122</span></a>        <span class="p">)</span>
</span><span id="LogisticRegression-123"><a href="#LogisticRegression-123"><span class="linenos">123</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">_lambda</span> <span class="o">=</span> <span class="n">lambda_</span>
</span><span id="LogisticRegression-124"><a href="#LogisticRegression-124"><span class="linenos">124</span></a>
</span><span id="LogisticRegression-125"><a href="#LogisticRegression-125"><span class="linenos">125</span></a>        <span class="k">if</span> <span class="n">x_scalar</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
</span><span id="LogisticRegression-126"><a href="#LogisticRegression-126"><span class="linenos">126</span></a>            <span class="n">x_scalar</span> <span class="o">=</span> <span class="p">[]</span>
</span><span id="LogisticRegression-127"><a href="#LogisticRegression-127"><span class="linenos">127</span></a>        <span class="k">elif</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">x_scalar</span><span class="p">,</span> <span class="n">IFeatureEngineering</span><span class="p">):</span>
</span><span id="LogisticRegression-128"><a href="#LogisticRegression-128"><span class="linenos">128</span></a>            <span class="n">x_scalar</span> <span class="o">=</span> <span class="p">[</span><span class="n">x_scalar</span><span class="p">]</span>
</span><span id="LogisticRegression-129"><a href="#LogisticRegression-129"><span class="linenos">129</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">_x_scalar</span> <span class="o">=</span> <span class="n">x_scalar</span>
</span><span id="LogisticRegression-130"><a href="#LogisticRegression-130"><span class="linenos">130</span></a>
</span><span id="LogisticRegression-131"><a href="#LogisticRegression-131"><span class="linenos">131</span></a>    <span class="k">def</span> <span class="nf">_sigmoid</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">z</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">float64</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">np</span><span class="o">.</span><span class="n">float64</span><span class="p">:</span>
</span><span id="LogisticRegression-132"><a href="#LogisticRegression-132"><span class="linenos">132</span></a><span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
</span><span id="LogisticRegression-133"><a href="#LogisticRegression-133"><span class="linenos">133</span></a><span class="sd">        ### Sigmoid function</span>
</span><span id="LogisticRegression-134"><a href="#LogisticRegression-134"><span class="linenos">134</span></a>
</span><span id="LogisticRegression-135"><a href="#LogisticRegression-135"><span class="linenos">135</span></a><span class="sd">        Parameters</span>
</span><span id="LogisticRegression-136"><a href="#LogisticRegression-136"><span class="linenos">136</span></a><span class="sd">        ----------</span>
</span><span id="LogisticRegression-137"><a href="#LogisticRegression-137"><span class="linenos">137</span></a>
</span><span id="LogisticRegression-138"><a href="#LogisticRegression-138"><span class="linenos">138</span></a><span class="sd">        `z` : np.float64</span>
</span><span id="LogisticRegression-139"><a href="#LogisticRegression-139"><span class="linenos">139</span></a><span class="sd">        - The input</span>
</span><span id="LogisticRegression-140"><a href="#LogisticRegression-140"><span class="linenos">140</span></a>
</span><span id="LogisticRegression-141"><a href="#LogisticRegression-141"><span class="linenos">141</span></a><span class="sd">        Returns</span>
</span><span id="LogisticRegression-142"><a href="#LogisticRegression-142"><span class="linenos">142</span></a><span class="sd">        -------</span>
</span><span id="LogisticRegression-143"><a href="#LogisticRegression-143"><span class="linenos">143</span></a>
</span><span id="LogisticRegression-144"><a href="#LogisticRegression-144"><span class="linenos">144</span></a><span class="sd">        `np.float64`</span>
</span><span id="LogisticRegression-145"><a href="#LogisticRegression-145"><span class="linenos">145</span></a><span class="sd">        - The sigmoid of z</span>
</span><span id="LogisticRegression-146"><a href="#LogisticRegression-146"><span class="linenos">146</span></a>
</span><span id="LogisticRegression-147"><a href="#LogisticRegression-147"><span class="linenos">147</span></a><span class="sd">        ---</span>
</span><span id="LogisticRegression-148"><a href="#LogisticRegression-148"><span class="linenos">148</span></a><span class="sd">        &quot;&quot;&quot;</span>
</span><span id="LogisticRegression-149"><a href="#LogisticRegression-149"><span class="linenos">149</span></a>        <span class="c1"># 1 / (1 + e^(-z))</span>
</span><span id="LogisticRegression-150"><a href="#LogisticRegression-150"><span class="linenos">150</span></a>        <span class="k">return</span> <span class="mi">1</span> <span class="o">/</span> <span class="p">(</span><span class="mi">1</span> <span class="o">+</span> <span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="o">-</span><span class="n">z</span><span class="p">))</span>
</span><span id="LogisticRegression-151"><a href="#LogisticRegression-151"><span class="linenos">151</span></a>
</span><span id="LogisticRegression-152"><a href="#LogisticRegression-152"><span class="linenos">152</span></a>    <span class="k">def</span> <span class="nf">_y_hat</span><span class="p">(</span>
</span><span id="LogisticRegression-153"><a href="#LogisticRegression-153"><span class="linenos">153</span></a>        <span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">,</span> <span class="n">W</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">,</span> <span class="n">b</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">float64</span>
</span><span id="LogisticRegression-154"><a href="#LogisticRegression-154"><span class="linenos">154</span></a>    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Union</span><span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">float64</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">]:</span>
</span><span id="LogisticRegression-155"><a href="#LogisticRegression-155"><span class="linenos">155</span></a><span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
</span><span id="LogisticRegression-156"><a href="#LogisticRegression-156"><span class="linenos">156</span></a><span class="sd">        ### Return the predicted value of y given X, W, and b.</span>
</span><span id="LogisticRegression-157"><a href="#LogisticRegression-157"><span class="linenos">157</span></a>
</span><span id="LogisticRegression-158"><a href="#LogisticRegression-158"><span class="linenos">158</span></a><span class="sd">        Parameters</span>
</span><span id="LogisticRegression-159"><a href="#LogisticRegression-159"><span class="linenos">159</span></a><span class="sd">        ----------</span>
</span><span id="LogisticRegression-160"><a href="#LogisticRegression-160"><span class="linenos">160</span></a>
</span><span id="LogisticRegression-161"><a href="#LogisticRegression-161"><span class="linenos">161</span></a><span class="sd">        `X` : np.ndarray</span>
</span><span id="LogisticRegression-162"><a href="#LogisticRegression-162"><span class="linenos">162</span></a><span class="sd">        - The input array of shape (n_features,) or (n_samples, n_features)</span>
</span><span id="LogisticRegression-163"><a href="#LogisticRegression-163"><span class="linenos">163</span></a>
</span><span id="LogisticRegression-164"><a href="#LogisticRegression-164"><span class="linenos">164</span></a><span class="sd">        `W` : np.ndarray</span>
</span><span id="LogisticRegression-165"><a href="#LogisticRegression-165"><span class="linenos">165</span></a><span class="sd">        - The weight array of shape (n_features,)</span>
</span><span id="LogisticRegression-166"><a href="#LogisticRegression-166"><span class="linenos">166</span></a>
</span><span id="LogisticRegression-167"><a href="#LogisticRegression-167"><span class="linenos">167</span></a><span class="sd">        `b` : np.float64</span>
</span><span id="LogisticRegression-168"><a href="#LogisticRegression-168"><span class="linenos">168</span></a><span class="sd">        - The intercept value</span>
</span><span id="LogisticRegression-169"><a href="#LogisticRegression-169"><span class="linenos">169</span></a>
</span><span id="LogisticRegression-170"><a href="#LogisticRegression-170"><span class="linenos">170</span></a>
</span><span id="LogisticRegression-171"><a href="#LogisticRegression-171"><span class="linenos">171</span></a><span class="sd">        Returns</span>
</span><span id="LogisticRegression-172"><a href="#LogisticRegression-172"><span class="linenos">172</span></a><span class="sd">        -------</span>
</span><span id="LogisticRegression-173"><a href="#LogisticRegression-173"><span class="linenos">173</span></a>
</span><span id="LogisticRegression-174"><a href="#LogisticRegression-174"><span class="linenos">174</span></a><span class="sd">        `np.float64`</span>
</span><span id="LogisticRegression-175"><a href="#LogisticRegression-175"><span class="linenos">175</span></a><span class="sd">        - The predicted value of y</span>
</span><span id="LogisticRegression-176"><a href="#LogisticRegression-176"><span class="linenos">176</span></a><span class="sd">        - If X is a 1D array, return a scalar</span>
</span><span id="LogisticRegression-177"><a href="#LogisticRegression-177"><span class="linenos">177</span></a><span class="sd">        - If X is a 2D array, return an array of shape (n_samples,)</span>
</span><span id="LogisticRegression-178"><a href="#LogisticRegression-178"><span class="linenos">178</span></a>
</span><span id="LogisticRegression-179"><a href="#LogisticRegression-179"><span class="linenos">179</span></a><span class="sd">        ---</span>
</span><span id="LogisticRegression-180"><a href="#LogisticRegression-180"><span class="linenos">180</span></a><span class="sd">        &quot;&quot;&quot;</span>
</span><span id="LogisticRegression-181"><a href="#LogisticRegression-181"><span class="linenos">181</span></a>        <span class="n">z</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">W</span><span class="p">)</span> <span class="o">+</span> <span class="n">b</span>
</span><span id="LogisticRegression-182"><a href="#LogisticRegression-182"><span class="linenos">182</span></a>        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_sigmoid</span><span class="p">(</span><span class="n">z</span><span class="p">)</span>
</span><span id="LogisticRegression-183"><a href="#LogisticRegression-183"><span class="linenos">183</span></a>
</span><span id="LogisticRegression-184"><a href="#LogisticRegression-184"><span class="linenos">184</span></a>    <span class="k">def</span> <span class="nf">_cost</span><span class="p">(</span>
</span><span id="LogisticRegression-185"><a href="#LogisticRegression-185"><span class="linenos">185</span></a>        <span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">,</span> <span class="n">y</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">float64</span><span class="p">,</span> <span class="n">W</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">,</span> <span class="n">b</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">float64</span>
</span><span id="LogisticRegression-186"><a href="#LogisticRegression-186"><span class="linenos">186</span></a>    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">np</span><span class="o">.</span><span class="n">float64</span><span class="p">:</span>
</span><span id="LogisticRegression-187"><a href="#LogisticRegression-187"><span class="linenos">187</span></a><span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
</span><span id="LogisticRegression-188"><a href="#LogisticRegression-188"><span class="linenos">188</span></a><span class="sd">        ### Return the cost of the model given X, y, W, and b.</span>
</span><span id="LogisticRegression-189"><a href="#LogisticRegression-189"><span class="linenos">189</span></a><span class="sd">        (Cross-entropy loss)</span>
</span><span id="LogisticRegression-190"><a href="#LogisticRegression-190"><span class="linenos">190</span></a>
</span><span id="LogisticRegression-191"><a href="#LogisticRegression-191"><span class="linenos">191</span></a><span class="sd">        Parameters</span>
</span><span id="LogisticRegression-192"><a href="#LogisticRegression-192"><span class="linenos">192</span></a><span class="sd">        ----------</span>
</span><span id="LogisticRegression-193"><a href="#LogisticRegression-193"><span class="linenos">193</span></a>
</span><span id="LogisticRegression-194"><a href="#LogisticRegression-194"><span class="linenos">194</span></a><span class="sd">        `X` : np.ndarray</span>
</span><span id="LogisticRegression-195"><a href="#LogisticRegression-195"><span class="linenos">195</span></a><span class="sd">        - The input array of shape (n_samples, n_features)</span>
</span><span id="LogisticRegression-196"><a href="#LogisticRegression-196"><span class="linenos">196</span></a>
</span><span id="LogisticRegression-197"><a href="#LogisticRegression-197"><span class="linenos">197</span></a><span class="sd">        `y` : np.float64</span>
</span><span id="LogisticRegression-198"><a href="#LogisticRegression-198"><span class="linenos">198</span></a><span class="sd">        - The output array of shape (n_samples,)</span>
</span><span id="LogisticRegression-199"><a href="#LogisticRegression-199"><span class="linenos">199</span></a>
</span><span id="LogisticRegression-200"><a href="#LogisticRegression-200"><span class="linenos">200</span></a><span class="sd">        `W` : np.ndarray</span>
</span><span id="LogisticRegression-201"><a href="#LogisticRegression-201"><span class="linenos">201</span></a><span class="sd">        - The weight array of shape (n_features,)</span>
</span><span id="LogisticRegression-202"><a href="#LogisticRegression-202"><span class="linenos">202</span></a>
</span><span id="LogisticRegression-203"><a href="#LogisticRegression-203"><span class="linenos">203</span></a><span class="sd">        `b` : np.float64</span>
</span><span id="LogisticRegression-204"><a href="#LogisticRegression-204"><span class="linenos">204</span></a><span class="sd">        - The intercept value</span>
</span><span id="LogisticRegression-205"><a href="#LogisticRegression-205"><span class="linenos">205</span></a>
</span><span id="LogisticRegression-206"><a href="#LogisticRegression-206"><span class="linenos">206</span></a>
</span><span id="LogisticRegression-207"><a href="#LogisticRegression-207"><span class="linenos">207</span></a><span class="sd">        Returns</span>
</span><span id="LogisticRegression-208"><a href="#LogisticRegression-208"><span class="linenos">208</span></a><span class="sd">        -------</span>
</span><span id="LogisticRegression-209"><a href="#LogisticRegression-209"><span class="linenos">209</span></a>
</span><span id="LogisticRegression-210"><a href="#LogisticRegression-210"><span class="linenos">210</span></a><span class="sd">        `np.float64`</span>
</span><span id="LogisticRegression-211"><a href="#LogisticRegression-211"><span class="linenos">211</span></a><span class="sd">        - The cost of the model</span>
</span><span id="LogisticRegression-212"><a href="#LogisticRegression-212"><span class="linenos">212</span></a>
</span><span id="LogisticRegression-213"><a href="#LogisticRegression-213"><span class="linenos">213</span></a><span class="sd">        ---</span>
</span><span id="LogisticRegression-214"><a href="#LogisticRegression-214"><span class="linenos">214</span></a><span class="sd">        &quot;&quot;&quot;</span>
</span><span id="LogisticRegression-215"><a href="#LogisticRegression-215"><span class="linenos">215</span></a><span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
</span><span id="LogisticRegression-216"><a href="#LogisticRegression-216"><span class="linenos">216</span></a><span class="sd">        ALTERNATIVE IMPLEMENTATION</span>
</span><span id="LogisticRegression-217"><a href="#LogisticRegression-217"><span class="linenos">217</span></a><span class="sd">        --------------------------</span>
</span><span id="LogisticRegression-218"><a href="#LogisticRegression-218"><span class="linenos">218</span></a><span class="sd">        cost = 0.0</span>
</span><span id="LogisticRegression-219"><a href="#LogisticRegression-219"><span class="linenos">219</span></a><span class="sd">        m = x.shape[0]</span>
</span><span id="LogisticRegression-220"><a href="#LogisticRegression-220"><span class="linenos">220</span></a>
</span><span id="LogisticRegression-221"><a href="#LogisticRegression-221"><span class="linenos">221</span></a><span class="sd">        for i in range(m):</span>
</span><span id="LogisticRegression-222"><a href="#LogisticRegression-222"><span class="linenos">222</span></a><span class="sd">            y_hat = self._y_hat(x[i], w, b)</span>
</span><span id="LogisticRegression-223"><a href="#LogisticRegression-223"><span class="linenos">223</span></a><span class="sd">            pos_cost = np.dot(y[i], np.log(y_hat))</span>
</span><span id="LogisticRegression-224"><a href="#LogisticRegression-224"><span class="linenos">224</span></a><span class="sd">            neg_cost = np.dot(1 - y[i], np.log(1 - y_hat))</span>
</span><span id="LogisticRegression-225"><a href="#LogisticRegression-225"><span class="linenos">225</span></a><span class="sd">            cost += pos_cost + neg_cost</span>
</span><span id="LogisticRegression-226"><a href="#LogisticRegression-226"><span class="linenos">226</span></a>
</span><span id="LogisticRegression-227"><a href="#LogisticRegression-227"><span class="linenos">227</span></a><span class="sd">        return -cost / m</span>
</span><span id="LogisticRegression-228"><a href="#LogisticRegression-228"><span class="linenos">228</span></a><span class="sd">        &quot;&quot;&quot;</span>
</span><span id="LogisticRegression-229"><a href="#LogisticRegression-229"><span class="linenos">229</span></a>        <span class="n">y_hat</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_y_hat</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">W</span><span class="p">,</span> <span class="n">b</span><span class="p">)</span>
</span><span id="LogisticRegression-230"><a href="#LogisticRegression-230"><span class="linenos">230</span></a>
</span><span id="LogisticRegression-231"><a href="#LogisticRegression-231"><span class="linenos">231</span></a>        <span class="c1"># Cost of logistic regression : -[ylog(y_hat) + (1-y)log(1-y_hat)] / m</span>
</span><span id="LogisticRegression-232"><a href="#LogisticRegression-232"><span class="linenos">232</span></a>        <span class="n">pos_cost</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">y</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">y_hat</span><span class="p">))</span>
</span><span id="LogisticRegression-233"><a href="#LogisticRegression-233"><span class="linenos">233</span></a>        <span class="n">neg_cost</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">y</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">y_hat</span><span class="p">))</span>
</span><span id="LogisticRegression-234"><a href="#LogisticRegression-234"><span class="linenos">234</span></a>
</span><span id="LogisticRegression-235"><a href="#LogisticRegression-235"><span class="linenos">235</span></a>        <span class="k">return</span> <span class="o">-</span><span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">pos_cost</span> <span class="o">+</span> <span class="n">neg_cost</span><span class="p">)</span> <span class="o">/</span> <span class="n">X</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
</span><span id="LogisticRegression-236"><a href="#LogisticRegression-236"><span class="linenos">236</span></a>
</span><span id="LogisticRegression-237"><a href="#LogisticRegression-237"><span class="linenos">237</span></a>    <span class="k">def</span> <span class="nf">_gradient</span><span class="p">(</span>
</span><span id="LogisticRegression-238"><a href="#LogisticRegression-238"><span class="linenos">238</span></a>        <span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">,</span> <span class="n">Y</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">float64</span><span class="p">,</span> <span class="n">W</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">,</span> <span class="n">b</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">float64</span>
</span><span id="LogisticRegression-239"><a href="#LogisticRegression-239"><span class="linenos">239</span></a>    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Union</span><span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">float64</span><span class="p">]:</span>
</span><span id="LogisticRegression-240"><a href="#LogisticRegression-240"><span class="linenos">240</span></a><span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
</span><span id="LogisticRegression-241"><a href="#LogisticRegression-241"><span class="linenos">241</span></a><span class="sd">        ### Return the gradient of the model given X, y, W, and b.</span>
</span><span id="LogisticRegression-242"><a href="#LogisticRegression-242"><span class="linenos">242</span></a>
</span><span id="LogisticRegression-243"><a href="#LogisticRegression-243"><span class="linenos">243</span></a><span class="sd">        Parameters</span>
</span><span id="LogisticRegression-244"><a href="#LogisticRegression-244"><span class="linenos">244</span></a><span class="sd">        ----------</span>
</span><span id="LogisticRegression-245"><a href="#LogisticRegression-245"><span class="linenos">245</span></a>
</span><span id="LogisticRegression-246"><a href="#LogisticRegression-246"><span class="linenos">246</span></a><span class="sd">        `x` : np.ndarray</span>
</span><span id="LogisticRegression-247"><a href="#LogisticRegression-247"><span class="linenos">247</span></a><span class="sd">        - The input array of shape (n_samples, n_features)</span>
</span><span id="LogisticRegression-248"><a href="#LogisticRegression-248"><span class="linenos">248</span></a>
</span><span id="LogisticRegression-249"><a href="#LogisticRegression-249"><span class="linenos">249</span></a><span class="sd">        `y` : np.float64</span>
</span><span id="LogisticRegression-250"><a href="#LogisticRegression-250"><span class="linenos">250</span></a><span class="sd">        - The output array of shape (n_samples,)</span>
</span><span id="LogisticRegression-251"><a href="#LogisticRegression-251"><span class="linenos">251</span></a>
</span><span id="LogisticRegression-252"><a href="#LogisticRegression-252"><span class="linenos">252</span></a><span class="sd">        `w` : np.ndarray</span>
</span><span id="LogisticRegression-253"><a href="#LogisticRegression-253"><span class="linenos">253</span></a><span class="sd">        - The weight array of shape (n_features,)</span>
</span><span id="LogisticRegression-254"><a href="#LogisticRegression-254"><span class="linenos">254</span></a>
</span><span id="LogisticRegression-255"><a href="#LogisticRegression-255"><span class="linenos">255</span></a><span class="sd">        `b` : np.float64</span>
</span><span id="LogisticRegression-256"><a href="#LogisticRegression-256"><span class="linenos">256</span></a><span class="sd">        - The intercept value</span>
</span><span id="LogisticRegression-257"><a href="#LogisticRegression-257"><span class="linenos">257</span></a>
</span><span id="LogisticRegression-258"><a href="#LogisticRegression-258"><span class="linenos">258</span></a>
</span><span id="LogisticRegression-259"><a href="#LogisticRegression-259"><span class="linenos">259</span></a><span class="sd">        Returns</span>
</span><span id="LogisticRegression-260"><a href="#LogisticRegression-260"><span class="linenos">260</span></a><span class="sd">        -------</span>
</span><span id="LogisticRegression-261"><a href="#LogisticRegression-261"><span class="linenos">261</span></a>
</span><span id="LogisticRegression-262"><a href="#LogisticRegression-262"><span class="linenos">262</span></a><span class="sd">        `Union[np.ndarray, np.float64]`</span>
</span><span id="LogisticRegression-263"><a href="#LogisticRegression-263"><span class="linenos">263</span></a><span class="sd">        - The gradient of the model with respect to w and b</span>
</span><span id="LogisticRegression-264"><a href="#LogisticRegression-264"><span class="linenos">264</span></a>
</span><span id="LogisticRegression-265"><a href="#LogisticRegression-265"><span class="linenos">265</span></a><span class="sd">        ---</span>
</span><span id="LogisticRegression-266"><a href="#LogisticRegression-266"><span class="linenos">266</span></a><span class="sd">        &quot;&quot;&quot;</span>
</span><span id="LogisticRegression-267"><a href="#LogisticRegression-267"><span class="linenos">267</span></a><span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
</span><span id="LogisticRegression-268"><a href="#LogisticRegression-268"><span class="linenos">268</span></a><span class="sd">        ALTERNATIVE IMPLEMENTATION</span>
</span><span id="LogisticRegression-269"><a href="#LogisticRegression-269"><span class="linenos">269</span></a><span class="sd">        --------------------------</span>
</span><span id="LogisticRegression-270"><a href="#LogisticRegression-270"><span class="linenos">270</span></a><span class="sd">        dw = np.zeros(w.shape)</span>
</span><span id="LogisticRegression-271"><a href="#LogisticRegression-271"><span class="linenos">271</span></a><span class="sd">        db = 0.0</span>
</span><span id="LogisticRegression-272"><a href="#LogisticRegression-272"><span class="linenos">272</span></a><span class="sd">        m = x.shape[0]</span>
</span><span id="LogisticRegression-273"><a href="#LogisticRegression-273"><span class="linenos">273</span></a>
</span><span id="LogisticRegression-274"><a href="#LogisticRegression-274"><span class="linenos">274</span></a><span class="sd">        for i in range(m):</span>
</span><span id="LogisticRegression-275"><a href="#LogisticRegression-275"><span class="linenos">275</span></a><span class="sd">            y_hat = self._y_hat(x[i], w, b)</span>
</span><span id="LogisticRegression-276"><a href="#LogisticRegression-276"><span class="linenos">276</span></a><span class="sd">            dw += np.dot(x[i].T, y_hat - y[i])</span>
</span><span id="LogisticRegression-277"><a href="#LogisticRegression-277"><span class="linenos">277</span></a><span class="sd">            db += y_hat - y[i]</span>
</span><span id="LogisticRegression-278"><a href="#LogisticRegression-278"><span class="linenos">278</span></a>
</span><span id="LogisticRegression-279"><a href="#LogisticRegression-279"><span class="linenos">279</span></a><span class="sd">        return dw / m, db / m</span>
</span><span id="LogisticRegression-280"><a href="#LogisticRegression-280"><span class="linenos">280</span></a><span class="sd">        &quot;&quot;&quot;</span>
</span><span id="LogisticRegression-281"><a href="#LogisticRegression-281"><span class="linenos">281</span></a>        <span class="n">y_hat</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_y_hat</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">W</span><span class="p">,</span> <span class="n">b</span><span class="p">)</span>
</span><span id="LogisticRegression-282"><a href="#LogisticRegression-282"><span class="linenos">282</span></a>
</span><span id="LogisticRegression-283"><a href="#LogisticRegression-283"><span class="linenos">283</span></a>        <span class="c1"># Gradient of logistic regression</span>
</span><span id="LogisticRegression-284"><a href="#LogisticRegression-284"><span class="linenos">284</span></a>
</span><span id="LogisticRegression-285"><a href="#LogisticRegression-285"><span class="linenos">285</span></a>        <span class="c1"># dJ/dW = (1/m) * sum((y_hat - y) * x)</span>
</span><span id="LogisticRegression-286"><a href="#LogisticRegression-286"><span class="linenos">286</span></a>        <span class="c1"># We use X.T to get the transpose of X so that we can multiply it with (y_hat - y)</span>
</span><span id="LogisticRegression-287"><a href="#LogisticRegression-287"><span class="linenos">287</span></a>        <span class="c1"># transpose of X has shape (n_features, n_samples)</span>
</span><span id="LogisticRegression-288"><a href="#LogisticRegression-288"><span class="linenos">288</span></a>        <span class="c1"># (y_hat - y) has shape (n_samples,)</span>
</span><span id="LogisticRegression-289"><a href="#LogisticRegression-289"><span class="linenos">289</span></a>        <span class="c1"># The result of the multiplication has shape (n_features,)</span>
</span><span id="LogisticRegression-290"><a href="#LogisticRegression-290"><span class="linenos">290</span></a>        <span class="n">dJ_dW</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">X</span><span class="o">.</span><span class="n">T</span><span class="p">,</span> <span class="n">y_hat</span> <span class="o">-</span> <span class="n">Y</span><span class="p">)</span> <span class="o">/</span> <span class="n">X</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
</span><span id="LogisticRegression-291"><a href="#LogisticRegression-291"><span class="linenos">291</span></a>
</span><span id="LogisticRegression-292"><a href="#LogisticRegression-292"><span class="linenos">292</span></a>        <span class="c1"># dJ/db = (1/m) * sum(y_hat - y)</span>
</span><span id="LogisticRegression-293"><a href="#LogisticRegression-293"><span class="linenos">293</span></a>        <span class="c1"># (y_hat - y) has shape (n_samples,)</span>
</span><span id="LogisticRegression-294"><a href="#LogisticRegression-294"><span class="linenos">294</span></a>        <span class="c1"># The result of the multiplication has shape (1,)</span>
</span><span id="LogisticRegression-295"><a href="#LogisticRegression-295"><span class="linenos">295</span></a>        <span class="n">dJ_db</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">y_hat</span> <span class="o">-</span> <span class="n">Y</span><span class="p">)</span> <span class="o">/</span> <span class="n">X</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
</span><span id="LogisticRegression-296"><a href="#LogisticRegression-296"><span class="linenos">296</span></a>
</span><span id="LogisticRegression-297"><a href="#LogisticRegression-297"><span class="linenos">297</span></a>        <span class="k">return</span> <span class="n">dJ_dW</span><span class="p">,</span> <span class="n">dJ_db</span>
</span><span id="LogisticRegression-298"><a href="#LogisticRegression-298"><span class="linenos">298</span></a>
</span><span id="LogisticRegression-299"><a href="#LogisticRegression-299"><span class="linenos">299</span></a>    <span class="k">def</span> <span class="nf">_validate_data</span><span class="p">(</span>
</span><span id="LogisticRegression-300"><a href="#LogisticRegression-300"><span class="linenos">300</span></a>        <span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">,</span> <span class="n">Y</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span> <span class="o">=</span> <span class="kc">None</span>
</span><span id="LogisticRegression-301"><a href="#LogisticRegression-301"><span class="linenos">301</span></a>    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tuple</span><span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">]:</span>
</span><span id="LogisticRegression-302"><a href="#LogisticRegression-302"><span class="linenos">302</span></a><span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
</span><span id="LogisticRegression-303"><a href="#LogisticRegression-303"><span class="linenos">303</span></a><span class="sd">        ### Return the input and output arrays of the model.</span>
</span><span id="LogisticRegression-304"><a href="#LogisticRegression-304"><span class="linenos">304</span></a>
</span><span id="LogisticRegression-305"><a href="#LogisticRegression-305"><span class="linenos">305</span></a><span class="sd">        Parameters</span>
</span><span id="LogisticRegression-306"><a href="#LogisticRegression-306"><span class="linenos">306</span></a><span class="sd">        ----------</span>
</span><span id="LogisticRegression-307"><a href="#LogisticRegression-307"><span class="linenos">307</span></a>
</span><span id="LogisticRegression-308"><a href="#LogisticRegression-308"><span class="linenos">308</span></a><span class="sd">        `X` : np.ndarray</span>
</span><span id="LogisticRegression-309"><a href="#LogisticRegression-309"><span class="linenos">309</span></a><span class="sd">        - The input array of shape (n_samples, n_features)</span>
</span><span id="LogisticRegression-310"><a href="#LogisticRegression-310"><span class="linenos">310</span></a>
</span><span id="LogisticRegression-311"><a href="#LogisticRegression-311"><span class="linenos">311</span></a><span class="sd">        `Y` : np.ndarray, optional</span>
</span><span id="LogisticRegression-312"><a href="#LogisticRegression-312"><span class="linenos">312</span></a><span class="sd">        - The output array of shape (n_samples,)</span>
</span><span id="LogisticRegression-313"><a href="#LogisticRegression-313"><span class="linenos">313</span></a>
</span><span id="LogisticRegression-314"><a href="#LogisticRegression-314"><span class="linenos">314</span></a>
</span><span id="LogisticRegression-315"><a href="#LogisticRegression-315"><span class="linenos">315</span></a><span class="sd">        Returns</span>
</span><span id="LogisticRegression-316"><a href="#LogisticRegression-316"><span class="linenos">316</span></a><span class="sd">        -------</span>
</span><span id="LogisticRegression-317"><a href="#LogisticRegression-317"><span class="linenos">317</span></a>
</span><span id="LogisticRegression-318"><a href="#LogisticRegression-318"><span class="linenos">318</span></a><span class="sd">        `Tuple[np.ndarray, np.ndarray]`</span>
</span><span id="LogisticRegression-319"><a href="#LogisticRegression-319"><span class="linenos">319</span></a><span class="sd">        - The input and output arrays of the model</span>
</span><span id="LogisticRegression-320"><a href="#LogisticRegression-320"><span class="linenos">320</span></a><span class="sd">        - X has shape (n_samples, n_features)</span>
</span><span id="LogisticRegression-321"><a href="#LogisticRegression-321"><span class="linenos">321</span></a><span class="sd">        - Y has shape (n_samples,)</span>
</span><span id="LogisticRegression-322"><a href="#LogisticRegression-322"><span class="linenos">322</span></a>
</span><span id="LogisticRegression-323"><a href="#LogisticRegression-323"><span class="linenos">323</span></a><span class="sd">        ---</span>
</span><span id="LogisticRegression-324"><a href="#LogisticRegression-324"><span class="linenos">324</span></a><span class="sd">        &quot;&quot;&quot;</span>
</span><span id="LogisticRegression-325"><a href="#LogisticRegression-325"><span class="linenos">325</span></a>        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_copy_x</span><span class="p">:</span>
</span><span id="LogisticRegression-326"><a href="#LogisticRegression-326"><span class="linenos">326</span></a>            <span class="n">X</span> <span class="o">=</span> <span class="n">copy</span><span class="o">.</span><span class="n">deepcopy</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
</span><span id="LogisticRegression-327"><a href="#LogisticRegression-327"><span class="linenos">327</span></a>
</span><span id="LogisticRegression-328"><a href="#LogisticRegression-328"><span class="linenos">328</span></a>        <span class="n">X</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">__get_numpy_array</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
</span><span id="LogisticRegression-329"><a href="#LogisticRegression-329"><span class="linenos">329</span></a>        <span class="n">Y</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">__get_numpy_array</span><span class="p">(</span><span class="n">Y</span><span class="p">)</span> <span class="k">if</span> <span class="n">Y</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="k">else</span> <span class="kc">None</span>
</span><span id="LogisticRegression-330"><a href="#LogisticRegression-330"><span class="linenos">330</span></a>
</span><span id="LogisticRegression-331"><a href="#LogisticRegression-331"><span class="linenos">331</span></a>        <span class="k">if</span> <span class="n">X</span><span class="o">.</span><span class="n">ndim</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>
</span><span id="LogisticRegression-332"><a href="#LogisticRegression-332"><span class="linenos">332</span></a>            <span class="n">X</span> <span class="o">=</span> <span class="n">X</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
</span><span id="LogisticRegression-333"><a href="#LogisticRegression-333"><span class="linenos">333</span></a>
</span><span id="LogisticRegression-334"><a href="#LogisticRegression-334"><span class="linenos">334</span></a>        <span class="k">for</span> <span class="n">scalar</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">_x_scalar</span><span class="p">:</span>
</span><span id="LogisticRegression-335"><a href="#LogisticRegression-335"><span class="linenos">335</span></a>            <span class="n">X</span> <span class="o">=</span> <span class="n">scalar</span><span class="o">.</span><span class="n">transform</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
</span><span id="LogisticRegression-336"><a href="#LogisticRegression-336"><span class="linenos">336</span></a>
</span><span id="LogisticRegression-337"><a href="#LogisticRegression-337"><span class="linenos">337</span></a>        <span class="k">if</span> <span class="n">Y</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
</span><span id="LogisticRegression-338"><a href="#LogisticRegression-338"><span class="linenos">338</span></a>            <span class="k">if</span> <span class="n">Y</span><span class="o">.</span><span class="n">ndim</span> <span class="o">==</span> <span class="mi">2</span><span class="p">:</span>
</span><span id="LogisticRegression-339"><a href="#LogisticRegression-339"><span class="linenos">339</span></a>                <span class="n">Y</span> <span class="o">=</span> <span class="n">Y</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span>
</span><span id="LogisticRegression-340"><a href="#LogisticRegression-340"><span class="linenos">340</span></a>
</span><span id="LogisticRegression-341"><a href="#LogisticRegression-341"><span class="linenos">341</span></a>        <span class="k">if</span> <span class="n">Y</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
</span><span id="LogisticRegression-342"><a href="#LogisticRegression-342"><span class="linenos">342</span></a>            <span class="k">return</span> <span class="n">X</span>
</span><span id="LogisticRegression-343"><a href="#LogisticRegression-343"><span class="linenos">343</span></a>        <span class="k">return</span> <span class="n">X</span><span class="p">,</span> <span class="n">Y</span>
</span><span id="LogisticRegression-344"><a href="#LogisticRegression-344"><span class="linenos">344</span></a>
</span><span id="LogisticRegression-345"><a href="#LogisticRegression-345"><span class="linenos">345</span></a>    <span class="k">def</span> <span class="nf">fit</span><span class="p">(</span>
</span><span id="LogisticRegression-346"><a href="#LogisticRegression-346"><span class="linenos">346</span></a>        <span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">,</span> <span class="n">Y</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">,</span> <span class="n">W</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span> <span class="n">b</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">float64</span> <span class="o">=</span> <span class="mf">0.0</span>
</span><span id="LogisticRegression-347"><a href="#LogisticRegression-347"><span class="linenos">347</span></a>    <span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
</span><span id="LogisticRegression-348"><a href="#LogisticRegression-348"><span class="linenos">348</span></a><span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
</span><span id="LogisticRegression-349"><a href="#LogisticRegression-349"><span class="linenos">349</span></a><span class="sd">        ### Fit the model to the data.</span>
</span><span id="LogisticRegression-350"><a href="#LogisticRegression-350"><span class="linenos">350</span></a>
</span><span id="LogisticRegression-351"><a href="#LogisticRegression-351"><span class="linenos">351</span></a><span class="sd">        Parameters</span>
</span><span id="LogisticRegression-352"><a href="#LogisticRegression-352"><span class="linenos">352</span></a><span class="sd">        ----------</span>
</span><span id="LogisticRegression-353"><a href="#LogisticRegression-353"><span class="linenos">353</span></a>
</span><span id="LogisticRegression-354"><a href="#LogisticRegression-354"><span class="linenos">354</span></a><span class="sd">        `X` : np.ndarray</span>
</span><span id="LogisticRegression-355"><a href="#LogisticRegression-355"><span class="linenos">355</span></a><span class="sd">        - The input array of shape (n_samples, n_features) or (n_samples,)</span>
</span><span id="LogisticRegression-356"><a href="#LogisticRegression-356"><span class="linenos">356</span></a>
</span><span id="LogisticRegression-357"><a href="#LogisticRegression-357"><span class="linenos">357</span></a><span class="sd">        `Y` : np.ndarray</span>
</span><span id="LogisticRegression-358"><a href="#LogisticRegression-358"><span class="linenos">358</span></a><span class="sd">        - The output array of shape (n_samples,) or (n_samples, 1)</span>
</span><span id="LogisticRegression-359"><a href="#LogisticRegression-359"><span class="linenos">359</span></a>
</span><span id="LogisticRegression-360"><a href="#LogisticRegression-360"><span class="linenos">360</span></a><span class="sd">        `w` : np.ndarray, optional</span>
</span><span id="LogisticRegression-361"><a href="#LogisticRegression-361"><span class="linenos">361</span></a><span class="sd">        - The weight array, by default None</span>
</span><span id="LogisticRegression-362"><a href="#LogisticRegression-362"><span class="linenos">362</span></a><span class="sd">        - If None, then the weight array will be initialized to an array of</span>
</span><span id="LogisticRegression-363"><a href="#LogisticRegression-363"><span class="linenos">363</span></a><span class="sd">            zeros of shape (n_features,)</span>
</span><span id="LogisticRegression-364"><a href="#LogisticRegression-364"><span class="linenos">364</span></a><span class="sd">        - If not None, then the weight array will be initialized to the given</span>
</span><span id="LogisticRegression-365"><a href="#LogisticRegression-365"><span class="linenos">365</span></a><span class="sd">            array</span>
</span><span id="LogisticRegression-366"><a href="#LogisticRegression-366"><span class="linenos">366</span></a>
</span><span id="LogisticRegression-367"><a href="#LogisticRegression-367"><span class="linenos">367</span></a><span class="sd">        `b` : np.float64, optional</span>
</span><span id="LogisticRegression-368"><a href="#LogisticRegression-368"><span class="linenos">368</span></a><span class="sd">        - The intercept, by default 0.0</span>
</span><span id="LogisticRegression-369"><a href="#LogisticRegression-369"><span class="linenos">369</span></a><span class="sd">        - If None, then the intercept will be initialized to 0.0</span>
</span><span id="LogisticRegression-370"><a href="#LogisticRegression-370"><span class="linenos">370</span></a><span class="sd">        - If not None, then the intercept will be initialized to the given</span>
</span><span id="LogisticRegression-371"><a href="#LogisticRegression-371"><span class="linenos">371</span></a><span class="sd">            value</span>
</span><span id="LogisticRegression-372"><a href="#LogisticRegression-372"><span class="linenos">372</span></a>
</span><span id="LogisticRegression-373"><a href="#LogisticRegression-373"><span class="linenos">373</span></a>
</span><span id="LogisticRegression-374"><a href="#LogisticRegression-374"><span class="linenos">374</span></a><span class="sd">        Returns</span>
</span><span id="LogisticRegression-375"><a href="#LogisticRegression-375"><span class="linenos">375</span></a><span class="sd">        -------</span>
</span><span id="LogisticRegression-376"><a href="#LogisticRegression-376"><span class="linenos">376</span></a>
</span><span id="LogisticRegression-377"><a href="#LogisticRegression-377"><span class="linenos">377</span></a><span class="sd">        None</span>
</span><span id="LogisticRegression-378"><a href="#LogisticRegression-378"><span class="linenos">378</span></a>
</span><span id="LogisticRegression-379"><a href="#LogisticRegression-379"><span class="linenos">379</span></a><span class="sd">        ---</span>
</span><span id="LogisticRegression-380"><a href="#LogisticRegression-380"><span class="linenos">380</span></a><span class="sd">        &quot;&quot;&quot;</span>
</span><span id="LogisticRegression-381"><a href="#LogisticRegression-381"><span class="linenos">381</span></a>        <span class="n">X</span><span class="p">,</span> <span class="n">Y</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_validate_data</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">Y</span><span class="p">)</span>
</span><span id="LogisticRegression-382"><a href="#LogisticRegression-382"><span class="linenos">382</span></a>
</span><span id="LogisticRegression-383"><a href="#LogisticRegression-383"><span class="linenos">383</span></a>        <span class="k">assert</span> <span class="n">X</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">==</span> <span class="n">Y</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="s2">&quot;X and Y must have the same number of samples&quot;</span>
</span><span id="LogisticRegression-384"><a href="#LogisticRegression-384"><span class="linenos">384</span></a>
</span><span id="LogisticRegression-385"><a href="#LogisticRegression-385"><span class="linenos">385</span></a>        <span class="c1"># Initialize weights and intercept</span>
</span><span id="LogisticRegression-386"><a href="#LogisticRegression-386"><span class="linenos">386</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">_weights</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">X</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span> <span class="k">if</span> <span class="n">W</span> <span class="ow">is</span> <span class="kc">None</span> <span class="k">else</span> <span class="n">W</span>
</span><span id="LogisticRegression-387"><a href="#LogisticRegression-387"><span class="linenos">387</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">_intercept</span> <span class="o">=</span> <span class="n">b</span>
</span><span id="LogisticRegression-388"><a href="#LogisticRegression-388"><span class="linenos">388</span></a>
</span><span id="LogisticRegression-389"><a href="#LogisticRegression-389"><span class="linenos">389</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">_cost_history</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span>
</span><span id="LogisticRegression-390"><a href="#LogisticRegression-390"><span class="linenos">390</span></a>            <span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">_cost</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">Y</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">_weights</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">_intercept</span><span class="p">)]</span>
</span><span id="LogisticRegression-391"><a href="#LogisticRegression-391"><span class="linenos">391</span></a>        <span class="p">)</span>
</span><span id="LogisticRegression-392"><a href="#LogisticRegression-392"><span class="linenos">392</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">_params_history</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span>
</span><span id="LogisticRegression-393"><a href="#LogisticRegression-393"><span class="linenos">393</span></a>            <span class="p">[[</span><span class="bp">self</span><span class="o">.</span><span class="n">_weights</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">_intercept</span><span class="p">]],</span> <span class="n">dtype</span><span class="o">=</span><span class="nb">object</span>
</span><span id="LogisticRegression-394"><a href="#LogisticRegression-394"><span class="linenos">394</span></a>        <span class="p">)</span>
</span><span id="LogisticRegression-395"><a href="#LogisticRegression-395"><span class="linenos">395</span></a>
</span><span id="LogisticRegression-396"><a href="#LogisticRegression-396"><span class="linenos">396</span></a>        <span class="c1"># Iterate and update weights and intercept</span>
</span><span id="LogisticRegression-397"><a href="#LogisticRegression-397"><span class="linenos">397</span></a>        <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_n_iterations</span><span class="p">):</span>
</span><span id="LogisticRegression-398"><a href="#LogisticRegression-398"><span class="linenos">398</span></a>            <span class="c1"># Calculate gradient</span>
</span><span id="LogisticRegression-399"><a href="#LogisticRegression-399"><span class="linenos">399</span></a>            <span class="n">dw</span><span class="p">,</span> <span class="n">db</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_gradient</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">Y</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">_weights</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">_intercept</span><span class="p">)</span>
</span><span id="LogisticRegression-400"><a href="#LogisticRegression-400"><span class="linenos">400</span></a>
</span><span id="LogisticRegression-401"><a href="#LogisticRegression-401"><span class="linenos">401</span></a>            <span class="c1"># Update weights and intercept</span>
</span><span id="LogisticRegression-402"><a href="#LogisticRegression-402"><span class="linenos">402</span></a>            <span class="bp">self</span><span class="o">.</span><span class="n">_weights</span> <span class="o">-=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_learning_rate</span> <span class="o">*</span> <span class="n">dw</span>
</span><span id="LogisticRegression-403"><a href="#LogisticRegression-403"><span class="linenos">403</span></a>            <span class="bp">self</span><span class="o">.</span><span class="n">_intercept</span> <span class="o">-=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_learning_rate</span> <span class="o">*</span> <span class="n">db</span>
</span><span id="LogisticRegression-404"><a href="#LogisticRegression-404"><span class="linenos">404</span></a>
</span><span id="LogisticRegression-405"><a href="#LogisticRegression-405"><span class="linenos">405</span></a>            <span class="c1"># Save cost and params history</span>
</span><span id="LogisticRegression-406"><a href="#LogisticRegression-406"><span class="linenos">406</span></a>            <span class="n">cost</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_cost</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">Y</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">_weights</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">_intercept</span><span class="p">)</span>
</span><span id="LogisticRegression-407"><a href="#LogisticRegression-407"><span class="linenos">407</span></a>
</span><span id="LogisticRegression-408"><a href="#LogisticRegression-408"><span class="linenos">408</span></a>            <span class="k">if</span> <span class="n">cost</span> <span class="o">==</span> <span class="n">np</span><span class="o">.</span><span class="n">nan</span> <span class="ow">or</span> <span class="n">cost</span> <span class="o">==</span> <span class="n">np</span><span class="o">.</span><span class="n">inf</span><span class="p">:</span>
</span><span id="LogisticRegression-409"><a href="#LogisticRegression-409"><span class="linenos">409</span></a>                <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
</span><span id="LogisticRegression-410"><a href="#LogisticRegression-410"><span class="linenos">410</span></a>                    <span class="s2">&quot;Gradient descent failed. Try normalizing the input array or reducing the learning rate. &quot;</span>
</span><span id="LogisticRegression-411"><a href="#LogisticRegression-411"><span class="linenos">411</span></a>                    <span class="s2">&quot;If the problem persists, try reducing the number of iterations.&quot;</span>
</span><span id="LogisticRegression-412"><a href="#LogisticRegression-412"><span class="linenos">412</span></a>                <span class="p">)</span>
</span><span id="LogisticRegression-413"><a href="#LogisticRegression-413"><span class="linenos">413</span></a>
</span><span id="LogisticRegression-414"><a href="#LogisticRegression-414"><span class="linenos">414</span></a>            <span class="bp">self</span><span class="o">.</span><span class="n">_cost_history</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_cost_history</span><span class="p">,</span> <span class="n">cost</span><span class="p">)</span>
</span><span id="LogisticRegression-415"><a href="#LogisticRegression-415"><span class="linenos">415</span></a>            <span class="bp">self</span><span class="o">.</span><span class="n">_params_history</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">append</span><span class="p">(</span>
</span><span id="LogisticRegression-416"><a href="#LogisticRegression-416"><span class="linenos">416</span></a>                <span class="bp">self</span><span class="o">.</span><span class="n">_params_history</span><span class="p">,</span>
</span><span id="LogisticRegression-417"><a href="#LogisticRegression-417"><span class="linenos">417</span></a>                <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([[</span><span class="bp">self</span><span class="o">.</span><span class="n">_weights</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">_intercept</span><span class="p">]],</span> <span class="n">dtype</span><span class="o">=</span><span class="nb">object</span><span class="p">),</span>
</span><span id="LogisticRegression-418"><a href="#LogisticRegression-418"><span class="linenos">418</span></a>                <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span>
</span><span id="LogisticRegression-419"><a href="#LogisticRegression-419"><span class="linenos">419</span></a>            <span class="p">)</span>
</span><span id="LogisticRegression-420"><a href="#LogisticRegression-420"><span class="linenos">420</span></a>
</span><span id="LogisticRegression-421"><a href="#LogisticRegression-421"><span class="linenos">421</span></a>            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_debug</span> <span class="ow">and</span> <span class="n">i</span> <span class="o">%</span> <span class="bp">self</span><span class="o">.</span><span class="n">_debug_freq</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
</span><span id="LogisticRegression-422"><a href="#LogisticRegression-422"><span class="linenos">422</span></a>                <span class="bp">self</span><span class="o">.</span><span class="n">_debug_print</span><span class="p">(</span><span class="n">i</span><span class="p">,</span> <span class="n">cost</span><span class="p">)</span>
</span><span id="LogisticRegression-423"><a href="#LogisticRegression-423"><span class="linenos">423</span></a>
</span><span id="LogisticRegression-424"><a href="#LogisticRegression-424"><span class="linenos">424</span></a>        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_debug</span><span class="p">:</span>
</span><span id="LogisticRegression-425"><a href="#LogisticRegression-425"><span class="linenos">425</span></a>            <span class="bp">self</span><span class="o">.</span><span class="n">_debug_print</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_n_iterations</span><span class="p">,</span> <span class="n">cost</span><span class="p">)</span>
</span><span id="LogisticRegression-426"><a href="#LogisticRegression-426"><span class="linenos">426</span></a>
</span><span id="LogisticRegression-427"><a href="#LogisticRegression-427"><span class="linenos">427</span></a>    <span class="k">def</span> <span class="nf">predict_proba</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">:</span>
</span><span id="LogisticRegression-428"><a href="#LogisticRegression-428"><span class="linenos">428</span></a><span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
</span><span id="LogisticRegression-429"><a href="#LogisticRegression-429"><span class="linenos">429</span></a><span class="sd">        ### Predict the probability of the output given the input.</span>
</span><span id="LogisticRegression-430"><a href="#LogisticRegression-430"><span class="linenos">430</span></a>
</span><span id="LogisticRegression-431"><a href="#LogisticRegression-431"><span class="linenos">431</span></a><span class="sd">        Parameters</span>
</span><span id="LogisticRegression-432"><a href="#LogisticRegression-432"><span class="linenos">432</span></a><span class="sd">        ----------</span>
</span><span id="LogisticRegression-433"><a href="#LogisticRegression-433"><span class="linenos">433</span></a>
</span><span id="LogisticRegression-434"><a href="#LogisticRegression-434"><span class="linenos">434</span></a><span class="sd">        `X` : np.ndarray</span>
</span><span id="LogisticRegression-435"><a href="#LogisticRegression-435"><span class="linenos">435</span></a><span class="sd">        - The input array of shape (n_samples, n_features) or (n_features,)</span>
</span><span id="LogisticRegression-436"><a href="#LogisticRegression-436"><span class="linenos">436</span></a>
</span><span id="LogisticRegression-437"><a href="#LogisticRegression-437"><span class="linenos">437</span></a>
</span><span id="LogisticRegression-438"><a href="#LogisticRegression-438"><span class="linenos">438</span></a><span class="sd">        Returns</span>
</span><span id="LogisticRegression-439"><a href="#LogisticRegression-439"><span class="linenos">439</span></a><span class="sd">        -------</span>
</span><span id="LogisticRegression-440"><a href="#LogisticRegression-440"><span class="linenos">440</span></a>
</span><span id="LogisticRegression-441"><a href="#LogisticRegression-441"><span class="linenos">441</span></a><span class="sd">        `np.ndarray`</span>
</span><span id="LogisticRegression-442"><a href="#LogisticRegression-442"><span class="linenos">442</span></a><span class="sd">        - The predicted output array of shape (n_samples,) or (1,)</span>
</span><span id="LogisticRegression-443"><a href="#LogisticRegression-443"><span class="linenos">443</span></a>
</span><span id="LogisticRegression-444"><a href="#LogisticRegression-444"><span class="linenos">444</span></a><span class="sd">        ---</span>
</span><span id="LogisticRegression-445"><a href="#LogisticRegression-445"><span class="linenos">445</span></a><span class="sd">        &quot;&quot;&quot;</span>
</span><span id="LogisticRegression-446"><a href="#LogisticRegression-446"><span class="linenos">446</span></a>        <span class="k">assert</span> <span class="bp">self</span><span class="o">.</span><span class="n">_weights</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">and</span> <span class="bp">self</span><span class="o">.</span><span class="n">_intercept</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">,</span> <span class="p">(</span>
</span><span id="LogisticRegression-447"><a href="#LogisticRegression-447"><span class="linenos">447</span></a>            <span class="s2">&quot;The model must be trained before making predictions. &quot;</span>
</span><span id="LogisticRegression-448"><a href="#LogisticRegression-448"><span class="linenos">448</span></a>            <span class="s2">&quot;Call the fit method first.&quot;</span>
</span><span id="LogisticRegression-449"><a href="#LogisticRegression-449"><span class="linenos">449</span></a>        <span class="p">)</span>
</span><span id="LogisticRegression-450"><a href="#LogisticRegression-450"><span class="linenos">450</span></a>
</span><span id="LogisticRegression-451"><a href="#LogisticRegression-451"><span class="linenos">451</span></a>        <span class="n">X</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_validate_data</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
</span><span id="LogisticRegression-452"><a href="#LogisticRegression-452"><span class="linenos">452</span></a>        <span class="c1"># Return the probability of the output being 1</span>
</span><span id="LogisticRegression-453"><a href="#LogisticRegression-453"><span class="linenos">453</span></a>        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_y_hat</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">_weights</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">_intercept</span><span class="p">)</span>
</span><span id="LogisticRegression-454"><a href="#LogisticRegression-454"><span class="linenos">454</span></a>
</span><span id="LogisticRegression-455"><a href="#LogisticRegression-455"><span class="linenos">455</span></a>    <span class="k">def</span> <span class="nf">predict</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">:</span>
</span><span id="LogisticRegression-456"><a href="#LogisticRegression-456"><span class="linenos">456</span></a><span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
</span><span id="LogisticRegression-457"><a href="#LogisticRegression-457"><span class="linenos">457</span></a><span class="sd">        ### Predict the output given the input.</span>
</span><span id="LogisticRegression-458"><a href="#LogisticRegression-458"><span class="linenos">458</span></a>
</span><span id="LogisticRegression-459"><a href="#LogisticRegression-459"><span class="linenos">459</span></a><span class="sd">        Parameters</span>
</span><span id="LogisticRegression-460"><a href="#LogisticRegression-460"><span class="linenos">460</span></a><span class="sd">        ----------</span>
</span><span id="LogisticRegression-461"><a href="#LogisticRegression-461"><span class="linenos">461</span></a>
</span><span id="LogisticRegression-462"><a href="#LogisticRegression-462"><span class="linenos">462</span></a><span class="sd">        `X` : np.ndarray</span>
</span><span id="LogisticRegression-463"><a href="#LogisticRegression-463"><span class="linenos">463</span></a><span class="sd">        - The input array of shape (n_samples, n_features) or (n_features,)</span>
</span><span id="LogisticRegression-464"><a href="#LogisticRegression-464"><span class="linenos">464</span></a>
</span><span id="LogisticRegression-465"><a href="#LogisticRegression-465"><span class="linenos">465</span></a>
</span><span id="LogisticRegression-466"><a href="#LogisticRegression-466"><span class="linenos">466</span></a><span class="sd">        Returns</span>
</span><span id="LogisticRegression-467"><a href="#LogisticRegression-467"><span class="linenos">467</span></a><span class="sd">        -------</span>
</span><span id="LogisticRegression-468"><a href="#LogisticRegression-468"><span class="linenos">468</span></a>
</span><span id="LogisticRegression-469"><a href="#LogisticRegression-469"><span class="linenos">469</span></a><span class="sd">        `np.ndarray`</span>
</span><span id="LogisticRegression-470"><a href="#LogisticRegression-470"><span class="linenos">470</span></a><span class="sd">        - The predicted output array of shape (n_samples,) or (1,)</span>
</span><span id="LogisticRegression-471"><a href="#LogisticRegression-471"><span class="linenos">471</span></a>
</span><span id="LogisticRegression-472"><a href="#LogisticRegression-472"><span class="linenos">472</span></a><span class="sd">        ---</span>
</span><span id="LogisticRegression-473"><a href="#LogisticRegression-473"><span class="linenos">473</span></a><span class="sd">        &quot;&quot;&quot;</span>
</span><span id="LogisticRegression-474"><a href="#LogisticRegression-474"><span class="linenos">474</span></a>        <span class="c1"># Return 1 if the probability of the output being 1 is greater than or equal to 0.5</span>
</span><span id="LogisticRegression-475"><a href="#LogisticRegression-475"><span class="linenos">475</span></a>        <span class="c1"># Return 0 otherwise</span>
</span><span id="LogisticRegression-476"><a href="#LogisticRegression-476"><span class="linenos">476</span></a>        <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">where</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">predict_proba</span><span class="p">(</span><span class="n">X</span><span class="p">)</span> <span class="o">&gt;=</span> <span class="mf">0.5</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">)</span>
</span><span id="LogisticRegression-477"><a href="#LogisticRegression-477"><span class="linenos">477</span></a>
</span><span id="LogisticRegression-478"><a href="#LogisticRegression-478"><span class="linenos">478</span></a>    <span class="k">def</span> <span class="nf">score</span><span class="p">(</span>
</span><span id="LogisticRegression-479"><a href="#LogisticRegression-479"><span class="linenos">479</span></a>        <span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">,</span> <span class="n">Y</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">,</span> <span class="n">W</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span> <span class="n">b</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">float64</span> <span class="o">=</span> <span class="kc">None</span>
</span><span id="LogisticRegression-480"><a href="#LogisticRegression-480"><span class="linenos">480</span></a>    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">np</span><span class="o">.</span><span class="n">float64</span><span class="p">:</span>
</span><span id="LogisticRegression-481"><a href="#LogisticRegression-481"><span class="linenos">481</span></a><span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
</span><span id="LogisticRegression-482"><a href="#LogisticRegression-482"><span class="linenos">482</span></a><span class="sd">        ### Return the cost of the model given X, Y, W, and b.</span>
</span><span id="LogisticRegression-483"><a href="#LogisticRegression-483"><span class="linenos">483</span></a>
</span><span id="LogisticRegression-484"><a href="#LogisticRegression-484"><span class="linenos">484</span></a><span class="sd">        Parameters</span>
</span><span id="LogisticRegression-485"><a href="#LogisticRegression-485"><span class="linenos">485</span></a><span class="sd">        ----------</span>
</span><span id="LogisticRegression-486"><a href="#LogisticRegression-486"><span class="linenos">486</span></a><span class="sd">        `X` : np.ndarray</span>
</span><span id="LogisticRegression-487"><a href="#LogisticRegression-487"><span class="linenos">487</span></a><span class="sd">        - The input array of shape (n_samples, n_features)</span>
</span><span id="LogisticRegression-488"><a href="#LogisticRegression-488"><span class="linenos">488</span></a>
</span><span id="LogisticRegression-489"><a href="#LogisticRegression-489"><span class="linenos">489</span></a><span class="sd">        `Y` : np.ndarray</span>
</span><span id="LogisticRegression-490"><a href="#LogisticRegression-490"><span class="linenos">490</span></a><span class="sd">        - The output array of shape (n_samples,)</span>
</span><span id="LogisticRegression-491"><a href="#LogisticRegression-491"><span class="linenos">491</span></a>
</span><span id="LogisticRegression-492"><a href="#LogisticRegression-492"><span class="linenos">492</span></a><span class="sd">        `W` : np.ndarray, optional</span>
</span><span id="LogisticRegression-493"><a href="#LogisticRegression-493"><span class="linenos">493</span></a><span class="sd">        - The weight array of shape (n_features,), by default None</span>
</span><span id="LogisticRegression-494"><a href="#LogisticRegression-494"><span class="linenos">494</span></a><span class="sd">        - If None, then the weight array will be cosidered as the trained</span>
</span><span id="LogisticRegression-495"><a href="#LogisticRegression-495"><span class="linenos">495</span></a><span class="sd">            weight array</span>
</span><span id="LogisticRegression-496"><a href="#LogisticRegression-496"><span class="linenos">496</span></a>
</span><span id="LogisticRegression-497"><a href="#LogisticRegression-497"><span class="linenos">497</span></a><span class="sd">        `b` : np.float64, optional</span>
</span><span id="LogisticRegression-498"><a href="#LogisticRegression-498"><span class="linenos">498</span></a><span class="sd">        - The intercept, by default None</span>
</span><span id="LogisticRegression-499"><a href="#LogisticRegression-499"><span class="linenos">499</span></a><span class="sd">        - If None, then the intercept will be cosidered as the trained</span>
</span><span id="LogisticRegression-500"><a href="#LogisticRegression-500"><span class="linenos">500</span></a><span class="sd">            intercept</span>
</span><span id="LogisticRegression-501"><a href="#LogisticRegression-501"><span class="linenos">501</span></a>
</span><span id="LogisticRegression-502"><a href="#LogisticRegression-502"><span class="linenos">502</span></a>
</span><span id="LogisticRegression-503"><a href="#LogisticRegression-503"><span class="linenos">503</span></a><span class="sd">        Returns</span>
</span><span id="LogisticRegression-504"><a href="#LogisticRegression-504"><span class="linenos">504</span></a><span class="sd">        -------</span>
</span><span id="LogisticRegression-505"><a href="#LogisticRegression-505"><span class="linenos">505</span></a><span class="sd">        `np.float64`</span>
</span><span id="LogisticRegression-506"><a href="#LogisticRegression-506"><span class="linenos">506</span></a><span class="sd">        - The cost of the model</span>
</span><span id="LogisticRegression-507"><a href="#LogisticRegression-507"><span class="linenos">507</span></a>
</span><span id="LogisticRegression-508"><a href="#LogisticRegression-508"><span class="linenos">508</span></a><span class="sd">        ---</span>
</span><span id="LogisticRegression-509"><a href="#LogisticRegression-509"><span class="linenos">509</span></a><span class="sd">        &quot;&quot;&quot;</span>
</span><span id="LogisticRegression-510"><a href="#LogisticRegression-510"><span class="linenos">510</span></a>        <span class="n">X</span><span class="p">,</span> <span class="n">Y</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_validate_data</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">Y</span><span class="p">)</span>
</span><span id="LogisticRegression-511"><a href="#LogisticRegression-511"><span class="linenos">511</span></a>
</span><span id="LogisticRegression-512"><a href="#LogisticRegression-512"><span class="linenos">512</span></a>        <span class="n">W</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_weights</span> <span class="k">if</span> <span class="n">W</span> <span class="ow">is</span> <span class="kc">None</span> <span class="k">else</span> <span class="n">W</span>
</span><span id="LogisticRegression-513"><a href="#LogisticRegression-513"><span class="linenos">513</span></a>        <span class="n">b</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_intercept</span> <span class="k">if</span> <span class="n">b</span> <span class="ow">is</span> <span class="kc">None</span> <span class="k">else</span> <span class="n">b</span>
</span><span id="LogisticRegression-514"><a href="#LogisticRegression-514"><span class="linenos">514</span></a>
</span><span id="LogisticRegression-515"><a href="#LogisticRegression-515"><span class="linenos">515</span></a>        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_cost</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">Y</span><span class="p">,</span> <span class="n">W</span><span class="p">,</span> <span class="n">b</span><span class="p">)</span>
</span></pre></div>


            <div class="docstring"><h1 id="logistic-regression-model">Logistic Regression Model</h1>

<p>Logistic Regression is a fundamental classification algorithm used to model the probability of a binary outcome. It's widely employed in machine learning for binary classification tasks and offers insights into the relationship between input features and class probabilities.</p>

<hr />

<h2 id="mathematical-approach">Mathematical Approach</h2>

<p>Logistic Regression aims to predict the probability of a binary outcome by modeling it as a sigmoid function of a linear combination of input features. The equation takes the form:</p>

<pre><code>P(y=1 | X) = 1 / (1 + e^(-z))
</code></pre>

<p>Where <code>P(y=1 | X)</code> is the probability of the positive class given input <code>X</code>, and <code>z</code> is the linear combination of input features, weights, and an intercept.</p>

<hr />

<h2 id="usage">Usage</h2>

<p>To use the Logistic Regression model, follow these steps:</p>

<ol>
<li>Import the <code><a href="#LogisticRegression">LogisticRegression</a></code> class from the appropriate module.</li>
<li>Create an instance of the <code><a href="#LogisticRegression">LogisticRegression</a></code> class, specifying hyperparameters.</li>
<li>Fit the model to your training data using the <code><a href="#LogisticRegression.fit">fit</a></code> method.</li>
<li>Make predictions on new data using the <code><a href="#LogisticRegression.predict">predict</a></code> method.</li>
<li>Evaluate the model's performance using the <code><a href="#LogisticRegression.score">score</a></code> method.</li>
</ol>

<div class="pdoc-code codehilite">
<pre><span></span><code><span class="kn">from</span> <span class="nn"><a href="">learnML.classification</a></span> <span class="kn">import</span> <span class="n">LogisticRegression</span>

<span class="c1"># Create an instance of LogisticRegression</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">LogisticRegression</span><span class="p">(</span><span class="n">learning_rate</span><span class="o">=</span><span class="mf">0.001</span><span class="p">,</span> <span class="n">n_iterations</span><span class="o">=</span><span class="mi">1000</span><span class="p">)</span>

<span class="c1"># Fit the model to training data</span>
<span class="n">model</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">Y_train</span><span class="p">)</span>

<span class="c1"># Make predictions on new data</span>
<span class="n">predictions</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_test</span><span class="p">)</span>

<span class="c1"># Calculate the model&#39;s score</span>
<span class="n">model_score</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">score</span><span class="p">(</span><span class="n">X_test</span><span class="p">,</span> <span class="n">Y_test</span><span class="p">)</span>
</code></pre>
</div>

<hr />

<h2 id="advantages">Advantages</h2>

<ul>
<li>Simple and efficient</li>
<li>Can be updated easily with new data using stochastic gradient descent</li>
<li>Outputs have a nice probabilistic interpretation</li>
<li>Can be regularized to avoid overfitting</li>
<li>Works well with high dimensional data</li>
<li>Works well with sparse data</li>
</ul>

<h2 id="disadvantages">Disadvantages</h2>

<ul>
<li>Not suitable for large number of features</li>
<li>Not suitable for non-linear problems</li>
</ul>

<hr />
</div>


                            <div id="LogisticRegression.__init__" class="classattr">
                                        <input id="LogisticRegression.__init__-view-source" class="view-source-toggle-state" type="checkbox" aria-hidden="true" tabindex="-1">
<div class="attr function">
            
        <span class="name">LogisticRegression</span><span class="signature pdoc-code multiline">(<span class="param">	<span class="n">learning_rate</span><span class="p">:</span> <span class="n">numpy</span><span class="o">.</span><span class="n">float64</span> <span class="o">=</span> <span class="mf">0.001</span>,</span><span class="param">	<span class="n">n_iterations</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">1000</span>,</span><span class="param">	<span class="n">lambda_</span><span class="p">:</span> <span class="n">numpy</span><span class="o">.</span><span class="n">float64</span> <span class="o">=</span> <span class="mi">0</span>,</span><span class="param">	<span class="n">x_scalar</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="n">learnML</span><span class="o">.</span><span class="n">interfaces</span><span class="o">.</span><span class="n">ifeature_engineering</span><span class="o">.</span><span class="n">IFeatureEngineering</span><span class="p">,</span> <span class="n">List</span><span class="p">[</span><span class="n">learnML</span><span class="o">.</span><span class="n">interfaces</span><span class="o">.</span><span class="n">ifeature_engineering</span><span class="o">.</span><span class="n">IFeatureEngineering</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span>,</span><span class="param">	<span class="n">debug</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span>,</span><span class="param">	<span class="n">copy_x</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span></span>)</span>

                <label class="view-source-button" for="LogisticRegression.__init__-view-source"><span>View Source</span></label>

    </div>
    <a class="headerlink" href="#LogisticRegression.__init__"></a>
            <div class="pdoc-code codehilite"><pre><span></span><span id="LogisticRegression.__init__-74"><a href="#LogisticRegression.__init__-74"><span class="linenos"> 74</span></a>    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span>
</span><span id="LogisticRegression.__init__-75"><a href="#LogisticRegression.__init__-75"><span class="linenos"> 75</span></a>        <span class="bp">self</span><span class="p">,</span>
</span><span id="LogisticRegression.__init__-76"><a href="#LogisticRegression.__init__-76"><span class="linenos"> 76</span></a>        <span class="n">learning_rate</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">float64</span> <span class="o">=</span> <span class="mf">0.001</span><span class="p">,</span>
</span><span id="LogisticRegression.__init__-77"><a href="#LogisticRegression.__init__-77"><span class="linenos"> 77</span></a>        <span class="n">n_iterations</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">1000</span><span class="p">,</span>
</span><span id="LogisticRegression.__init__-78"><a href="#LogisticRegression.__init__-78"><span class="linenos"> 78</span></a>        <span class="n">lambda_</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">float64</span> <span class="o">=</span> <span class="mi">0</span><span class="p">,</span>
</span><span id="LogisticRegression.__init__-79"><a href="#LogisticRegression.__init__-79"><span class="linenos"> 79</span></a>        <span class="n">x_scalar</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="n">IFeatureEngineering</span><span class="p">,</span> <span class="n">List</span><span class="p">[</span><span class="n">IFeatureEngineering</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
</span><span id="LogisticRegression.__init__-80"><a href="#LogisticRegression.__init__-80"><span class="linenos"> 80</span></a>        <span class="n">debug</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
</span><span id="LogisticRegression.__init__-81"><a href="#LogisticRegression.__init__-81"><span class="linenos"> 81</span></a>        <span class="n">copy_x</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
</span><span id="LogisticRegression.__init__-82"><a href="#LogisticRegression.__init__-82"><span class="linenos"> 82</span></a>    <span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
</span><span id="LogisticRegression.__init__-83"><a href="#LogisticRegression.__init__-83"><span class="linenos"> 83</span></a><span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
</span><span id="LogisticRegression.__init__-84"><a href="#LogisticRegression.__init__-84"><span class="linenos"> 84</span></a><span class="sd">        Parameters</span>
</span><span id="LogisticRegression.__init__-85"><a href="#LogisticRegression.__init__-85"><span class="linenos"> 85</span></a><span class="sd">        ----------</span>
</span><span id="LogisticRegression.__init__-86"><a href="#LogisticRegression.__init__-86"><span class="linenos"> 86</span></a>
</span><span id="LogisticRegression.__init__-87"><a href="#LogisticRegression.__init__-87"><span class="linenos"> 87</span></a><span class="sd">        `learning_rate` : np.float64, optional</span>
</span><span id="LogisticRegression.__init__-88"><a href="#LogisticRegression.__init__-88"><span class="linenos"> 88</span></a><span class="sd">        - The learning rate, by default 0.001</span>
</span><span id="LogisticRegression.__init__-89"><a href="#LogisticRegression.__init__-89"><span class="linenos"> 89</span></a><span class="sd">        - The learning rate determines how much the weights are updated at each iteration</span>
</span><span id="LogisticRegression.__init__-90"><a href="#LogisticRegression.__init__-90"><span class="linenos"> 90</span></a><span class="sd">        - A low learning rate will take longer to converge, but a high learning rate may overshoot the optimal solution</span>
</span><span id="LogisticRegression.__init__-91"><a href="#LogisticRegression.__init__-91"><span class="linenos"> 91</span></a>
</span><span id="LogisticRegression.__init__-92"><a href="#LogisticRegression.__init__-92"><span class="linenos"> 92</span></a><span class="sd">        `n_iterations` : int, optional</span>
</span><span id="LogisticRegression.__init__-93"><a href="#LogisticRegression.__init__-93"><span class="linenos"> 93</span></a><span class="sd">        - The number of iterations, by default 1000</span>
</span><span id="LogisticRegression.__init__-94"><a href="#LogisticRegression.__init__-94"><span class="linenos"> 94</span></a><span class="sd">        - The number of iterations determines how many times the weights are updated</span>
</span><span id="LogisticRegression.__init__-95"><a href="#LogisticRegression.__init__-95"><span class="linenos"> 95</span></a><span class="sd">        - A higher number of iterations will take longer to converge, but a lower number of iterations may not be enough to converge</span>
</span><span id="LogisticRegression.__init__-96"><a href="#LogisticRegression.__init__-96"><span class="linenos"> 96</span></a>
</span><span id="LogisticRegression.__init__-97"><a href="#LogisticRegression.__init__-97"><span class="linenos"> 97</span></a><span class="sd">        `lambda_` : np.float64, optional</span>
</span><span id="LogisticRegression.__init__-98"><a href="#LogisticRegression.__init__-98"><span class="linenos"> 98</span></a><span class="sd">        - The regularization parameter, by default 0</span>
</span><span id="LogisticRegression.__init__-99"><a href="#LogisticRegression.__init__-99"><span class="linenos"> 99</span></a><span class="sd">        - The regularization parameter helps prevent overfitting by penalizing large weights</span>
</span><span id="LogisticRegression.__init__-100"><a href="#LogisticRegression.__init__-100"><span class="linenos">100</span></a><span class="sd">        - A higher regularization parameter will penalize large weights more, but a lower regularization parameter may not be enough to prevent overfitting</span>
</span><span id="LogisticRegression.__init__-101"><a href="#LogisticRegression.__init__-101"><span class="linenos">101</span></a>
</span><span id="LogisticRegression.__init__-102"><a href="#LogisticRegression.__init__-102"><span class="linenos">102</span></a><span class="sd">        `x_scalar` : Union[IFeatureEngineering, List[IFeatureEngineering]], optional</span>
</span><span id="LogisticRegression.__init__-103"><a href="#LogisticRegression.__init__-103"><span class="linenos">103</span></a><span class="sd">        - The feature engineering for the input data, by default None</span>
</span><span id="LogisticRegression.__init__-104"><a href="#LogisticRegression.__init__-104"><span class="linenos">104</span></a><span class="sd">        - If a list is provided, the feature engineering will be applied in the order provided</span>
</span><span id="LogisticRegression.__init__-105"><a href="#LogisticRegression.__init__-105"><span class="linenos">105</span></a><span class="sd">        - If a single feature engineering is provided, it will be applied to all input data</span>
</span><span id="LogisticRegression.__init__-106"><a href="#LogisticRegression.__init__-106"><span class="linenos">106</span></a>
</span><span id="LogisticRegression.__init__-107"><a href="#LogisticRegression.__init__-107"><span class="linenos">107</span></a><span class="sd">        `debug` : bool, optional</span>
</span><span id="LogisticRegression.__init__-108"><a href="#LogisticRegression.__init__-108"><span class="linenos">108</span></a><span class="sd">        - Whether to print debug messages, by default True</span>
</span><span id="LogisticRegression.__init__-109"><a href="#LogisticRegression.__init__-109"><span class="linenos">109</span></a><span class="sd">        - Debug messages include the cost at each iteration</span>
</span><span id="LogisticRegression.__init__-110"><a href="#LogisticRegression.__init__-110"><span class="linenos">110</span></a>
</span><span id="LogisticRegression.__init__-111"><a href="#LogisticRegression.__init__-111"><span class="linenos">111</span></a><span class="sd">        `copy_x` : bool, optional</span>
</span><span id="LogisticRegression.__init__-112"><a href="#LogisticRegression.__init__-112"><span class="linenos">112</span></a><span class="sd">        - Whether to copy the input array, by default True</span>
</span><span id="LogisticRegression.__init__-113"><a href="#LogisticRegression.__init__-113"><span class="linenos">113</span></a><span class="sd">        - If False, the input array will be overwritten</span>
</span><span id="LogisticRegression.__init__-114"><a href="#LogisticRegression.__init__-114"><span class="linenos">114</span></a>
</span><span id="LogisticRegression.__init__-115"><a href="#LogisticRegression.__init__-115"><span class="linenos">115</span></a><span class="sd">        ---</span>
</span><span id="LogisticRegression.__init__-116"><a href="#LogisticRegression.__init__-116"><span class="linenos">116</span></a><span class="sd">        &quot;&quot;&quot;</span>
</span><span id="LogisticRegression.__init__-117"><a href="#LogisticRegression.__init__-117"><span class="linenos">117</span></a>        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span>
</span><span id="LogisticRegression.__init__-118"><a href="#LogisticRegression.__init__-118"><span class="linenos">118</span></a>            <span class="n">learning_rate</span><span class="o">=</span><span class="n">learning_rate</span><span class="p">,</span>
</span><span id="LogisticRegression.__init__-119"><a href="#LogisticRegression.__init__-119"><span class="linenos">119</span></a>            <span class="n">n_iterations</span><span class="o">=</span><span class="n">n_iterations</span><span class="p">,</span>
</span><span id="LogisticRegression.__init__-120"><a href="#LogisticRegression.__init__-120"><span class="linenos">120</span></a>            <span class="n">debug</span><span class="o">=</span><span class="n">debug</span><span class="p">,</span>
</span><span id="LogisticRegression.__init__-121"><a href="#LogisticRegression.__init__-121"><span class="linenos">121</span></a>            <span class="n">copy_x</span><span class="o">=</span><span class="n">copy_x</span><span class="p">,</span>
</span><span id="LogisticRegression.__init__-122"><a href="#LogisticRegression.__init__-122"><span class="linenos">122</span></a>        <span class="p">)</span>
</span><span id="LogisticRegression.__init__-123"><a href="#LogisticRegression.__init__-123"><span class="linenos">123</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">_lambda</span> <span class="o">=</span> <span class="n">lambda_</span>
</span><span id="LogisticRegression.__init__-124"><a href="#LogisticRegression.__init__-124"><span class="linenos">124</span></a>
</span><span id="LogisticRegression.__init__-125"><a href="#LogisticRegression.__init__-125"><span class="linenos">125</span></a>        <span class="k">if</span> <span class="n">x_scalar</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
</span><span id="LogisticRegression.__init__-126"><a href="#LogisticRegression.__init__-126"><span class="linenos">126</span></a>            <span class="n">x_scalar</span> <span class="o">=</span> <span class="p">[]</span>
</span><span id="LogisticRegression.__init__-127"><a href="#LogisticRegression.__init__-127"><span class="linenos">127</span></a>        <span class="k">elif</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">x_scalar</span><span class="p">,</span> <span class="n">IFeatureEngineering</span><span class="p">):</span>
</span><span id="LogisticRegression.__init__-128"><a href="#LogisticRegression.__init__-128"><span class="linenos">128</span></a>            <span class="n">x_scalar</span> <span class="o">=</span> <span class="p">[</span><span class="n">x_scalar</span><span class="p">]</span>
</span><span id="LogisticRegression.__init__-129"><a href="#LogisticRegression.__init__-129"><span class="linenos">129</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">_x_scalar</span> <span class="o">=</span> <span class="n">x_scalar</span>
</span></pre></div>


            <div class="docstring"><h2 id="parameters">Parameters</h2>

<p><code>learning_rate</code> : np.float64, optional</p>

<ul>
<li>The learning rate, by default 0.001</li>
<li>The learning rate determines how much the weights are updated at each iteration</li>
<li>A low learning rate will take longer to converge, but a high learning rate may overshoot the optimal solution</li>
</ul>

<p><code>n_iterations</code> : int, optional</p>

<ul>
<li>The number of iterations, by default 1000</li>
<li>The number of iterations determines how many times the weights are updated</li>
<li>A higher number of iterations will take longer to converge, but a lower number of iterations may not be enough to converge</li>
</ul>

<p><code>lambda_</code> : np.float64, optional</p>

<ul>
<li>The regularization parameter, by default 0</li>
<li>The regularization parameter helps prevent overfitting by penalizing large weights</li>
<li>A higher regularization parameter will penalize large weights more, but a lower regularization parameter may not be enough to prevent overfitting</li>
</ul>

<p><code>x_scalar</code> : Union[IFeatureEngineering, List[IFeatureEngineering]], optional</p>

<ul>
<li>The feature engineering for the input data, by default None</li>
<li>If a list is provided, the feature engineering will be applied in the order provided</li>
<li>If a single feature engineering is provided, it will be applied to all input data</li>
</ul>

<p><code>debug</code> : bool, optional</p>

<ul>
<li>Whether to print debug messages, by default True</li>
<li>Debug messages include the cost at each iteration</li>
</ul>

<p><code>copy_x</code> : bool, optional</p>

<ul>
<li>Whether to copy the input array, by default True</li>
<li>If False, the input array will be overwritten</li>
</ul>

<hr />
</div>


                            </div>
                            <div id="LogisticRegression.fit" class="classattr">
                                        <input id="LogisticRegression.fit-view-source" class="view-source-toggle-state" type="checkbox" aria-hidden="true" tabindex="-1">
<div class="attr function">
            
        <span class="def">def</span>
        <span class="name">fit</span><span class="signature pdoc-code multiline">(<span class="param">	<span class="bp">self</span>,</span><span class="param">	<span class="n">X</span><span class="p">:</span> <span class="n">numpy</span><span class="o">.</span><span class="n">ndarray</span>,</span><span class="param">	<span class="n">Y</span><span class="p">:</span> <span class="n">numpy</span><span class="o">.</span><span class="n">ndarray</span>,</span><span class="param">	<span class="n">W</span><span class="p">:</span> <span class="n">numpy</span><span class="o">.</span><span class="n">ndarray</span> <span class="o">=</span> <span class="kc">None</span>,</span><span class="param">	<span class="n">b</span><span class="p">:</span> <span class="n">numpy</span><span class="o">.</span><span class="n">float64</span> <span class="o">=</span> <span class="mf">0.0</span></span><span class="return-annotation">) -> <span class="kc">None</span>:</span></span>

                <label class="view-source-button" for="LogisticRegression.fit-view-source"><span>View Source</span></label>

    </div>
    <a class="headerlink" href="#LogisticRegression.fit"></a>
            <div class="pdoc-code codehilite"><pre><span></span><span id="LogisticRegression.fit-345"><a href="#LogisticRegression.fit-345"><span class="linenos">345</span></a>    <span class="k">def</span> <span class="nf">fit</span><span class="p">(</span>
</span><span id="LogisticRegression.fit-346"><a href="#LogisticRegression.fit-346"><span class="linenos">346</span></a>        <span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">,</span> <span class="n">Y</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">,</span> <span class="n">W</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span> <span class="n">b</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">float64</span> <span class="o">=</span> <span class="mf">0.0</span>
</span><span id="LogisticRegression.fit-347"><a href="#LogisticRegression.fit-347"><span class="linenos">347</span></a>    <span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
</span><span id="LogisticRegression.fit-348"><a href="#LogisticRegression.fit-348"><span class="linenos">348</span></a><span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
</span><span id="LogisticRegression.fit-349"><a href="#LogisticRegression.fit-349"><span class="linenos">349</span></a><span class="sd">        ### Fit the model to the data.</span>
</span><span id="LogisticRegression.fit-350"><a href="#LogisticRegression.fit-350"><span class="linenos">350</span></a>
</span><span id="LogisticRegression.fit-351"><a href="#LogisticRegression.fit-351"><span class="linenos">351</span></a><span class="sd">        Parameters</span>
</span><span id="LogisticRegression.fit-352"><a href="#LogisticRegression.fit-352"><span class="linenos">352</span></a><span class="sd">        ----------</span>
</span><span id="LogisticRegression.fit-353"><a href="#LogisticRegression.fit-353"><span class="linenos">353</span></a>
</span><span id="LogisticRegression.fit-354"><a href="#LogisticRegression.fit-354"><span class="linenos">354</span></a><span class="sd">        `X` : np.ndarray</span>
</span><span id="LogisticRegression.fit-355"><a href="#LogisticRegression.fit-355"><span class="linenos">355</span></a><span class="sd">        - The input array of shape (n_samples, n_features) or (n_samples,)</span>
</span><span id="LogisticRegression.fit-356"><a href="#LogisticRegression.fit-356"><span class="linenos">356</span></a>
</span><span id="LogisticRegression.fit-357"><a href="#LogisticRegression.fit-357"><span class="linenos">357</span></a><span class="sd">        `Y` : np.ndarray</span>
</span><span id="LogisticRegression.fit-358"><a href="#LogisticRegression.fit-358"><span class="linenos">358</span></a><span class="sd">        - The output array of shape (n_samples,) or (n_samples, 1)</span>
</span><span id="LogisticRegression.fit-359"><a href="#LogisticRegression.fit-359"><span class="linenos">359</span></a>
</span><span id="LogisticRegression.fit-360"><a href="#LogisticRegression.fit-360"><span class="linenos">360</span></a><span class="sd">        `w` : np.ndarray, optional</span>
</span><span id="LogisticRegression.fit-361"><a href="#LogisticRegression.fit-361"><span class="linenos">361</span></a><span class="sd">        - The weight array, by default None</span>
</span><span id="LogisticRegression.fit-362"><a href="#LogisticRegression.fit-362"><span class="linenos">362</span></a><span class="sd">        - If None, then the weight array will be initialized to an array of</span>
</span><span id="LogisticRegression.fit-363"><a href="#LogisticRegression.fit-363"><span class="linenos">363</span></a><span class="sd">            zeros of shape (n_features,)</span>
</span><span id="LogisticRegression.fit-364"><a href="#LogisticRegression.fit-364"><span class="linenos">364</span></a><span class="sd">        - If not None, then the weight array will be initialized to the given</span>
</span><span id="LogisticRegression.fit-365"><a href="#LogisticRegression.fit-365"><span class="linenos">365</span></a><span class="sd">            array</span>
</span><span id="LogisticRegression.fit-366"><a href="#LogisticRegression.fit-366"><span class="linenos">366</span></a>
</span><span id="LogisticRegression.fit-367"><a href="#LogisticRegression.fit-367"><span class="linenos">367</span></a><span class="sd">        `b` : np.float64, optional</span>
</span><span id="LogisticRegression.fit-368"><a href="#LogisticRegression.fit-368"><span class="linenos">368</span></a><span class="sd">        - The intercept, by default 0.0</span>
</span><span id="LogisticRegression.fit-369"><a href="#LogisticRegression.fit-369"><span class="linenos">369</span></a><span class="sd">        - If None, then the intercept will be initialized to 0.0</span>
</span><span id="LogisticRegression.fit-370"><a href="#LogisticRegression.fit-370"><span class="linenos">370</span></a><span class="sd">        - If not None, then the intercept will be initialized to the given</span>
</span><span id="LogisticRegression.fit-371"><a href="#LogisticRegression.fit-371"><span class="linenos">371</span></a><span class="sd">            value</span>
</span><span id="LogisticRegression.fit-372"><a href="#LogisticRegression.fit-372"><span class="linenos">372</span></a>
</span><span id="LogisticRegression.fit-373"><a href="#LogisticRegression.fit-373"><span class="linenos">373</span></a>
</span><span id="LogisticRegression.fit-374"><a href="#LogisticRegression.fit-374"><span class="linenos">374</span></a><span class="sd">        Returns</span>
</span><span id="LogisticRegression.fit-375"><a href="#LogisticRegression.fit-375"><span class="linenos">375</span></a><span class="sd">        -------</span>
</span><span id="LogisticRegression.fit-376"><a href="#LogisticRegression.fit-376"><span class="linenos">376</span></a>
</span><span id="LogisticRegression.fit-377"><a href="#LogisticRegression.fit-377"><span class="linenos">377</span></a><span class="sd">        None</span>
</span><span id="LogisticRegression.fit-378"><a href="#LogisticRegression.fit-378"><span class="linenos">378</span></a>
</span><span id="LogisticRegression.fit-379"><a href="#LogisticRegression.fit-379"><span class="linenos">379</span></a><span class="sd">        ---</span>
</span><span id="LogisticRegression.fit-380"><a href="#LogisticRegression.fit-380"><span class="linenos">380</span></a><span class="sd">        &quot;&quot;&quot;</span>
</span><span id="LogisticRegression.fit-381"><a href="#LogisticRegression.fit-381"><span class="linenos">381</span></a>        <span class="n">X</span><span class="p">,</span> <span class="n">Y</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_validate_data</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">Y</span><span class="p">)</span>
</span><span id="LogisticRegression.fit-382"><a href="#LogisticRegression.fit-382"><span class="linenos">382</span></a>
</span><span id="LogisticRegression.fit-383"><a href="#LogisticRegression.fit-383"><span class="linenos">383</span></a>        <span class="k">assert</span> <span class="n">X</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">==</span> <span class="n">Y</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="s2">&quot;X and Y must have the same number of samples&quot;</span>
</span><span id="LogisticRegression.fit-384"><a href="#LogisticRegression.fit-384"><span class="linenos">384</span></a>
</span><span id="LogisticRegression.fit-385"><a href="#LogisticRegression.fit-385"><span class="linenos">385</span></a>        <span class="c1"># Initialize weights and intercept</span>
</span><span id="LogisticRegression.fit-386"><a href="#LogisticRegression.fit-386"><span class="linenos">386</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">_weights</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">X</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span> <span class="k">if</span> <span class="n">W</span> <span class="ow">is</span> <span class="kc">None</span> <span class="k">else</span> <span class="n">W</span>
</span><span id="LogisticRegression.fit-387"><a href="#LogisticRegression.fit-387"><span class="linenos">387</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">_intercept</span> <span class="o">=</span> <span class="n">b</span>
</span><span id="LogisticRegression.fit-388"><a href="#LogisticRegression.fit-388"><span class="linenos">388</span></a>
</span><span id="LogisticRegression.fit-389"><a href="#LogisticRegression.fit-389"><span class="linenos">389</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">_cost_history</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span>
</span><span id="LogisticRegression.fit-390"><a href="#LogisticRegression.fit-390"><span class="linenos">390</span></a>            <span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">_cost</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">Y</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">_weights</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">_intercept</span><span class="p">)]</span>
</span><span id="LogisticRegression.fit-391"><a href="#LogisticRegression.fit-391"><span class="linenos">391</span></a>        <span class="p">)</span>
</span><span id="LogisticRegression.fit-392"><a href="#LogisticRegression.fit-392"><span class="linenos">392</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">_params_history</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span>
</span><span id="LogisticRegression.fit-393"><a href="#LogisticRegression.fit-393"><span class="linenos">393</span></a>            <span class="p">[[</span><span class="bp">self</span><span class="o">.</span><span class="n">_weights</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">_intercept</span><span class="p">]],</span> <span class="n">dtype</span><span class="o">=</span><span class="nb">object</span>
</span><span id="LogisticRegression.fit-394"><a href="#LogisticRegression.fit-394"><span class="linenos">394</span></a>        <span class="p">)</span>
</span><span id="LogisticRegression.fit-395"><a href="#LogisticRegression.fit-395"><span class="linenos">395</span></a>
</span><span id="LogisticRegression.fit-396"><a href="#LogisticRegression.fit-396"><span class="linenos">396</span></a>        <span class="c1"># Iterate and update weights and intercept</span>
</span><span id="LogisticRegression.fit-397"><a href="#LogisticRegression.fit-397"><span class="linenos">397</span></a>        <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_n_iterations</span><span class="p">):</span>
</span><span id="LogisticRegression.fit-398"><a href="#LogisticRegression.fit-398"><span class="linenos">398</span></a>            <span class="c1"># Calculate gradient</span>
</span><span id="LogisticRegression.fit-399"><a href="#LogisticRegression.fit-399"><span class="linenos">399</span></a>            <span class="n">dw</span><span class="p">,</span> <span class="n">db</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_gradient</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">Y</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">_weights</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">_intercept</span><span class="p">)</span>
</span><span id="LogisticRegression.fit-400"><a href="#LogisticRegression.fit-400"><span class="linenos">400</span></a>
</span><span id="LogisticRegression.fit-401"><a href="#LogisticRegression.fit-401"><span class="linenos">401</span></a>            <span class="c1"># Update weights and intercept</span>
</span><span id="LogisticRegression.fit-402"><a href="#LogisticRegression.fit-402"><span class="linenos">402</span></a>            <span class="bp">self</span><span class="o">.</span><span class="n">_weights</span> <span class="o">-=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_learning_rate</span> <span class="o">*</span> <span class="n">dw</span>
</span><span id="LogisticRegression.fit-403"><a href="#LogisticRegression.fit-403"><span class="linenos">403</span></a>            <span class="bp">self</span><span class="o">.</span><span class="n">_intercept</span> <span class="o">-=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_learning_rate</span> <span class="o">*</span> <span class="n">db</span>
</span><span id="LogisticRegression.fit-404"><a href="#LogisticRegression.fit-404"><span class="linenos">404</span></a>
</span><span id="LogisticRegression.fit-405"><a href="#LogisticRegression.fit-405"><span class="linenos">405</span></a>            <span class="c1"># Save cost and params history</span>
</span><span id="LogisticRegression.fit-406"><a href="#LogisticRegression.fit-406"><span class="linenos">406</span></a>            <span class="n">cost</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_cost</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">Y</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">_weights</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">_intercept</span><span class="p">)</span>
</span><span id="LogisticRegression.fit-407"><a href="#LogisticRegression.fit-407"><span class="linenos">407</span></a>
</span><span id="LogisticRegression.fit-408"><a href="#LogisticRegression.fit-408"><span class="linenos">408</span></a>            <span class="k">if</span> <span class="n">cost</span> <span class="o">==</span> <span class="n">np</span><span class="o">.</span><span class="n">nan</span> <span class="ow">or</span> <span class="n">cost</span> <span class="o">==</span> <span class="n">np</span><span class="o">.</span><span class="n">inf</span><span class="p">:</span>
</span><span id="LogisticRegression.fit-409"><a href="#LogisticRegression.fit-409"><span class="linenos">409</span></a>                <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
</span><span id="LogisticRegression.fit-410"><a href="#LogisticRegression.fit-410"><span class="linenos">410</span></a>                    <span class="s2">&quot;Gradient descent failed. Try normalizing the input array or reducing the learning rate. &quot;</span>
</span><span id="LogisticRegression.fit-411"><a href="#LogisticRegression.fit-411"><span class="linenos">411</span></a>                    <span class="s2">&quot;If the problem persists, try reducing the number of iterations.&quot;</span>
</span><span id="LogisticRegression.fit-412"><a href="#LogisticRegression.fit-412"><span class="linenos">412</span></a>                <span class="p">)</span>
</span><span id="LogisticRegression.fit-413"><a href="#LogisticRegression.fit-413"><span class="linenos">413</span></a>
</span><span id="LogisticRegression.fit-414"><a href="#LogisticRegression.fit-414"><span class="linenos">414</span></a>            <span class="bp">self</span><span class="o">.</span><span class="n">_cost_history</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_cost_history</span><span class="p">,</span> <span class="n">cost</span><span class="p">)</span>
</span><span id="LogisticRegression.fit-415"><a href="#LogisticRegression.fit-415"><span class="linenos">415</span></a>            <span class="bp">self</span><span class="o">.</span><span class="n">_params_history</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">append</span><span class="p">(</span>
</span><span id="LogisticRegression.fit-416"><a href="#LogisticRegression.fit-416"><span class="linenos">416</span></a>                <span class="bp">self</span><span class="o">.</span><span class="n">_params_history</span><span class="p">,</span>
</span><span id="LogisticRegression.fit-417"><a href="#LogisticRegression.fit-417"><span class="linenos">417</span></a>                <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([[</span><span class="bp">self</span><span class="o">.</span><span class="n">_weights</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">_intercept</span><span class="p">]],</span> <span class="n">dtype</span><span class="o">=</span><span class="nb">object</span><span class="p">),</span>
</span><span id="LogisticRegression.fit-418"><a href="#LogisticRegression.fit-418"><span class="linenos">418</span></a>                <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span>
</span><span id="LogisticRegression.fit-419"><a href="#LogisticRegression.fit-419"><span class="linenos">419</span></a>            <span class="p">)</span>
</span><span id="LogisticRegression.fit-420"><a href="#LogisticRegression.fit-420"><span class="linenos">420</span></a>
</span><span id="LogisticRegression.fit-421"><a href="#LogisticRegression.fit-421"><span class="linenos">421</span></a>            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_debug</span> <span class="ow">and</span> <span class="n">i</span> <span class="o">%</span> <span class="bp">self</span><span class="o">.</span><span class="n">_debug_freq</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
</span><span id="LogisticRegression.fit-422"><a href="#LogisticRegression.fit-422"><span class="linenos">422</span></a>                <span class="bp">self</span><span class="o">.</span><span class="n">_debug_print</span><span class="p">(</span><span class="n">i</span><span class="p">,</span> <span class="n">cost</span><span class="p">)</span>
</span><span id="LogisticRegression.fit-423"><a href="#LogisticRegression.fit-423"><span class="linenos">423</span></a>
</span><span id="LogisticRegression.fit-424"><a href="#LogisticRegression.fit-424"><span class="linenos">424</span></a>        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_debug</span><span class="p">:</span>
</span><span id="LogisticRegression.fit-425"><a href="#LogisticRegression.fit-425"><span class="linenos">425</span></a>            <span class="bp">self</span><span class="o">.</span><span class="n">_debug_print</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_n_iterations</span><span class="p">,</span> <span class="n">cost</span><span class="p">)</span>
</span></pre></div>


            <div class="docstring"><h3 id="fit-the-model-to-the-data">Fit the model to the data.</h3>

<h2 id="parameters">Parameters</h2>

<p><code>X</code> : np.ndarray</p>

<ul>
<li>The input array of shape (n_samples, n_features) or (n_samples,)</li>
</ul>

<p><code>Y</code> : np.ndarray</p>

<ul>
<li>The output array of shape (n_samples,) or (n_samples, 1)</li>
</ul>

<p><code>w</code> : np.ndarray, optional</p>

<ul>
<li>The weight array, by default None</li>
<li>If None, then the weight array will be initialized to an array of
zeros of shape (n_features,)</li>
<li>If not None, then the weight array will be initialized to the given
array</li>
</ul>

<p><code>b</code> : np.float64, optional</p>

<ul>
<li>The intercept, by default 0.0</li>
<li>If None, then the intercept will be initialized to 0.0</li>
<li>If not None, then the intercept will be initialized to the given
value</li>
</ul>

<h2 id="returns">Returns</h2>

<p>None</p>

<hr />
</div>


                            </div>
                            <div id="LogisticRegression.predict_proba" class="classattr">
                                        <input id="LogisticRegression.predict_proba-view-source" class="view-source-toggle-state" type="checkbox" aria-hidden="true" tabindex="-1">
<div class="attr function">
            
        <span class="def">def</span>
        <span class="name">predict_proba</span><span class="signature pdoc-code condensed">(<span class="param"><span class="bp">self</span>, </span><span class="param"><span class="n">X</span><span class="p">:</span> <span class="n">numpy</span><span class="o">.</span><span class="n">ndarray</span></span><span class="return-annotation">) -> <span class="n">numpy</span><span class="o">.</span><span class="n">ndarray</span>:</span></span>

                <label class="view-source-button" for="LogisticRegression.predict_proba-view-source"><span>View Source</span></label>

    </div>
    <a class="headerlink" href="#LogisticRegression.predict_proba"></a>
            <div class="pdoc-code codehilite"><pre><span></span><span id="LogisticRegression.predict_proba-427"><a href="#LogisticRegression.predict_proba-427"><span class="linenos">427</span></a>    <span class="k">def</span> <span class="nf">predict_proba</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">:</span>
</span><span id="LogisticRegression.predict_proba-428"><a href="#LogisticRegression.predict_proba-428"><span class="linenos">428</span></a><span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
</span><span id="LogisticRegression.predict_proba-429"><a href="#LogisticRegression.predict_proba-429"><span class="linenos">429</span></a><span class="sd">        ### Predict the probability of the output given the input.</span>
</span><span id="LogisticRegression.predict_proba-430"><a href="#LogisticRegression.predict_proba-430"><span class="linenos">430</span></a>
</span><span id="LogisticRegression.predict_proba-431"><a href="#LogisticRegression.predict_proba-431"><span class="linenos">431</span></a><span class="sd">        Parameters</span>
</span><span id="LogisticRegression.predict_proba-432"><a href="#LogisticRegression.predict_proba-432"><span class="linenos">432</span></a><span class="sd">        ----------</span>
</span><span id="LogisticRegression.predict_proba-433"><a href="#LogisticRegression.predict_proba-433"><span class="linenos">433</span></a>
</span><span id="LogisticRegression.predict_proba-434"><a href="#LogisticRegression.predict_proba-434"><span class="linenos">434</span></a><span class="sd">        `X` : np.ndarray</span>
</span><span id="LogisticRegression.predict_proba-435"><a href="#LogisticRegression.predict_proba-435"><span class="linenos">435</span></a><span class="sd">        - The input array of shape (n_samples, n_features) or (n_features,)</span>
</span><span id="LogisticRegression.predict_proba-436"><a href="#LogisticRegression.predict_proba-436"><span class="linenos">436</span></a>
</span><span id="LogisticRegression.predict_proba-437"><a href="#LogisticRegression.predict_proba-437"><span class="linenos">437</span></a>
</span><span id="LogisticRegression.predict_proba-438"><a href="#LogisticRegression.predict_proba-438"><span class="linenos">438</span></a><span class="sd">        Returns</span>
</span><span id="LogisticRegression.predict_proba-439"><a href="#LogisticRegression.predict_proba-439"><span class="linenos">439</span></a><span class="sd">        -------</span>
</span><span id="LogisticRegression.predict_proba-440"><a href="#LogisticRegression.predict_proba-440"><span class="linenos">440</span></a>
</span><span id="LogisticRegression.predict_proba-441"><a href="#LogisticRegression.predict_proba-441"><span class="linenos">441</span></a><span class="sd">        `np.ndarray`</span>
</span><span id="LogisticRegression.predict_proba-442"><a href="#LogisticRegression.predict_proba-442"><span class="linenos">442</span></a><span class="sd">        - The predicted output array of shape (n_samples,) or (1,)</span>
</span><span id="LogisticRegression.predict_proba-443"><a href="#LogisticRegression.predict_proba-443"><span class="linenos">443</span></a>
</span><span id="LogisticRegression.predict_proba-444"><a href="#LogisticRegression.predict_proba-444"><span class="linenos">444</span></a><span class="sd">        ---</span>
</span><span id="LogisticRegression.predict_proba-445"><a href="#LogisticRegression.predict_proba-445"><span class="linenos">445</span></a><span class="sd">        &quot;&quot;&quot;</span>
</span><span id="LogisticRegression.predict_proba-446"><a href="#LogisticRegression.predict_proba-446"><span class="linenos">446</span></a>        <span class="k">assert</span> <span class="bp">self</span><span class="o">.</span><span class="n">_weights</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">and</span> <span class="bp">self</span><span class="o">.</span><span class="n">_intercept</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">,</span> <span class="p">(</span>
</span><span id="LogisticRegression.predict_proba-447"><a href="#LogisticRegression.predict_proba-447"><span class="linenos">447</span></a>            <span class="s2">&quot;The model must be trained before making predictions. &quot;</span>
</span><span id="LogisticRegression.predict_proba-448"><a href="#LogisticRegression.predict_proba-448"><span class="linenos">448</span></a>            <span class="s2">&quot;Call the fit method first.&quot;</span>
</span><span id="LogisticRegression.predict_proba-449"><a href="#LogisticRegression.predict_proba-449"><span class="linenos">449</span></a>        <span class="p">)</span>
</span><span id="LogisticRegression.predict_proba-450"><a href="#LogisticRegression.predict_proba-450"><span class="linenos">450</span></a>
</span><span id="LogisticRegression.predict_proba-451"><a href="#LogisticRegression.predict_proba-451"><span class="linenos">451</span></a>        <span class="n">X</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_validate_data</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
</span><span id="LogisticRegression.predict_proba-452"><a href="#LogisticRegression.predict_proba-452"><span class="linenos">452</span></a>        <span class="c1"># Return the probability of the output being 1</span>
</span><span id="LogisticRegression.predict_proba-453"><a href="#LogisticRegression.predict_proba-453"><span class="linenos">453</span></a>        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_y_hat</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">_weights</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">_intercept</span><span class="p">)</span>
</span></pre></div>


            <div class="docstring"><h3 id="predict-the-probability-of-the-output-given-the-input">Predict the probability of the output given the input.</h3>

<h2 id="parameters">Parameters</h2>

<p><code>X</code> : np.ndarray</p>

<ul>
<li>The input array of shape (n_samples, n_features) or (n_features,)</li>
</ul>

<h2 id="returns">Returns</h2>

<p><code>np.ndarray</code></p>

<ul>
<li>The predicted output array of shape (n_samples,) or (1,)</li>
</ul>

<hr />
</div>


                            </div>
                            <div id="LogisticRegression.predict" class="classattr">
                                        <input id="LogisticRegression.predict-view-source" class="view-source-toggle-state" type="checkbox" aria-hidden="true" tabindex="-1">
<div class="attr function">
            
        <span class="def">def</span>
        <span class="name">predict</span><span class="signature pdoc-code condensed">(<span class="param"><span class="bp">self</span>, </span><span class="param"><span class="n">X</span><span class="p">:</span> <span class="n">numpy</span><span class="o">.</span><span class="n">ndarray</span></span><span class="return-annotation">) -> <span class="n">numpy</span><span class="o">.</span><span class="n">ndarray</span>:</span></span>

                <label class="view-source-button" for="LogisticRegression.predict-view-source"><span>View Source</span></label>

    </div>
    <a class="headerlink" href="#LogisticRegression.predict"></a>
            <div class="pdoc-code codehilite"><pre><span></span><span id="LogisticRegression.predict-455"><a href="#LogisticRegression.predict-455"><span class="linenos">455</span></a>    <span class="k">def</span> <span class="nf">predict</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">:</span>
</span><span id="LogisticRegression.predict-456"><a href="#LogisticRegression.predict-456"><span class="linenos">456</span></a><span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
</span><span id="LogisticRegression.predict-457"><a href="#LogisticRegression.predict-457"><span class="linenos">457</span></a><span class="sd">        ### Predict the output given the input.</span>
</span><span id="LogisticRegression.predict-458"><a href="#LogisticRegression.predict-458"><span class="linenos">458</span></a>
</span><span id="LogisticRegression.predict-459"><a href="#LogisticRegression.predict-459"><span class="linenos">459</span></a><span class="sd">        Parameters</span>
</span><span id="LogisticRegression.predict-460"><a href="#LogisticRegression.predict-460"><span class="linenos">460</span></a><span class="sd">        ----------</span>
</span><span id="LogisticRegression.predict-461"><a href="#LogisticRegression.predict-461"><span class="linenos">461</span></a>
</span><span id="LogisticRegression.predict-462"><a href="#LogisticRegression.predict-462"><span class="linenos">462</span></a><span class="sd">        `X` : np.ndarray</span>
</span><span id="LogisticRegression.predict-463"><a href="#LogisticRegression.predict-463"><span class="linenos">463</span></a><span class="sd">        - The input array of shape (n_samples, n_features) or (n_features,)</span>
</span><span id="LogisticRegression.predict-464"><a href="#LogisticRegression.predict-464"><span class="linenos">464</span></a>
</span><span id="LogisticRegression.predict-465"><a href="#LogisticRegression.predict-465"><span class="linenos">465</span></a>
</span><span id="LogisticRegression.predict-466"><a href="#LogisticRegression.predict-466"><span class="linenos">466</span></a><span class="sd">        Returns</span>
</span><span id="LogisticRegression.predict-467"><a href="#LogisticRegression.predict-467"><span class="linenos">467</span></a><span class="sd">        -------</span>
</span><span id="LogisticRegression.predict-468"><a href="#LogisticRegression.predict-468"><span class="linenos">468</span></a>
</span><span id="LogisticRegression.predict-469"><a href="#LogisticRegression.predict-469"><span class="linenos">469</span></a><span class="sd">        `np.ndarray`</span>
</span><span id="LogisticRegression.predict-470"><a href="#LogisticRegression.predict-470"><span class="linenos">470</span></a><span class="sd">        - The predicted output array of shape (n_samples,) or (1,)</span>
</span><span id="LogisticRegression.predict-471"><a href="#LogisticRegression.predict-471"><span class="linenos">471</span></a>
</span><span id="LogisticRegression.predict-472"><a href="#LogisticRegression.predict-472"><span class="linenos">472</span></a><span class="sd">        ---</span>
</span><span id="LogisticRegression.predict-473"><a href="#LogisticRegression.predict-473"><span class="linenos">473</span></a><span class="sd">        &quot;&quot;&quot;</span>
</span><span id="LogisticRegression.predict-474"><a href="#LogisticRegression.predict-474"><span class="linenos">474</span></a>        <span class="c1"># Return 1 if the probability of the output being 1 is greater than or equal to 0.5</span>
</span><span id="LogisticRegression.predict-475"><a href="#LogisticRegression.predict-475"><span class="linenos">475</span></a>        <span class="c1"># Return 0 otherwise</span>
</span><span id="LogisticRegression.predict-476"><a href="#LogisticRegression.predict-476"><span class="linenos">476</span></a>        <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">where</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">predict_proba</span><span class="p">(</span><span class="n">X</span><span class="p">)</span> <span class="o">&gt;=</span> <span class="mf">0.5</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">)</span>
</span></pre></div>


            <div class="docstring"><h3 id="predict-the-output-given-the-input">Predict the output given the input.</h3>

<h2 id="parameters">Parameters</h2>

<p><code>X</code> : np.ndarray</p>

<ul>
<li>The input array of shape (n_samples, n_features) or (n_features,)</li>
</ul>

<h2 id="returns">Returns</h2>

<p><code>np.ndarray</code></p>

<ul>
<li>The predicted output array of shape (n_samples,) or (1,)</li>
</ul>

<hr />
</div>


                            </div>
                            <div id="LogisticRegression.score" class="classattr">
                                        <input id="LogisticRegression.score-view-source" class="view-source-toggle-state" type="checkbox" aria-hidden="true" tabindex="-1">
<div class="attr function">
            
        <span class="def">def</span>
        <span class="name">score</span><span class="signature pdoc-code multiline">(<span class="param">	<span class="bp">self</span>,</span><span class="param">	<span class="n">X</span><span class="p">:</span> <span class="n">numpy</span><span class="o">.</span><span class="n">ndarray</span>,</span><span class="param">	<span class="n">Y</span><span class="p">:</span> <span class="n">numpy</span><span class="o">.</span><span class="n">ndarray</span>,</span><span class="param">	<span class="n">W</span><span class="p">:</span> <span class="n">numpy</span><span class="o">.</span><span class="n">ndarray</span> <span class="o">=</span> <span class="kc">None</span>,</span><span class="param">	<span class="n">b</span><span class="p">:</span> <span class="n">numpy</span><span class="o">.</span><span class="n">float64</span> <span class="o">=</span> <span class="kc">None</span></span><span class="return-annotation">) -> <span class="n">numpy</span><span class="o">.</span><span class="n">float64</span>:</span></span>

                <label class="view-source-button" for="LogisticRegression.score-view-source"><span>View Source</span></label>

    </div>
    <a class="headerlink" href="#LogisticRegression.score"></a>
            <div class="pdoc-code codehilite"><pre><span></span><span id="LogisticRegression.score-478"><a href="#LogisticRegression.score-478"><span class="linenos">478</span></a>    <span class="k">def</span> <span class="nf">score</span><span class="p">(</span>
</span><span id="LogisticRegression.score-479"><a href="#LogisticRegression.score-479"><span class="linenos">479</span></a>        <span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">,</span> <span class="n">Y</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">,</span> <span class="n">W</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span> <span class="n">b</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">float64</span> <span class="o">=</span> <span class="kc">None</span>
</span><span id="LogisticRegression.score-480"><a href="#LogisticRegression.score-480"><span class="linenos">480</span></a>    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">np</span><span class="o">.</span><span class="n">float64</span><span class="p">:</span>
</span><span id="LogisticRegression.score-481"><a href="#LogisticRegression.score-481"><span class="linenos">481</span></a><span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
</span><span id="LogisticRegression.score-482"><a href="#LogisticRegression.score-482"><span class="linenos">482</span></a><span class="sd">        ### Return the cost of the model given X, Y, W, and b.</span>
</span><span id="LogisticRegression.score-483"><a href="#LogisticRegression.score-483"><span class="linenos">483</span></a>
</span><span id="LogisticRegression.score-484"><a href="#LogisticRegression.score-484"><span class="linenos">484</span></a><span class="sd">        Parameters</span>
</span><span id="LogisticRegression.score-485"><a href="#LogisticRegression.score-485"><span class="linenos">485</span></a><span class="sd">        ----------</span>
</span><span id="LogisticRegression.score-486"><a href="#LogisticRegression.score-486"><span class="linenos">486</span></a><span class="sd">        `X` : np.ndarray</span>
</span><span id="LogisticRegression.score-487"><a href="#LogisticRegression.score-487"><span class="linenos">487</span></a><span class="sd">        - The input array of shape (n_samples, n_features)</span>
</span><span id="LogisticRegression.score-488"><a href="#LogisticRegression.score-488"><span class="linenos">488</span></a>
</span><span id="LogisticRegression.score-489"><a href="#LogisticRegression.score-489"><span class="linenos">489</span></a><span class="sd">        `Y` : np.ndarray</span>
</span><span id="LogisticRegression.score-490"><a href="#LogisticRegression.score-490"><span class="linenos">490</span></a><span class="sd">        - The output array of shape (n_samples,)</span>
</span><span id="LogisticRegression.score-491"><a href="#LogisticRegression.score-491"><span class="linenos">491</span></a>
</span><span id="LogisticRegression.score-492"><a href="#LogisticRegression.score-492"><span class="linenos">492</span></a><span class="sd">        `W` : np.ndarray, optional</span>
</span><span id="LogisticRegression.score-493"><a href="#LogisticRegression.score-493"><span class="linenos">493</span></a><span class="sd">        - The weight array of shape (n_features,), by default None</span>
</span><span id="LogisticRegression.score-494"><a href="#LogisticRegression.score-494"><span class="linenos">494</span></a><span class="sd">        - If None, then the weight array will be cosidered as the trained</span>
</span><span id="LogisticRegression.score-495"><a href="#LogisticRegression.score-495"><span class="linenos">495</span></a><span class="sd">            weight array</span>
</span><span id="LogisticRegression.score-496"><a href="#LogisticRegression.score-496"><span class="linenos">496</span></a>
</span><span id="LogisticRegression.score-497"><a href="#LogisticRegression.score-497"><span class="linenos">497</span></a><span class="sd">        `b` : np.float64, optional</span>
</span><span id="LogisticRegression.score-498"><a href="#LogisticRegression.score-498"><span class="linenos">498</span></a><span class="sd">        - The intercept, by default None</span>
</span><span id="LogisticRegression.score-499"><a href="#LogisticRegression.score-499"><span class="linenos">499</span></a><span class="sd">        - If None, then the intercept will be cosidered as the trained</span>
</span><span id="LogisticRegression.score-500"><a href="#LogisticRegression.score-500"><span class="linenos">500</span></a><span class="sd">            intercept</span>
</span><span id="LogisticRegression.score-501"><a href="#LogisticRegression.score-501"><span class="linenos">501</span></a>
</span><span id="LogisticRegression.score-502"><a href="#LogisticRegression.score-502"><span class="linenos">502</span></a>
</span><span id="LogisticRegression.score-503"><a href="#LogisticRegression.score-503"><span class="linenos">503</span></a><span class="sd">        Returns</span>
</span><span id="LogisticRegression.score-504"><a href="#LogisticRegression.score-504"><span class="linenos">504</span></a><span class="sd">        -------</span>
</span><span id="LogisticRegression.score-505"><a href="#LogisticRegression.score-505"><span class="linenos">505</span></a><span class="sd">        `np.float64`</span>
</span><span id="LogisticRegression.score-506"><a href="#LogisticRegression.score-506"><span class="linenos">506</span></a><span class="sd">        - The cost of the model</span>
</span><span id="LogisticRegression.score-507"><a href="#LogisticRegression.score-507"><span class="linenos">507</span></a>
</span><span id="LogisticRegression.score-508"><a href="#LogisticRegression.score-508"><span class="linenos">508</span></a><span class="sd">        ---</span>
</span><span id="LogisticRegression.score-509"><a href="#LogisticRegression.score-509"><span class="linenos">509</span></a><span class="sd">        &quot;&quot;&quot;</span>
</span><span id="LogisticRegression.score-510"><a href="#LogisticRegression.score-510"><span class="linenos">510</span></a>        <span class="n">X</span><span class="p">,</span> <span class="n">Y</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_validate_data</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">Y</span><span class="p">)</span>
</span><span id="LogisticRegression.score-511"><a href="#LogisticRegression.score-511"><span class="linenos">511</span></a>
</span><span id="LogisticRegression.score-512"><a href="#LogisticRegression.score-512"><span class="linenos">512</span></a>        <span class="n">W</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_weights</span> <span class="k">if</span> <span class="n">W</span> <span class="ow">is</span> <span class="kc">None</span> <span class="k">else</span> <span class="n">W</span>
</span><span id="LogisticRegression.score-513"><a href="#LogisticRegression.score-513"><span class="linenos">513</span></a>        <span class="n">b</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_intercept</span> <span class="k">if</span> <span class="n">b</span> <span class="ow">is</span> <span class="kc">None</span> <span class="k">else</span> <span class="n">b</span>
</span><span id="LogisticRegression.score-514"><a href="#LogisticRegression.score-514"><span class="linenos">514</span></a>
</span><span id="LogisticRegression.score-515"><a href="#LogisticRegression.score-515"><span class="linenos">515</span></a>        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_cost</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">Y</span><span class="p">,</span> <span class="n">W</span><span class="p">,</span> <span class="n">b</span><span class="p">)</span>
</span></pre></div>


            <div class="docstring"><h3 id="return-the-cost-of-the-model-given-x-y-w-and-b">Return the cost of the model given X, Y, W, and b.</h3>

<h2 id="parameters">Parameters</h2>

<p><code>X</code> : np.ndarray</p>

<ul>
<li>The input array of shape (n_samples, n_features)</li>
</ul>

<p><code>Y</code> : np.ndarray</p>

<ul>
<li>The output array of shape (n_samples,)</li>
</ul>

<p><code>W</code> : np.ndarray, optional</p>

<ul>
<li>The weight array of shape (n_features,), by default None</li>
<li>If None, then the weight array will be cosidered as the trained
weight array</li>
</ul>

<p><code>b</code> : np.float64, optional</p>

<ul>
<li>The intercept, by default None</li>
<li>If None, then the intercept will be cosidered as the trained
intercept</li>
</ul>

<h2 id="returns">Returns</h2>

<p><code>np.float64</code></p>

<ul>
<li>The cost of the model</li>
</ul>

<hr />
</div>


                            </div>
                            <div class="inherited">
                                <h5>Inherited Members</h5>
                                <dl>
                                    <div><dt>learnML.interfaces.iregression.IRegression</dt>
                                <dd id="LogisticRegression.get_cost_history" class="function">get_cost_history</dd>
                <dd id="LogisticRegression.get_parameter_history" class="function">get_parameter_history</dd>
                <dd id="LogisticRegression.get_weights" class="function">get_weights</dd>
                <dd id="LogisticRegression.get_intercept" class="function">get_intercept</dd>

            </div>
                                </dl>
                            </div>
                </section>
                <section id="LinearSVC">
                            <input id="LinearSVC-view-source" class="view-source-toggle-state" type="checkbox" aria-hidden="true" tabindex="-1">
<div class="attr class">
            
    <span class="def">class</span>
    <span class="name">LinearSVC</span><wbr>(<span class="base">learnML.interfaces.iregression.IRegression</span>):

                <label class="view-source-button" for="LinearSVC-view-source"><span>View Source</span></label>

    </div>
    <a class="headerlink" href="#LinearSVC"></a>
            <div class="pdoc-code codehilite"><pre><span></span><span id="LinearSVC-8"><a href="#LinearSVC-8"><span class="linenos">  8</span></a><span class="k">class</span> <span class="nc">LinearSVC</span><span class="p">(</span><span class="n">IRegression</span><span class="p">):</span>
</span><span id="LinearSVC-9"><a href="#LinearSVC-9"><span class="linenos">  9</span></a><span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
</span><span id="LinearSVC-10"><a href="#LinearSVC-10"><span class="linenos"> 10</span></a><span class="sd">    # Support Vector Classifier</span>
</span><span id="LinearSVC-11"><a href="#LinearSVC-11"><span class="linenos"> 11</span></a>
</span><span id="LinearSVC-12"><a href="#LinearSVC-12"><span class="linenos"> 12</span></a><span class="sd">    `LinearSVC` (Support Vector Classifier) is a classification algorithm that aims to separate data into two classes by finding a hyperplane that maximizes the margin between them. It&#39;s effective for high-dimensional data and can handle small sample sizes efficiently. This algorithm seeks to create a decision boundary by minimizing classification errors and maximizing the margin between the classes.</span>
</span><span id="LinearSVC-13"><a href="#LinearSVC-13"><span class="linenos"> 13</span></a>
</span><span id="LinearSVC-14"><a href="#LinearSVC-14"><span class="linenos"> 14</span></a><span class="sd">    ---</span>
</span><span id="LinearSVC-15"><a href="#LinearSVC-15"><span class="linenos"> 15</span></a>
</span><span id="LinearSVC-16"><a href="#LinearSVC-16"><span class="linenos"> 16</span></a><span class="sd">    ## Mathematical Approach</span>
</span><span id="LinearSVC-17"><a href="#LinearSVC-17"><span class="linenos"> 17</span></a>
</span><span id="LinearSVC-18"><a href="#LinearSVC-18"><span class="linenos"> 18</span></a><span class="sd">    `LinearSVC` aims to find a hyperplane that best separates two classes by minimizing a loss function known as the hinge loss. The decision boundary is represented as a linear combination of input features, weights, and an intercept:</span>
</span><span id="LinearSVC-19"><a href="#LinearSVC-19"><span class="linenos"> 19</span></a>
</span><span id="LinearSVC-20"><a href="#LinearSVC-20"><span class="linenos"> 20</span></a><span class="sd">    ```</span>
</span><span id="LinearSVC-21"><a href="#LinearSVC-21"><span class="linenos"> 21</span></a><span class="sd">    z = X * W - b</span>
</span><span id="LinearSVC-22"><a href="#LinearSVC-22"><span class="linenos"> 22</span></a><span class="sd">    ```</span>
</span><span id="LinearSVC-23"><a href="#LinearSVC-23"><span class="linenos"> 23</span></a>
</span><span id="LinearSVC-24"><a href="#LinearSVC-24"><span class="linenos"> 24</span></a><span class="sd">    Where:</span>
</span><span id="LinearSVC-25"><a href="#LinearSVC-25"><span class="linenos"> 25</span></a>
</span><span id="LinearSVC-26"><a href="#LinearSVC-26"><span class="linenos"> 26</span></a><span class="sd">    - `X` is the input data matrix of shape `(n_samples, n_features)`.</span>
</span><span id="LinearSVC-27"><a href="#LinearSVC-27"><span class="linenos"> 27</span></a><span class="sd">    - `W` is the weight vector of shape `(n_features,)`.</span>
</span><span id="LinearSVC-28"><a href="#LinearSVC-28"><span class="linenos"> 28</span></a><span class="sd">    - `b` is the intercept.</span>
</span><span id="LinearSVC-29"><a href="#LinearSVC-29"><span class="linenos"> 29</span></a>
</span><span id="LinearSVC-30"><a href="#LinearSVC-30"><span class="linenos"> 30</span></a><span class="sd">    The predicted class `y_hat` is determined based on the sign of `z`:</span>
</span><span id="LinearSVC-31"><a href="#LinearSVC-31"><span class="linenos"> 31</span></a>
</span><span id="LinearSVC-32"><a href="#LinearSVC-32"><span class="linenos"> 32</span></a><span class="sd">    ```</span>
</span><span id="LinearSVC-33"><a href="#LinearSVC-33"><span class="linenos"> 33</span></a><span class="sd">    y_hat = sign(z)</span>
</span><span id="LinearSVC-34"><a href="#LinearSVC-34"><span class="linenos"> 34</span></a><span class="sd">    ```</span>
</span><span id="LinearSVC-35"><a href="#LinearSVC-35"><span class="linenos"> 35</span></a>
</span><span id="LinearSVC-36"><a href="#LinearSVC-36"><span class="linenos"> 36</span></a><span class="sd">    ---</span>
</span><span id="LinearSVC-37"><a href="#LinearSVC-37"><span class="linenos"> 37</span></a>
</span><span id="LinearSVC-38"><a href="#LinearSVC-38"><span class="linenos"> 38</span></a><span class="sd">    ### Hinge Loss</span>
</span><span id="LinearSVC-39"><a href="#LinearSVC-39"><span class="linenos"> 39</span></a>
</span><span id="LinearSVC-40"><a href="#LinearSVC-40"><span class="linenos"> 40</span></a><span class="sd">    The hinge loss function measures the degree of violation of a sample&#39;s classification. For a sample `(x_i, y_i)`, where `x_i` is the input data and `y_i` is the true class label (-1 or 1), the hinge loss is defined as:</span>
</span><span id="LinearSVC-41"><a href="#LinearSVC-41"><span class="linenos"> 41</span></a>
</span><span id="LinearSVC-42"><a href="#LinearSVC-42"><span class="linenos"> 42</span></a><span class="sd">    ```</span>
</span><span id="LinearSVC-43"><a href="#LinearSVC-43"><span class="linenos"> 43</span></a><span class="sd">    loss_i = max(0, 1 - y_i * z_i)</span>
</span><span id="LinearSVC-44"><a href="#LinearSVC-44"><span class="linenos"> 44</span></a><span class="sd">    ```</span>
</span><span id="LinearSVC-45"><a href="#LinearSVC-45"><span class="linenos"> 45</span></a>
</span><span id="LinearSVC-46"><a href="#LinearSVC-46"><span class="linenos"> 46</span></a><span class="sd">    Where `z_i` is the linear combination for the `i`-th sample. The overall hinge loss for the entire dataset is the sum of individual hinge losses:</span>
</span><span id="LinearSVC-47"><a href="#LinearSVC-47"><span class="linenos"> 47</span></a>
</span><span id="LinearSVC-48"><a href="#LinearSVC-48"><span class="linenos"> 48</span></a><span class="sd">    ```</span>
</span><span id="LinearSVC-49"><a href="#LinearSVC-49"><span class="linenos"> 49</span></a><span class="sd">    loss = sum(max(0, 1 - y_i * z_i)) for all samples i</span>
</span><span id="LinearSVC-50"><a href="#LinearSVC-50"><span class="linenos"> 50</span></a><span class="sd">    ```</span>
</span><span id="LinearSVC-51"><a href="#LinearSVC-51"><span class="linenos"> 51</span></a>
</span><span id="LinearSVC-52"><a href="#LinearSVC-52"><span class="linenos"> 52</span></a><span class="sd">    ---</span>
</span><span id="LinearSVC-53"><a href="#LinearSVC-53"><span class="linenos"> 53</span></a>
</span><span id="LinearSVC-54"><a href="#LinearSVC-54"><span class="linenos"> 54</span></a><span class="sd">    ### Margin and Support Vectors</span>
</span><span id="LinearSVC-55"><a href="#LinearSVC-55"><span class="linenos"> 55</span></a>
</span><span id="LinearSVC-56"><a href="#LinearSVC-56"><span class="linenos"> 56</span></a><span class="sd">    The margin is the distance between the decision boundary and the closest data points. The goal is to maximize this margin while minimizing the hinge loss. Support vectors are the data points that are closest to the decision boundary and play a crucial role in defining the hyperplane.</span>
</span><span id="LinearSVC-57"><a href="#LinearSVC-57"><span class="linenos"> 57</span></a>
</span><span id="LinearSVC-58"><a href="#LinearSVC-58"><span class="linenos"> 58</span></a><span class="sd">    Maximizing the margin is equivalent to minimizing the norm of the weight vector `W`:</span>
</span><span id="LinearSVC-59"><a href="#LinearSVC-59"><span class="linenos"> 59</span></a>
</span><span id="LinearSVC-60"><a href="#LinearSVC-60"><span class="linenos"> 60</span></a><span class="sd">    ```</span>
</span><span id="LinearSVC-61"><a href="#LinearSVC-61"><span class="linenos"> 61</span></a><span class="sd">    min (||W|| / 2)</span>
</span><span id="LinearSVC-62"><a href="#LinearSVC-62"><span class="linenos"> 62</span></a><span class="sd">    ```</span>
</span><span id="LinearSVC-63"><a href="#LinearSVC-63"><span class="linenos"> 63</span></a>
</span><span id="LinearSVC-64"><a href="#LinearSVC-64"><span class="linenos"> 64</span></a><span class="sd">    ---</span>
</span><span id="LinearSVC-65"><a href="#LinearSVC-65"><span class="linenos"> 65</span></a>
</span><span id="LinearSVC-66"><a href="#LinearSVC-66"><span class="linenos"> 66</span></a><span class="sd">    ### Regularization</span>
</span><span id="LinearSVC-67"><a href="#LinearSVC-67"><span class="linenos"> 67</span></a>
</span><span id="LinearSVC-68"><a href="#LinearSVC-68"><span class="linenos"> 68</span></a><span class="sd">    Regularization is used to prevent overfitting by penalizing large weights. The regularization term is added to the loss function and is defined as:</span>
</span><span id="LinearSVC-69"><a href="#LinearSVC-69"><span class="linenos"> 69</span></a>
</span><span id="LinearSVC-70"><a href="#LinearSVC-70"><span class="linenos"> 70</span></a><span class="sd">    ```</span>
</span><span id="LinearSVC-71"><a href="#LinearSVC-71"><span class="linenos"> 71</span></a><span class="sd">    lambda_ * ||W||^2</span>
</span><span id="LinearSVC-72"><a href="#LinearSVC-72"><span class="linenos"> 72</span></a><span class="sd">    ```</span>
</span><span id="LinearSVC-73"><a href="#LinearSVC-73"><span class="linenos"> 73</span></a>
</span><span id="LinearSVC-74"><a href="#LinearSVC-74"><span class="linenos"> 74</span></a><span class="sd">    Where `lambda_` is the regularization parameter.</span>
</span><span id="LinearSVC-75"><a href="#LinearSVC-75"><span class="linenos"> 75</span></a>
</span><span id="LinearSVC-76"><a href="#LinearSVC-76"><span class="linenos"> 76</span></a><span class="sd">    ---</span>
</span><span id="LinearSVC-77"><a href="#LinearSVC-77"><span class="linenos"> 77</span></a>
</span><span id="LinearSVC-78"><a href="#LinearSVC-78"><span class="linenos"> 78</span></a><span class="sd">    ### Optimization</span>
</span><span id="LinearSVC-79"><a href="#LinearSVC-79"><span class="linenos"> 79</span></a>
</span><span id="LinearSVC-80"><a href="#LinearSVC-80"><span class="linenos"> 80</span></a><span class="sd">    The goal is to minimize the hinge loss and the regularization term. This is achieved by using gradient descent to iteratively update the weights and intercept. The gradient of the hinge loss function with respect to the weights `W` and intercept `b` is calculated for each sample. For correctly classified samples (`y * z &gt;= 1`), only the regularization term contributes to the gradient. For misclassified samples (`y * z &lt; 1`), both the regularization term and the hinge loss gradient contribute.</span>
</span><span id="LinearSVC-81"><a href="#LinearSVC-81"><span class="linenos"> 81</span></a>
</span><span id="LinearSVC-82"><a href="#LinearSVC-82"><span class="linenos"> 82</span></a><span class="sd">    The gradients are averaged over all samples and used to update the weights and intercept using the learning rate. This process is repeated for the specified number of iterations.</span>
</span><span id="LinearSVC-83"><a href="#LinearSVC-83"><span class="linenos"> 83</span></a>
</span><span id="LinearSVC-84"><a href="#LinearSVC-84"><span class="linenos"> 84</span></a><span class="sd">    ---</span>
</span><span id="LinearSVC-85"><a href="#LinearSVC-85"><span class="linenos"> 85</span></a>
</span><span id="LinearSVC-86"><a href="#LinearSVC-86"><span class="linenos"> 86</span></a><span class="sd">    ## Usage</span>
</span><span id="LinearSVC-87"><a href="#LinearSVC-87"><span class="linenos"> 87</span></a>
</span><span id="LinearSVC-88"><a href="#LinearSVC-88"><span class="linenos"> 88</span></a><span class="sd">    To use the `LinearSVC` model, follow these steps:</span>
</span><span id="LinearSVC-89"><a href="#LinearSVC-89"><span class="linenos"> 89</span></a>
</span><span id="LinearSVC-90"><a href="#LinearSVC-90"><span class="linenos"> 90</span></a><span class="sd">    1. Import the `LinearSVC` class from the appropriate module.</span>
</span><span id="LinearSVC-91"><a href="#LinearSVC-91"><span class="linenos"> 91</span></a><span class="sd">    2. Create an instance of the `LinearSVC` class, specifying hyperparameters.</span>
</span><span id="LinearSVC-92"><a href="#LinearSVC-92"><span class="linenos"> 92</span></a><span class="sd">    3. Fit the model to your training data using the `fit` method.</span>
</span><span id="LinearSVC-93"><a href="#LinearSVC-93"><span class="linenos"> 93</span></a><span class="sd">    4. Make predictions on new data using the `predict` method.</span>
</span><span id="LinearSVC-94"><a href="#LinearSVC-94"><span class="linenos"> 94</span></a><span class="sd">    5. Evaluate the model&#39;s performance using the `score` method.</span>
</span><span id="LinearSVC-95"><a href="#LinearSVC-95"><span class="linenos"> 95</span></a>
</span><span id="LinearSVC-96"><a href="#LinearSVC-96"><span class="linenos"> 96</span></a><span class="sd">    ```python</span>
</span><span id="LinearSVC-97"><a href="#LinearSVC-97"><span class="linenos"> 97</span></a><span class="sd">    from learnML.classification import LinearSVC</span>
</span><span id="LinearSVC-98"><a href="#LinearSVC-98"><span class="linenos"> 98</span></a>
</span><span id="LinearSVC-99"><a href="#LinearSVC-99"><span class="linenos"> 99</span></a><span class="sd">    # Create an instance of LinearSVC</span>
</span><span id="LinearSVC-100"><a href="#LinearSVC-100"><span class="linenos">100</span></a><span class="sd">    model = LinearSVC(learning_rate=0.001, lambda_=0.01, n_iterations=1000)</span>
</span><span id="LinearSVC-101"><a href="#LinearSVC-101"><span class="linenos">101</span></a>
</span><span id="LinearSVC-102"><a href="#LinearSVC-102"><span class="linenos">102</span></a><span class="sd">    # Fit the model to training data</span>
</span><span id="LinearSVC-103"><a href="#LinearSVC-103"><span class="linenos">103</span></a><span class="sd">    model.fit(X_train, Y_train)</span>
</span><span id="LinearSVC-104"><a href="#LinearSVC-104"><span class="linenos">104</span></a>
</span><span id="LinearSVC-105"><a href="#LinearSVC-105"><span class="linenos">105</span></a><span class="sd">    # Make predictions on new data</span>
</span><span id="LinearSVC-106"><a href="#LinearSVC-106"><span class="linenos">106</span></a><span class="sd">    predictions = model.predict(X_test)</span>
</span><span id="LinearSVC-107"><a href="#LinearSVC-107"><span class="linenos">107</span></a>
</span><span id="LinearSVC-108"><a href="#LinearSVC-108"><span class="linenos">108</span></a><span class="sd">    # Calculate the model&#39;s score</span>
</span><span id="LinearSVC-109"><a href="#LinearSVC-109"><span class="linenos">109</span></a><span class="sd">    model_score = model.score(X_test, Y_test)</span>
</span><span id="LinearSVC-110"><a href="#LinearSVC-110"><span class="linenos">110</span></a><span class="sd">    ```</span>
</span><span id="LinearSVC-111"><a href="#LinearSVC-111"><span class="linenos">111</span></a>
</span><span id="LinearSVC-112"><a href="#LinearSVC-112"><span class="linenos">112</span></a><span class="sd">    ---</span>
</span><span id="LinearSVC-113"><a href="#LinearSVC-113"><span class="linenos">113</span></a>
</span><span id="LinearSVC-114"><a href="#LinearSVC-114"><span class="linenos">114</span></a><span class="sd">    ## Advantages</span>
</span><span id="LinearSVC-115"><a href="#LinearSVC-115"><span class="linenos">115</span></a>
</span><span id="LinearSVC-116"><a href="#LinearSVC-116"><span class="linenos">116</span></a><span class="sd">    - Effective in high dimensional spaces</span>
</span><span id="LinearSVC-117"><a href="#LinearSVC-117"><span class="linenos">117</span></a><span class="sd">    - Works well with small number of samples</span>
</span><span id="LinearSVC-118"><a href="#LinearSVC-118"><span class="linenos">118</span></a><span class="sd">    - Works efficiently when there is a clear margin of separation between classes</span>
</span><span id="LinearSVC-119"><a href="#LinearSVC-119"><span class="linenos">119</span></a><span class="sd">    - Uses a subset of training points in the decision function (called support vectors), so it is also memory efficient</span>
</span><span id="LinearSVC-120"><a href="#LinearSVC-120"><span class="linenos">120</span></a>
</span><span id="LinearSVC-121"><a href="#LinearSVC-121"><span class="linenos">121</span></a><span class="sd">    ## Disadvantages</span>
</span><span id="LinearSVC-122"><a href="#LinearSVC-122"><span class="linenos">122</span></a>
</span><span id="LinearSVC-123"><a href="#LinearSVC-123"><span class="linenos">123</span></a><span class="sd">    - Not suitable for large number of samples (training time is higher)</span>
</span><span id="LinearSVC-124"><a href="#LinearSVC-124"><span class="linenos">124</span></a><span class="sd">    - Not suitable for noisy data with overlapping classes</span>
</span><span id="LinearSVC-125"><a href="#LinearSVC-125"><span class="linenos">125</span></a>
</span><span id="LinearSVC-126"><a href="#LinearSVC-126"><span class="linenos">126</span></a><span class="sd">    ---</span>
</span><span id="LinearSVC-127"><a href="#LinearSVC-127"><span class="linenos">127</span></a><span class="sd">    &quot;&quot;&quot;</span>
</span><span id="LinearSVC-128"><a href="#LinearSVC-128"><span class="linenos">128</span></a>
</span><span id="LinearSVC-129"><a href="#LinearSVC-129"><span class="linenos">129</span></a>    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span>
</span><span id="LinearSVC-130"><a href="#LinearSVC-130"><span class="linenos">130</span></a>        <span class="bp">self</span><span class="p">,</span>
</span><span id="LinearSVC-131"><a href="#LinearSVC-131"><span class="linenos">131</span></a>        <span class="n">learning_rate</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">float64</span> <span class="o">=</span> <span class="mf">0.001</span><span class="p">,</span>
</span><span id="LinearSVC-132"><a href="#LinearSVC-132"><span class="linenos">132</span></a>        <span class="n">n_iterations</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">1000</span><span class="p">,</span>
</span><span id="LinearSVC-133"><a href="#LinearSVC-133"><span class="linenos">133</span></a>        <span class="n">lambda_</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">float64</span> <span class="o">=</span> <span class="mf">0.01</span><span class="p">,</span>
</span><span id="LinearSVC-134"><a href="#LinearSVC-134"><span class="linenos">134</span></a>        <span class="n">x_scalar</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="n">IFeatureEngineering</span><span class="p">,</span> <span class="n">List</span><span class="p">[</span><span class="n">IFeatureEngineering</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
</span><span id="LinearSVC-135"><a href="#LinearSVC-135"><span class="linenos">135</span></a>        <span class="n">debug</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
</span><span id="LinearSVC-136"><a href="#LinearSVC-136"><span class="linenos">136</span></a>        <span class="n">copy_x</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
</span><span id="LinearSVC-137"><a href="#LinearSVC-137"><span class="linenos">137</span></a>    <span class="p">):</span>
</span><span id="LinearSVC-138"><a href="#LinearSVC-138"><span class="linenos">138</span></a><span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
</span><span id="LinearSVC-139"><a href="#LinearSVC-139"><span class="linenos">139</span></a><span class="sd">        Parameters</span>
</span><span id="LinearSVC-140"><a href="#LinearSVC-140"><span class="linenos">140</span></a><span class="sd">        ----------</span>
</span><span id="LinearSVC-141"><a href="#LinearSVC-141"><span class="linenos">141</span></a>
</span><span id="LinearSVC-142"><a href="#LinearSVC-142"><span class="linenos">142</span></a><span class="sd">        `learning_rate` : np.float64, optional</span>
</span><span id="LinearSVC-143"><a href="#LinearSVC-143"><span class="linenos">143</span></a><span class="sd">        - The learning rate, by default 0.001</span>
</span><span id="LinearSVC-144"><a href="#LinearSVC-144"><span class="linenos">144</span></a><span class="sd">        - The learning rate determines how much the weights are updated at each iteration</span>
</span><span id="LinearSVC-145"><a href="#LinearSVC-145"><span class="linenos">145</span></a><span class="sd">        - A low learning rate will take longer to converge, but a high learning rate may overshoot the optimal solution</span>
</span><span id="LinearSVC-146"><a href="#LinearSVC-146"><span class="linenos">146</span></a>
</span><span id="LinearSVC-147"><a href="#LinearSVC-147"><span class="linenos">147</span></a><span class="sd">        `n_iterations` : int, optional</span>
</span><span id="LinearSVC-148"><a href="#LinearSVC-148"><span class="linenos">148</span></a><span class="sd">        - The number of iterations, by default 1000</span>
</span><span id="LinearSVC-149"><a href="#LinearSVC-149"><span class="linenos">149</span></a><span class="sd">        - The number of iterations determines how many times the weights are updated</span>
</span><span id="LinearSVC-150"><a href="#LinearSVC-150"><span class="linenos">150</span></a><span class="sd">        - A higher number of iterations will take longer to converge, but a lower number of iterations may not be enough to converge</span>
</span><span id="LinearSVC-151"><a href="#LinearSVC-151"><span class="linenos">151</span></a>
</span><span id="LinearSVC-152"><a href="#LinearSVC-152"><span class="linenos">152</span></a><span class="sd">        `lambda_` : np.float64, optional</span>
</span><span id="LinearSVC-153"><a href="#LinearSVC-153"><span class="linenos">153</span></a><span class="sd">        - The regularization parameter, by default 0</span>
</span><span id="LinearSVC-154"><a href="#LinearSVC-154"><span class="linenos">154</span></a><span class="sd">        - The regularization parameter helps prevent overfitting by penalizing large weights</span>
</span><span id="LinearSVC-155"><a href="#LinearSVC-155"><span class="linenos">155</span></a><span class="sd">        - A higher regularization parameter will penalize large weights more, but a lower regularization parameter may not be enough to prevent overfitting</span>
</span><span id="LinearSVC-156"><a href="#LinearSVC-156"><span class="linenos">156</span></a>
</span><span id="LinearSVC-157"><a href="#LinearSVC-157"><span class="linenos">157</span></a><span class="sd">        `x_scalar` : Union[IFeatureEngineering, List[IFeatureEngineering]], optional</span>
</span><span id="LinearSVC-158"><a href="#LinearSVC-158"><span class="linenos">158</span></a><span class="sd">        - The feature engineering for the input data, by default None</span>
</span><span id="LinearSVC-159"><a href="#LinearSVC-159"><span class="linenos">159</span></a><span class="sd">        - If a list is provided, the feature engineering will be applied in the order provided</span>
</span><span id="LinearSVC-160"><a href="#LinearSVC-160"><span class="linenos">160</span></a><span class="sd">        - If a single feature engineering is provided, it will be applied to all input data</span>
</span><span id="LinearSVC-161"><a href="#LinearSVC-161"><span class="linenos">161</span></a>
</span><span id="LinearSVC-162"><a href="#LinearSVC-162"><span class="linenos">162</span></a><span class="sd">        `debug` : bool, optional</span>
</span><span id="LinearSVC-163"><a href="#LinearSVC-163"><span class="linenos">163</span></a><span class="sd">        - Whether to print debug messages, by default True</span>
</span><span id="LinearSVC-164"><a href="#LinearSVC-164"><span class="linenos">164</span></a><span class="sd">        - Debug messages include the cost at each iteration</span>
</span><span id="LinearSVC-165"><a href="#LinearSVC-165"><span class="linenos">165</span></a>
</span><span id="LinearSVC-166"><a href="#LinearSVC-166"><span class="linenos">166</span></a><span class="sd">        `copy_x` : bool, optional</span>
</span><span id="LinearSVC-167"><a href="#LinearSVC-167"><span class="linenos">167</span></a><span class="sd">        - Whether to copy the input array, by default True</span>
</span><span id="LinearSVC-168"><a href="#LinearSVC-168"><span class="linenos">168</span></a><span class="sd">        - If False, the input array will be overwritten</span>
</span><span id="LinearSVC-169"><a href="#LinearSVC-169"><span class="linenos">169</span></a>
</span><span id="LinearSVC-170"><a href="#LinearSVC-170"><span class="linenos">170</span></a><span class="sd">        ---</span>
</span><span id="LinearSVC-171"><a href="#LinearSVC-171"><span class="linenos">171</span></a><span class="sd">        &quot;&quot;&quot;</span>
</span><span id="LinearSVC-172"><a href="#LinearSVC-172"><span class="linenos">172</span></a>        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span>
</span><span id="LinearSVC-173"><a href="#LinearSVC-173"><span class="linenos">173</span></a>            <span class="n">learning_rate</span><span class="o">=</span><span class="n">learning_rate</span><span class="p">,</span>
</span><span id="LinearSVC-174"><a href="#LinearSVC-174"><span class="linenos">174</span></a>            <span class="n">n_iterations</span><span class="o">=</span><span class="n">n_iterations</span><span class="p">,</span>
</span><span id="LinearSVC-175"><a href="#LinearSVC-175"><span class="linenos">175</span></a>            <span class="n">debug</span><span class="o">=</span><span class="n">debug</span><span class="p">,</span>
</span><span id="LinearSVC-176"><a href="#LinearSVC-176"><span class="linenos">176</span></a>            <span class="n">copy_x</span><span class="o">=</span><span class="n">copy_x</span><span class="p">,</span>
</span><span id="LinearSVC-177"><a href="#LinearSVC-177"><span class="linenos">177</span></a>        <span class="p">)</span>
</span><span id="LinearSVC-178"><a href="#LinearSVC-178"><span class="linenos">178</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">_lambda</span> <span class="o">=</span> <span class="n">lambda_</span>
</span><span id="LinearSVC-179"><a href="#LinearSVC-179"><span class="linenos">179</span></a>
</span><span id="LinearSVC-180"><a href="#LinearSVC-180"><span class="linenos">180</span></a>        <span class="k">if</span> <span class="n">x_scalar</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
</span><span id="LinearSVC-181"><a href="#LinearSVC-181"><span class="linenos">181</span></a>            <span class="n">x_scalar</span> <span class="o">=</span> <span class="p">[]</span>
</span><span id="LinearSVC-182"><a href="#LinearSVC-182"><span class="linenos">182</span></a>        <span class="k">elif</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">x_scalar</span><span class="p">,</span> <span class="n">IFeatureEngineering</span><span class="p">):</span>
</span><span id="LinearSVC-183"><a href="#LinearSVC-183"><span class="linenos">183</span></a>            <span class="n">x_scalar</span> <span class="o">=</span> <span class="p">[</span><span class="n">x_scalar</span><span class="p">]</span>
</span><span id="LinearSVC-184"><a href="#LinearSVC-184"><span class="linenos">184</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">_x_scalar</span> <span class="o">=</span> <span class="n">x_scalar</span>
</span><span id="LinearSVC-185"><a href="#LinearSVC-185"><span class="linenos">185</span></a>
</span><span id="LinearSVC-186"><a href="#LinearSVC-186"><span class="linenos">186</span></a>    <span class="k">def</span> <span class="nf">_y_hat</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">,</span> <span class="n">W</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">,</span> <span class="n">b</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">float64</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">:</span>
</span><span id="LinearSVC-187"><a href="#LinearSVC-187"><span class="linenos">187</span></a><span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
</span><span id="LinearSVC-188"><a href="#LinearSVC-188"><span class="linenos">188</span></a><span class="sd">        ### Calculate the predicted value</span>
</span><span id="LinearSVC-189"><a href="#LinearSVC-189"><span class="linenos">189</span></a>
</span><span id="LinearSVC-190"><a href="#LinearSVC-190"><span class="linenos">190</span></a><span class="sd">        Parameters</span>
</span><span id="LinearSVC-191"><a href="#LinearSVC-191"><span class="linenos">191</span></a><span class="sd">        ----------</span>
</span><span id="LinearSVC-192"><a href="#LinearSVC-192"><span class="linenos">192</span></a>
</span><span id="LinearSVC-193"><a href="#LinearSVC-193"><span class="linenos">193</span></a><span class="sd">        `X` : np.ndarray</span>
</span><span id="LinearSVC-194"><a href="#LinearSVC-194"><span class="linenos">194</span></a><span class="sd">        - The input data of shape (n_samples, n_features)</span>
</span><span id="LinearSVC-195"><a href="#LinearSVC-195"><span class="linenos">195</span></a>
</span><span id="LinearSVC-196"><a href="#LinearSVC-196"><span class="linenos">196</span></a><span class="sd">        `W` : np.ndarray</span>
</span><span id="LinearSVC-197"><a href="#LinearSVC-197"><span class="linenos">197</span></a><span class="sd">        - The weights of shape (n_features,)</span>
</span><span id="LinearSVC-198"><a href="#LinearSVC-198"><span class="linenos">198</span></a>
</span><span id="LinearSVC-199"><a href="#LinearSVC-199"><span class="linenos">199</span></a><span class="sd">        `b` : np.float64</span>
</span><span id="LinearSVC-200"><a href="#LinearSVC-200"><span class="linenos">200</span></a><span class="sd">        - The intercept</span>
</span><span id="LinearSVC-201"><a href="#LinearSVC-201"><span class="linenos">201</span></a>
</span><span id="LinearSVC-202"><a href="#LinearSVC-202"><span class="linenos">202</span></a>
</span><span id="LinearSVC-203"><a href="#LinearSVC-203"><span class="linenos">203</span></a><span class="sd">        Returns</span>
</span><span id="LinearSVC-204"><a href="#LinearSVC-204"><span class="linenos">204</span></a><span class="sd">        -------</span>
</span><span id="LinearSVC-205"><a href="#LinearSVC-205"><span class="linenos">205</span></a>
</span><span id="LinearSVC-206"><a href="#LinearSVC-206"><span class="linenos">206</span></a><span class="sd">        `np.ndarray`</span>
</span><span id="LinearSVC-207"><a href="#LinearSVC-207"><span class="linenos">207</span></a><span class="sd">        - The predicted value of shape (n_samples, 1)</span>
</span><span id="LinearSVC-208"><a href="#LinearSVC-208"><span class="linenos">208</span></a>
</span><span id="LinearSVC-209"><a href="#LinearSVC-209"><span class="linenos">209</span></a><span class="sd">        ---</span>
</span><span id="LinearSVC-210"><a href="#LinearSVC-210"><span class="linenos">210</span></a><span class="sd">        &quot;&quot;&quot;</span>
</span><span id="LinearSVC-211"><a href="#LinearSVC-211"><span class="linenos">211</span></a>        <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">W</span><span class="p">)</span> <span class="o">-</span> <span class="n">b</span>
</span><span id="LinearSVC-212"><a href="#LinearSVC-212"><span class="linenos">212</span></a>
</span><span id="LinearSVC-213"><a href="#LinearSVC-213"><span class="linenos">213</span></a>    <span class="k">def</span> <span class="nf">_cost</span><span class="p">(</span>
</span><span id="LinearSVC-214"><a href="#LinearSVC-214"><span class="linenos">214</span></a>        <span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">,</span> <span class="n">y</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">,</span> <span class="n">W</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">,</span> <span class="n">b</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">float64</span>
</span><span id="LinearSVC-215"><a href="#LinearSVC-215"><span class="linenos">215</span></a>    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">np</span><span class="o">.</span><span class="n">float64</span><span class="p">:</span>
</span><span id="LinearSVC-216"><a href="#LinearSVC-216"><span class="linenos">216</span></a><span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
</span><span id="LinearSVC-217"><a href="#LinearSVC-217"><span class="linenos">217</span></a><span class="sd">        ### Calculate the cost of the model</span>
</span><span id="LinearSVC-218"><a href="#LinearSVC-218"><span class="linenos">218</span></a><span class="sd">        (Hinge loss)</span>
</span><span id="LinearSVC-219"><a href="#LinearSVC-219"><span class="linenos">219</span></a>
</span><span id="LinearSVC-220"><a href="#LinearSVC-220"><span class="linenos">220</span></a><span class="sd">        Parameters</span>
</span><span id="LinearSVC-221"><a href="#LinearSVC-221"><span class="linenos">221</span></a><span class="sd">        ----------</span>
</span><span id="LinearSVC-222"><a href="#LinearSVC-222"><span class="linenos">222</span></a>
</span><span id="LinearSVC-223"><a href="#LinearSVC-223"><span class="linenos">223</span></a><span class="sd">        `X` : np.ndarray</span>
</span><span id="LinearSVC-224"><a href="#LinearSVC-224"><span class="linenos">224</span></a><span class="sd">        - The input data of shape (n_samples, n_features)</span>
</span><span id="LinearSVC-225"><a href="#LinearSVC-225"><span class="linenos">225</span></a>
</span><span id="LinearSVC-226"><a href="#LinearSVC-226"><span class="linenos">226</span></a><span class="sd">        `y` : np.ndarray</span>
</span><span id="LinearSVC-227"><a href="#LinearSVC-227"><span class="linenos">227</span></a><span class="sd">        - The target data of shape (n_samples, 1)</span>
</span><span id="LinearSVC-228"><a href="#LinearSVC-228"><span class="linenos">228</span></a>
</span><span id="LinearSVC-229"><a href="#LinearSVC-229"><span class="linenos">229</span></a><span class="sd">        `W` : np.ndarray</span>
</span><span id="LinearSVC-230"><a href="#LinearSVC-230"><span class="linenos">230</span></a><span class="sd">        - The weights of shape (n_features,)</span>
</span><span id="LinearSVC-231"><a href="#LinearSVC-231"><span class="linenos">231</span></a>
</span><span id="LinearSVC-232"><a href="#LinearSVC-232"><span class="linenos">232</span></a><span class="sd">        `b` : np.float64</span>
</span><span id="LinearSVC-233"><a href="#LinearSVC-233"><span class="linenos">233</span></a><span class="sd">        - The intercept</span>
</span><span id="LinearSVC-234"><a href="#LinearSVC-234"><span class="linenos">234</span></a>
</span><span id="LinearSVC-235"><a href="#LinearSVC-235"><span class="linenos">235</span></a>
</span><span id="LinearSVC-236"><a href="#LinearSVC-236"><span class="linenos">236</span></a><span class="sd">        Returns</span>
</span><span id="LinearSVC-237"><a href="#LinearSVC-237"><span class="linenos">237</span></a><span class="sd">        -------</span>
</span><span id="LinearSVC-238"><a href="#LinearSVC-238"><span class="linenos">238</span></a>
</span><span id="LinearSVC-239"><a href="#LinearSVC-239"><span class="linenos">239</span></a><span class="sd">        `np.float64`</span>
</span><span id="LinearSVC-240"><a href="#LinearSVC-240"><span class="linenos">240</span></a><span class="sd">        - The cost</span>
</span><span id="LinearSVC-241"><a href="#LinearSVC-241"><span class="linenos">241</span></a>
</span><span id="LinearSVC-242"><a href="#LinearSVC-242"><span class="linenos">242</span></a><span class="sd">        ---</span>
</span><span id="LinearSVC-243"><a href="#LinearSVC-243"><span class="linenos">243</span></a><span class="sd">        &quot;&quot;&quot;</span>
</span><span id="LinearSVC-244"><a href="#LinearSVC-244"><span class="linenos">244</span></a>        <span class="n">n_samples</span> <span class="o">=</span> <span class="n">X</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
</span><span id="LinearSVC-245"><a href="#LinearSVC-245"><span class="linenos">245</span></a>        <span class="n">y_hat</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_y_hat</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">W</span><span class="p">,</span> <span class="n">b</span><span class="p">)</span>
</span><span id="LinearSVC-246"><a href="#LinearSVC-246"><span class="linenos">246</span></a>
</span><span id="LinearSVC-247"><a href="#LinearSVC-247"><span class="linenos">247</span></a>        <span class="c1"># Hinge loss function</span>
</span><span id="LinearSVC-248"><a href="#LinearSVC-248"><span class="linenos">248</span></a>        <span class="c1"># max(0, 1 - y * y_hat)</span>
</span><span id="LinearSVC-249"><a href="#LinearSVC-249"><span class="linenos">249</span></a>        <span class="n">loss</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">maximum</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span> <span class="o">-</span> <span class="n">y</span> <span class="o">*</span> <span class="n">y_hat</span><span class="p">)</span>
</span><span id="LinearSVC-250"><a href="#LinearSVC-250"><span class="linenos">250</span></a>
</span><span id="LinearSVC-251"><a href="#LinearSVC-251"><span class="linenos">251</span></a>        <span class="c1"># Calculate the cost</span>
</span><span id="LinearSVC-252"><a href="#LinearSVC-252"><span class="linenos">252</span></a>        <span class="c1"># 1/n * sum(max(0, 1 - y * y_hat)) + lambda * ||W||^2</span>
</span><span id="LinearSVC-253"><a href="#LinearSVC-253"><span class="linenos">253</span></a>        <span class="n">cost</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">loss</span><span class="p">)</span> <span class="o">/</span> <span class="n">n_samples</span>
</span><span id="LinearSVC-254"><a href="#LinearSVC-254"><span class="linenos">254</span></a>        <span class="n">cost</span> <span class="o">+=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_lambda</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">W</span><span class="o">.</span><span class="n">T</span><span class="p">,</span> <span class="n">W</span><span class="p">)</span>
</span><span id="LinearSVC-255"><a href="#LinearSVC-255"><span class="linenos">255</span></a>
</span><span id="LinearSVC-256"><a href="#LinearSVC-256"><span class="linenos">256</span></a>        <span class="k">return</span> <span class="n">cost</span>
</span><span id="LinearSVC-257"><a href="#LinearSVC-257"><span class="linenos">257</span></a>
</span><span id="LinearSVC-258"><a href="#LinearSVC-258"><span class="linenos">258</span></a>    <span class="k">def</span> <span class="nf">_gradient</span><span class="p">(</span>
</span><span id="LinearSVC-259"><a href="#LinearSVC-259"><span class="linenos">259</span></a>        <span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">,</span> <span class="n">y</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">,</span> <span class="n">W</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">,</span> <span class="n">b</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">float64</span>
</span><span id="LinearSVC-260"><a href="#LinearSVC-260"><span class="linenos">260</span></a>    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Union</span><span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">float64</span><span class="p">]:</span>
</span><span id="LinearSVC-261"><a href="#LinearSVC-261"><span class="linenos">261</span></a><span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
</span><span id="LinearSVC-262"><a href="#LinearSVC-262"><span class="linenos">262</span></a><span class="sd">        ### Calculate the gradient of the model</span>
</span><span id="LinearSVC-263"><a href="#LinearSVC-263"><span class="linenos">263</span></a>
</span><span id="LinearSVC-264"><a href="#LinearSVC-264"><span class="linenos">264</span></a><span class="sd">        Parameters</span>
</span><span id="LinearSVC-265"><a href="#LinearSVC-265"><span class="linenos">265</span></a><span class="sd">        ----------</span>
</span><span id="LinearSVC-266"><a href="#LinearSVC-266"><span class="linenos">266</span></a>
</span><span id="LinearSVC-267"><a href="#LinearSVC-267"><span class="linenos">267</span></a><span class="sd">        `X` : np.ndarray</span>
</span><span id="LinearSVC-268"><a href="#LinearSVC-268"><span class="linenos">268</span></a><span class="sd">        - The input data of shape (n_samples, n_features)</span>
</span><span id="LinearSVC-269"><a href="#LinearSVC-269"><span class="linenos">269</span></a>
</span><span id="LinearSVC-270"><a href="#LinearSVC-270"><span class="linenos">270</span></a><span class="sd">        `y` : np.ndarray</span>
</span><span id="LinearSVC-271"><a href="#LinearSVC-271"><span class="linenos">271</span></a><span class="sd">        - The target data of shape (n_samples, 1)</span>
</span><span id="LinearSVC-272"><a href="#LinearSVC-272"><span class="linenos">272</span></a>
</span><span id="LinearSVC-273"><a href="#LinearSVC-273"><span class="linenos">273</span></a><span class="sd">        `W` : np.ndarray</span>
</span><span id="LinearSVC-274"><a href="#LinearSVC-274"><span class="linenos">274</span></a><span class="sd">        - The weights of shape (n_features,)</span>
</span><span id="LinearSVC-275"><a href="#LinearSVC-275"><span class="linenos">275</span></a>
</span><span id="LinearSVC-276"><a href="#LinearSVC-276"><span class="linenos">276</span></a><span class="sd">        `b` : np.float64</span>
</span><span id="LinearSVC-277"><a href="#LinearSVC-277"><span class="linenos">277</span></a><span class="sd">        - The intercept</span>
</span><span id="LinearSVC-278"><a href="#LinearSVC-278"><span class="linenos">278</span></a>
</span><span id="LinearSVC-279"><a href="#LinearSVC-279"><span class="linenos">279</span></a>
</span><span id="LinearSVC-280"><a href="#LinearSVC-280"><span class="linenos">280</span></a><span class="sd">        Returns</span>
</span><span id="LinearSVC-281"><a href="#LinearSVC-281"><span class="linenos">281</span></a><span class="sd">        -------</span>
</span><span id="LinearSVC-282"><a href="#LinearSVC-282"><span class="linenos">282</span></a>
</span><span id="LinearSVC-283"><a href="#LinearSVC-283"><span class="linenos">283</span></a><span class="sd">        `Union[np.ndarray, np.float64]`</span>
</span><span id="LinearSVC-284"><a href="#LinearSVC-284"><span class="linenos">284</span></a><span class="sd">        - The gradient of the model</span>
</span><span id="LinearSVC-285"><a href="#LinearSVC-285"><span class="linenos">285</span></a>
</span><span id="LinearSVC-286"><a href="#LinearSVC-286"><span class="linenos">286</span></a><span class="sd">        ---</span>
</span><span id="LinearSVC-287"><a href="#LinearSVC-287"><span class="linenos">287</span></a><span class="sd">        &quot;&quot;&quot;</span>
</span><span id="LinearSVC-288"><a href="#LinearSVC-288"><span class="linenos">288</span></a>        <span class="n">n_samples</span> <span class="o">=</span> <span class="n">X</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
</span><span id="LinearSVC-289"><a href="#LinearSVC-289"><span class="linenos">289</span></a>        <span class="n">y_hat</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_y_hat</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">W</span><span class="p">,</span> <span class="n">b</span><span class="p">)</span>
</span><span id="LinearSVC-290"><a href="#LinearSVC-290"><span class="linenos">290</span></a>
</span><span id="LinearSVC-291"><a href="#LinearSVC-291"><span class="linenos">291</span></a>        <span class="c1"># Calculate the gradient</span>
</span><span id="LinearSVC-292"><a href="#LinearSVC-292"><span class="linenos">292</span></a>        <span class="c1"># 1/n * sum(max(0, 1 - y * y_hat)) + lambda * ||W||^2</span>
</span><span id="LinearSVC-293"><a href="#LinearSVC-293"><span class="linenos">293</span></a>        <span class="n">dw</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">W</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
</span><span id="LinearSVC-294"><a href="#LinearSVC-294"><span class="linenos">294</span></a>        <span class="n">db</span> <span class="o">=</span> <span class="mi">0</span>
</span><span id="LinearSVC-295"><a href="#LinearSVC-295"><span class="linenos">295</span></a>
</span><span id="LinearSVC-296"><a href="#LinearSVC-296"><span class="linenos">296</span></a>        <span class="k">for</span> <span class="n">idx</span><span class="p">,</span> <span class="n">x_i</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">X</span><span class="p">):</span>
</span><span id="LinearSVC-297"><a href="#LinearSVC-297"><span class="linenos">297</span></a>            <span class="k">if</span> <span class="n">y</span><span class="p">[</span><span class="n">idx</span><span class="p">]</span> <span class="o">*</span> <span class="n">y_hat</span><span class="p">[</span><span class="n">idx</span><span class="p">]</span> <span class="o">&gt;=</span> <span class="mi">1</span><span class="p">:</span>
</span><span id="LinearSVC-298"><a href="#LinearSVC-298"><span class="linenos">298</span></a>                <span class="c1"># Gradient of the regularization term</span>
</span><span id="LinearSVC-299"><a href="#LinearSVC-299"><span class="linenos">299</span></a>                <span class="c1"># 2 * lambda * W</span>
</span><span id="LinearSVC-300"><a href="#LinearSVC-300"><span class="linenos">300</span></a>                <span class="n">dw</span> <span class="o">+=</span> <span class="mi">2</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">_lambda</span> <span class="o">*</span> <span class="n">W</span>
</span><span id="LinearSVC-301"><a href="#LinearSVC-301"><span class="linenos">301</span></a>            <span class="k">else</span><span class="p">:</span>
</span><span id="LinearSVC-302"><a href="#LinearSVC-302"><span class="linenos">302</span></a>                <span class="c1"># Gradient of the hinge loss function</span>
</span><span id="LinearSVC-303"><a href="#LinearSVC-303"><span class="linenos">303</span></a>                <span class="c1"># -y * x_i</span>
</span><span id="LinearSVC-304"><a href="#LinearSVC-304"><span class="linenos">304</span></a>                <span class="n">dw</span> <span class="o">+=</span> <span class="mi">2</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">_lambda</span> <span class="o">*</span> <span class="n">W</span> <span class="o">-</span> <span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">x_i</span><span class="p">,</span> <span class="n">y</span><span class="p">[</span><span class="n">idx</span><span class="p">])</span>
</span><span id="LinearSVC-305"><a href="#LinearSVC-305"><span class="linenos">305</span></a>                <span class="n">db</span> <span class="o">+=</span> <span class="n">y</span><span class="p">[</span><span class="n">idx</span><span class="p">]</span>
</span><span id="LinearSVC-306"><a href="#LinearSVC-306"><span class="linenos">306</span></a>
</span><span id="LinearSVC-307"><a href="#LinearSVC-307"><span class="linenos">307</span></a>        <span class="n">dw</span> <span class="o">/=</span> <span class="n">n_samples</span>
</span><span id="LinearSVC-308"><a href="#LinearSVC-308"><span class="linenos">308</span></a>        <span class="n">db</span> <span class="o">/=</span> <span class="n">n_samples</span>
</span><span id="LinearSVC-309"><a href="#LinearSVC-309"><span class="linenos">309</span></a>
</span><span id="LinearSVC-310"><a href="#LinearSVC-310"><span class="linenos">310</span></a>        <span class="k">return</span> <span class="n">dw</span><span class="p">,</span> <span class="n">db</span>
</span><span id="LinearSVC-311"><a href="#LinearSVC-311"><span class="linenos">311</span></a>
</span><span id="LinearSVC-312"><a href="#LinearSVC-312"><span class="linenos">312</span></a>    <span class="k">def</span> <span class="nf">_validate_data</span><span class="p">(</span>
</span><span id="LinearSVC-313"><a href="#LinearSVC-313"><span class="linenos">313</span></a>        <span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">,</span> <span class="n">Y</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span> <span class="o">=</span> <span class="kc">None</span>
</span><span id="LinearSVC-314"><a href="#LinearSVC-314"><span class="linenos">314</span></a>    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tuple</span><span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">]:</span>
</span><span id="LinearSVC-315"><a href="#LinearSVC-315"><span class="linenos">315</span></a><span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
</span><span id="LinearSVC-316"><a href="#LinearSVC-316"><span class="linenos">316</span></a><span class="sd">        ### Get the X and y data</span>
</span><span id="LinearSVC-317"><a href="#LinearSVC-317"><span class="linenos">317</span></a>
</span><span id="LinearSVC-318"><a href="#LinearSVC-318"><span class="linenos">318</span></a><span class="sd">        Parameters</span>
</span><span id="LinearSVC-319"><a href="#LinearSVC-319"><span class="linenos">319</span></a><span class="sd">        ----------</span>
</span><span id="LinearSVC-320"><a href="#LinearSVC-320"><span class="linenos">320</span></a>
</span><span id="LinearSVC-321"><a href="#LinearSVC-321"><span class="linenos">321</span></a><span class="sd">        `X` : np.ndarray</span>
</span><span id="LinearSVC-322"><a href="#LinearSVC-322"><span class="linenos">322</span></a><span class="sd">        - The input data of shape (n_samples, n_features)</span>
</span><span id="LinearSVC-323"><a href="#LinearSVC-323"><span class="linenos">323</span></a>
</span><span id="LinearSVC-324"><a href="#LinearSVC-324"><span class="linenos">324</span></a><span class="sd">        `Y` : np.ndarray, optional</span>
</span><span id="LinearSVC-325"><a href="#LinearSVC-325"><span class="linenos">325</span></a><span class="sd">        - The target data of shape (n_samples, 1)</span>
</span><span id="LinearSVC-326"><a href="#LinearSVC-326"><span class="linenos">326</span></a>
</span><span id="LinearSVC-327"><a href="#LinearSVC-327"><span class="linenos">327</span></a>
</span><span id="LinearSVC-328"><a href="#LinearSVC-328"><span class="linenos">328</span></a><span class="sd">        Returns</span>
</span><span id="LinearSVC-329"><a href="#LinearSVC-329"><span class="linenos">329</span></a><span class="sd">        -------</span>
</span><span id="LinearSVC-330"><a href="#LinearSVC-330"><span class="linenos">330</span></a>
</span><span id="LinearSVC-331"><a href="#LinearSVC-331"><span class="linenos">331</span></a><span class="sd">        `Tuple[np.ndarray, np.ndarray]`</span>
</span><span id="LinearSVC-332"><a href="#LinearSVC-332"><span class="linenos">332</span></a><span class="sd">        - The X and Y data</span>
</span><span id="LinearSVC-333"><a href="#LinearSVC-333"><span class="linenos">333</span></a>
</span><span id="LinearSVC-334"><a href="#LinearSVC-334"><span class="linenos">334</span></a><span class="sd">        ---</span>
</span><span id="LinearSVC-335"><a href="#LinearSVC-335"><span class="linenos">335</span></a><span class="sd">        &quot;&quot;&quot;</span>
</span><span id="LinearSVC-336"><a href="#LinearSVC-336"><span class="linenos">336</span></a>        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_copy_x</span><span class="p">:</span>
</span><span id="LinearSVC-337"><a href="#LinearSVC-337"><span class="linenos">337</span></a>            <span class="n">X</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">copy</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
</span><span id="LinearSVC-338"><a href="#LinearSVC-338"><span class="linenos">338</span></a>
</span><span id="LinearSVC-339"><a href="#LinearSVC-339"><span class="linenos">339</span></a>        <span class="k">if</span> <span class="n">X</span><span class="o">.</span><span class="n">ndim</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>
</span><span id="LinearSVC-340"><a href="#LinearSVC-340"><span class="linenos">340</span></a>            <span class="n">X</span> <span class="o">=</span> <span class="n">X</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
</span><span id="LinearSVC-341"><a href="#LinearSVC-341"><span class="linenos">341</span></a>
</span><span id="LinearSVC-342"><a href="#LinearSVC-342"><span class="linenos">342</span></a>        <span class="k">for</span> <span class="n">scalar</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">_x_scalar</span><span class="p">:</span>
</span><span id="LinearSVC-343"><a href="#LinearSVC-343"><span class="linenos">343</span></a>            <span class="n">X</span> <span class="o">=</span> <span class="n">scalar</span><span class="o">.</span><span class="n">transform</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
</span><span id="LinearSVC-344"><a href="#LinearSVC-344"><span class="linenos">344</span></a>
</span><span id="LinearSVC-345"><a href="#LinearSVC-345"><span class="linenos">345</span></a>        <span class="k">if</span> <span class="n">Y</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
</span><span id="LinearSVC-346"><a href="#LinearSVC-346"><span class="linenos">346</span></a>            <span class="k">if</span> <span class="n">Y</span><span class="o">.</span><span class="n">ndim</span> <span class="o">==</span> <span class="mi">2</span><span class="p">:</span>
</span><span id="LinearSVC-347"><a href="#LinearSVC-347"><span class="linenos">347</span></a>                <span class="n">Y</span> <span class="o">=</span> <span class="n">Y</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span>
</span><span id="LinearSVC-348"><a href="#LinearSVC-348"><span class="linenos">348</span></a>
</span><span id="LinearSVC-349"><a href="#LinearSVC-349"><span class="linenos">349</span></a>            <span class="n">Y</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">where</span><span class="p">(</span><span class="n">Y</span> <span class="o">&lt;=</span> <span class="mi">0</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
</span><span id="LinearSVC-350"><a href="#LinearSVC-350"><span class="linenos">350</span></a>
</span><span id="LinearSVC-351"><a href="#LinearSVC-351"><span class="linenos">351</span></a>        <span class="k">if</span> <span class="n">Y</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
</span><span id="LinearSVC-352"><a href="#LinearSVC-352"><span class="linenos">352</span></a>            <span class="k">return</span> <span class="n">X</span>
</span><span id="LinearSVC-353"><a href="#LinearSVC-353"><span class="linenos">353</span></a>        <span class="k">return</span> <span class="n">X</span><span class="p">,</span> <span class="n">Y</span>
</span><span id="LinearSVC-354"><a href="#LinearSVC-354"><span class="linenos">354</span></a>
</span><span id="LinearSVC-355"><a href="#LinearSVC-355"><span class="linenos">355</span></a>    <span class="k">def</span> <span class="nf">fit</span><span class="p">(</span>
</span><span id="LinearSVC-356"><a href="#LinearSVC-356"><span class="linenos">356</span></a>        <span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">,</span> <span class="n">Y</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">,</span> <span class="n">W</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span> <span class="n">b</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">float64</span> <span class="o">=</span> <span class="mf">0.0</span>
</span><span id="LinearSVC-357"><a href="#LinearSVC-357"><span class="linenos">357</span></a>    <span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
</span><span id="LinearSVC-358"><a href="#LinearSVC-358"><span class="linenos">358</span></a><span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
</span><span id="LinearSVC-359"><a href="#LinearSVC-359"><span class="linenos">359</span></a><span class="sd">        ### Fit the model to the data</span>
</span><span id="LinearSVC-360"><a href="#LinearSVC-360"><span class="linenos">360</span></a>
</span><span id="LinearSVC-361"><a href="#LinearSVC-361"><span class="linenos">361</span></a><span class="sd">        Parameters</span>
</span><span id="LinearSVC-362"><a href="#LinearSVC-362"><span class="linenos">362</span></a><span class="sd">        ----------</span>
</span><span id="LinearSVC-363"><a href="#LinearSVC-363"><span class="linenos">363</span></a>
</span><span id="LinearSVC-364"><a href="#LinearSVC-364"><span class="linenos">364</span></a><span class="sd">        `X` : np.ndarray</span>
</span><span id="LinearSVC-365"><a href="#LinearSVC-365"><span class="linenos">365</span></a><span class="sd">        - The input data of shape (n_samples, n_features)</span>
</span><span id="LinearSVC-366"><a href="#LinearSVC-366"><span class="linenos">366</span></a>
</span><span id="LinearSVC-367"><a href="#LinearSVC-367"><span class="linenos">367</span></a><span class="sd">        `y` : np.ndarray</span>
</span><span id="LinearSVC-368"><a href="#LinearSVC-368"><span class="linenos">368</span></a><span class="sd">        - The target data of shape (n_samples, 1) or (n_samples,)</span>
</span><span id="LinearSVC-369"><a href="#LinearSVC-369"><span class="linenos">369</span></a>
</span><span id="LinearSVC-370"><a href="#LinearSVC-370"><span class="linenos">370</span></a><span class="sd">        `W` : np.ndarray, optional</span>
</span><span id="LinearSVC-371"><a href="#LinearSVC-371"><span class="linenos">371</span></a><span class="sd">        - The weights of shape (n_features,), by default None</span>
</span><span id="LinearSVC-372"><a href="#LinearSVC-372"><span class="linenos">372</span></a><span class="sd">        - If None, then the weight array will be initialized to an array of</span>
</span><span id="LinearSVC-373"><a href="#LinearSVC-373"><span class="linenos">373</span></a><span class="sd">            zeros of shape (n_features,)</span>
</span><span id="LinearSVC-374"><a href="#LinearSVC-374"><span class="linenos">374</span></a><span class="sd">        - If not None, then the weight array will be initialized to the given</span>
</span><span id="LinearSVC-375"><a href="#LinearSVC-375"><span class="linenos">375</span></a><span class="sd">            array</span>
</span><span id="LinearSVC-376"><a href="#LinearSVC-376"><span class="linenos">376</span></a>
</span><span id="LinearSVC-377"><a href="#LinearSVC-377"><span class="linenos">377</span></a><span class="sd">        `b` : np.float64, optional</span>
</span><span id="LinearSVC-378"><a href="#LinearSVC-378"><span class="linenos">378</span></a><span class="sd">        - The intercept, by default 0.0</span>
</span><span id="LinearSVC-379"><a href="#LinearSVC-379"><span class="linenos">379</span></a><span class="sd">        - If None, then the intercept will be initialized to 0.0</span>
</span><span id="LinearSVC-380"><a href="#LinearSVC-380"><span class="linenos">380</span></a><span class="sd">        - If not None, then the intercept will be initialized to the given</span>
</span><span id="LinearSVC-381"><a href="#LinearSVC-381"><span class="linenos">381</span></a><span class="sd">            value</span>
</span><span id="LinearSVC-382"><a href="#LinearSVC-382"><span class="linenos">382</span></a>
</span><span id="LinearSVC-383"><a href="#LinearSVC-383"><span class="linenos">383</span></a>
</span><span id="LinearSVC-384"><a href="#LinearSVC-384"><span class="linenos">384</span></a><span class="sd">        Returns</span>
</span><span id="LinearSVC-385"><a href="#LinearSVC-385"><span class="linenos">385</span></a><span class="sd">        -------</span>
</span><span id="LinearSVC-386"><a href="#LinearSVC-386"><span class="linenos">386</span></a>
</span><span id="LinearSVC-387"><a href="#LinearSVC-387"><span class="linenos">387</span></a><span class="sd">        None</span>
</span><span id="LinearSVC-388"><a href="#LinearSVC-388"><span class="linenos">388</span></a>
</span><span id="LinearSVC-389"><a href="#LinearSVC-389"><span class="linenos">389</span></a><span class="sd">        ---</span>
</span><span id="LinearSVC-390"><a href="#LinearSVC-390"><span class="linenos">390</span></a><span class="sd">        &quot;&quot;&quot;</span>
</span><span id="LinearSVC-391"><a href="#LinearSVC-391"><span class="linenos">391</span></a>        <span class="n">X</span><span class="p">,</span> <span class="n">Y</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_validate_data</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">Y</span><span class="p">)</span>
</span><span id="LinearSVC-392"><a href="#LinearSVC-392"><span class="linenos">392</span></a>
</span><span id="LinearSVC-393"><a href="#LinearSVC-393"><span class="linenos">393</span></a>        <span class="k">assert</span> <span class="n">X</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">==</span> <span class="n">Y</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="s2">&quot;The number of samples must be equal&quot;</span>
</span><span id="LinearSVC-394"><a href="#LinearSVC-394"><span class="linenos">394</span></a>
</span><span id="LinearSVC-395"><a href="#LinearSVC-395"><span class="linenos">395</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">_weights</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">X</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span> <span class="k">if</span> <span class="n">W</span> <span class="ow">is</span> <span class="kc">None</span> <span class="k">else</span> <span class="n">W</span>
</span><span id="LinearSVC-396"><a href="#LinearSVC-396"><span class="linenos">396</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">_intercept</span> <span class="o">=</span> <span class="n">b</span>
</span><span id="LinearSVC-397"><a href="#LinearSVC-397"><span class="linenos">397</span></a>
</span><span id="LinearSVC-398"><a href="#LinearSVC-398"><span class="linenos">398</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">_cost_history</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span>
</span><span id="LinearSVC-399"><a href="#LinearSVC-399"><span class="linenos">399</span></a>            <span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">_cost</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">Y</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">_weights</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">_intercept</span><span class="p">)]</span>
</span><span id="LinearSVC-400"><a href="#LinearSVC-400"><span class="linenos">400</span></a>        <span class="p">)</span>
</span><span id="LinearSVC-401"><a href="#LinearSVC-401"><span class="linenos">401</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">_params_history</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span>
</span><span id="LinearSVC-402"><a href="#LinearSVC-402"><span class="linenos">402</span></a>            <span class="p">[[</span><span class="bp">self</span><span class="o">.</span><span class="n">_weights</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">_intercept</span><span class="p">]],</span> <span class="n">dtype</span><span class="o">=</span><span class="nb">object</span>
</span><span id="LinearSVC-403"><a href="#LinearSVC-403"><span class="linenos">403</span></a>        <span class="p">)</span>
</span><span id="LinearSVC-404"><a href="#LinearSVC-404"><span class="linenos">404</span></a>
</span><span id="LinearSVC-405"><a href="#LinearSVC-405"><span class="linenos">405</span></a>        <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_n_iterations</span><span class="p">):</span>
</span><span id="LinearSVC-406"><a href="#LinearSVC-406"><span class="linenos">406</span></a>            <span class="n">dw</span><span class="p">,</span> <span class="n">db</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_gradient</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">Y</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">_weights</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">_intercept</span><span class="p">)</span>
</span><span id="LinearSVC-407"><a href="#LinearSVC-407"><span class="linenos">407</span></a>
</span><span id="LinearSVC-408"><a href="#LinearSVC-408"><span class="linenos">408</span></a>            <span class="bp">self</span><span class="o">.</span><span class="n">_weights</span> <span class="o">-=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_learning_rate</span> <span class="o">*</span> <span class="n">dw</span>
</span><span id="LinearSVC-409"><a href="#LinearSVC-409"><span class="linenos">409</span></a>            <span class="bp">self</span><span class="o">.</span><span class="n">_intercept</span> <span class="o">-=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_learning_rate</span> <span class="o">*</span> <span class="n">db</span>
</span><span id="LinearSVC-410"><a href="#LinearSVC-410"><span class="linenos">410</span></a>
</span><span id="LinearSVC-411"><a href="#LinearSVC-411"><span class="linenos">411</span></a>            <span class="c1"># Save cost and params history</span>
</span><span id="LinearSVC-412"><a href="#LinearSVC-412"><span class="linenos">412</span></a>            <span class="n">cost</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_cost</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">Y</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">_weights</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">_intercept</span><span class="p">)</span>
</span><span id="LinearSVC-413"><a href="#LinearSVC-413"><span class="linenos">413</span></a>
</span><span id="LinearSVC-414"><a href="#LinearSVC-414"><span class="linenos">414</span></a>            <span class="k">if</span> <span class="n">cost</span> <span class="o">==</span> <span class="n">np</span><span class="o">.</span><span class="n">nan</span> <span class="ow">or</span> <span class="n">cost</span> <span class="o">==</span> <span class="n">np</span><span class="o">.</span><span class="n">inf</span><span class="p">:</span>
</span><span id="LinearSVC-415"><a href="#LinearSVC-415"><span class="linenos">415</span></a>                <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
</span><span id="LinearSVC-416"><a href="#LinearSVC-416"><span class="linenos">416</span></a>                    <span class="s2">&quot;Gradient descent failed. Try normalizing the input array or reducing the learning rate. &quot;</span>
</span><span id="LinearSVC-417"><a href="#LinearSVC-417"><span class="linenos">417</span></a>                    <span class="s2">&quot;If the problem persists, try reducing the number of iterations.&quot;</span>
</span><span id="LinearSVC-418"><a href="#LinearSVC-418"><span class="linenos">418</span></a>                <span class="p">)</span>
</span><span id="LinearSVC-419"><a href="#LinearSVC-419"><span class="linenos">419</span></a>
</span><span id="LinearSVC-420"><a href="#LinearSVC-420"><span class="linenos">420</span></a>            <span class="bp">self</span><span class="o">.</span><span class="n">_cost_history</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_cost_history</span><span class="p">,</span> <span class="n">cost</span><span class="p">)</span>
</span><span id="LinearSVC-421"><a href="#LinearSVC-421"><span class="linenos">421</span></a>            <span class="bp">self</span><span class="o">.</span><span class="n">_params_history</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">append</span><span class="p">(</span>
</span><span id="LinearSVC-422"><a href="#LinearSVC-422"><span class="linenos">422</span></a>                <span class="bp">self</span><span class="o">.</span><span class="n">_params_history</span><span class="p">,</span>
</span><span id="LinearSVC-423"><a href="#LinearSVC-423"><span class="linenos">423</span></a>                <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([[</span><span class="bp">self</span><span class="o">.</span><span class="n">_weights</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">_intercept</span><span class="p">]],</span> <span class="n">dtype</span><span class="o">=</span><span class="nb">object</span><span class="p">),</span>
</span><span id="LinearSVC-424"><a href="#LinearSVC-424"><span class="linenos">424</span></a>                <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span>
</span><span id="LinearSVC-425"><a href="#LinearSVC-425"><span class="linenos">425</span></a>            <span class="p">)</span>
</span><span id="LinearSVC-426"><a href="#LinearSVC-426"><span class="linenos">426</span></a>
</span><span id="LinearSVC-427"><a href="#LinearSVC-427"><span class="linenos">427</span></a>            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_debug</span> <span class="ow">and</span> <span class="n">i</span> <span class="o">%</span> <span class="bp">self</span><span class="o">.</span><span class="n">_debug_freq</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
</span><span id="LinearSVC-428"><a href="#LinearSVC-428"><span class="linenos">428</span></a>                <span class="bp">self</span><span class="o">.</span><span class="n">_debug_print</span><span class="p">(</span><span class="n">i</span><span class="p">,</span> <span class="n">cost</span><span class="p">)</span>
</span><span id="LinearSVC-429"><a href="#LinearSVC-429"><span class="linenos">429</span></a>
</span><span id="LinearSVC-430"><a href="#LinearSVC-430"><span class="linenos">430</span></a>        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_debug</span><span class="p">:</span>
</span><span id="LinearSVC-431"><a href="#LinearSVC-431"><span class="linenos">431</span></a>            <span class="bp">self</span><span class="o">.</span><span class="n">_debug_print</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_n_iterations</span><span class="p">,</span> <span class="n">cost</span><span class="p">)</span>
</span><span id="LinearSVC-432"><a href="#LinearSVC-432"><span class="linenos">432</span></a>
</span><span id="LinearSVC-433"><a href="#LinearSVC-433"><span class="linenos">433</span></a>    <span class="k">def</span> <span class="nf">predict</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">:</span>
</span><span id="LinearSVC-434"><a href="#LinearSVC-434"><span class="linenos">434</span></a><span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
</span><span id="LinearSVC-435"><a href="#LinearSVC-435"><span class="linenos">435</span></a><span class="sd">        ### Predict the target data</span>
</span><span id="LinearSVC-436"><a href="#LinearSVC-436"><span class="linenos">436</span></a>
</span><span id="LinearSVC-437"><a href="#LinearSVC-437"><span class="linenos">437</span></a><span class="sd">        Parameters</span>
</span><span id="LinearSVC-438"><a href="#LinearSVC-438"><span class="linenos">438</span></a><span class="sd">        ----------</span>
</span><span id="LinearSVC-439"><a href="#LinearSVC-439"><span class="linenos">439</span></a>
</span><span id="LinearSVC-440"><a href="#LinearSVC-440"><span class="linenos">440</span></a><span class="sd">        `X` : np.ndarray</span>
</span><span id="LinearSVC-441"><a href="#LinearSVC-441"><span class="linenos">441</span></a><span class="sd">        - The input data of shape (n_samples, n_features)</span>
</span><span id="LinearSVC-442"><a href="#LinearSVC-442"><span class="linenos">442</span></a>
</span><span id="LinearSVC-443"><a href="#LinearSVC-443"><span class="linenos">443</span></a>
</span><span id="LinearSVC-444"><a href="#LinearSVC-444"><span class="linenos">444</span></a><span class="sd">        Returns</span>
</span><span id="LinearSVC-445"><a href="#LinearSVC-445"><span class="linenos">445</span></a><span class="sd">        -------</span>
</span><span id="LinearSVC-446"><a href="#LinearSVC-446"><span class="linenos">446</span></a>
</span><span id="LinearSVC-447"><a href="#LinearSVC-447"><span class="linenos">447</span></a><span class="sd">        `np.ndarray`</span>
</span><span id="LinearSVC-448"><a href="#LinearSVC-448"><span class="linenos">448</span></a><span class="sd">        - The predicted target data of shape (n_samples,)</span>
</span><span id="LinearSVC-449"><a href="#LinearSVC-449"><span class="linenos">449</span></a>
</span><span id="LinearSVC-450"><a href="#LinearSVC-450"><span class="linenos">450</span></a><span class="sd">        ---</span>
</span><span id="LinearSVC-451"><a href="#LinearSVC-451"><span class="linenos">451</span></a><span class="sd">        &quot;&quot;&quot;</span>
</span><span id="LinearSVC-452"><a href="#LinearSVC-452"><span class="linenos">452</span></a>        <span class="n">X</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_validate_data</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
</span><span id="LinearSVC-453"><a href="#LinearSVC-453"><span class="linenos">453</span></a>
</span><span id="LinearSVC-454"><a href="#LinearSVC-454"><span class="linenos">454</span></a>        <span class="c1"># Return 1 if the prediction is greater than or equal to 0, otherwise return -1</span>
</span><span id="LinearSVC-455"><a href="#LinearSVC-455"><span class="linenos">455</span></a>        <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">where</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_y_hat</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">_weights</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">_intercept</span><span class="p">)</span> <span class="o">&gt;=</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">)</span>
</span><span id="LinearSVC-456"><a href="#LinearSVC-456"><span class="linenos">456</span></a>
</span><span id="LinearSVC-457"><a href="#LinearSVC-457"><span class="linenos">457</span></a>    <span class="k">def</span> <span class="nf">score</span><span class="p">(</span>
</span><span id="LinearSVC-458"><a href="#LinearSVC-458"><span class="linenos">458</span></a>        <span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">,</span> <span class="n">Y</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">,</span> <span class="n">W</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span> <span class="n">b</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">float64</span> <span class="o">=</span> <span class="kc">None</span>
</span><span id="LinearSVC-459"><a href="#LinearSVC-459"><span class="linenos">459</span></a>    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">np</span><span class="o">.</span><span class="n">float64</span><span class="p">:</span>
</span><span id="LinearSVC-460"><a href="#LinearSVC-460"><span class="linenos">460</span></a><span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
</span><span id="LinearSVC-461"><a href="#LinearSVC-461"><span class="linenos">461</span></a><span class="sd">        ### Return the cost of the model given X, Y, W, and b.</span>
</span><span id="LinearSVC-462"><a href="#LinearSVC-462"><span class="linenos">462</span></a>
</span><span id="LinearSVC-463"><a href="#LinearSVC-463"><span class="linenos">463</span></a><span class="sd">        Parameters</span>
</span><span id="LinearSVC-464"><a href="#LinearSVC-464"><span class="linenos">464</span></a><span class="sd">        ----------</span>
</span><span id="LinearSVC-465"><a href="#LinearSVC-465"><span class="linenos">465</span></a>
</span><span id="LinearSVC-466"><a href="#LinearSVC-466"><span class="linenos">466</span></a><span class="sd">        `X` : np.ndarray</span>
</span><span id="LinearSVC-467"><a href="#LinearSVC-467"><span class="linenos">467</span></a><span class="sd">        - The input array of shape (n_samples, n_features)</span>
</span><span id="LinearSVC-468"><a href="#LinearSVC-468"><span class="linenos">468</span></a>
</span><span id="LinearSVC-469"><a href="#LinearSVC-469"><span class="linenos">469</span></a><span class="sd">        `Y` : np.ndarray</span>
</span><span id="LinearSVC-470"><a href="#LinearSVC-470"><span class="linenos">470</span></a><span class="sd">        - The output array of shape (n_samples,)</span>
</span><span id="LinearSVC-471"><a href="#LinearSVC-471"><span class="linenos">471</span></a>
</span><span id="LinearSVC-472"><a href="#LinearSVC-472"><span class="linenos">472</span></a><span class="sd">        `W` : np.ndarray, optional</span>
</span><span id="LinearSVC-473"><a href="#LinearSVC-473"><span class="linenos">473</span></a><span class="sd">        - If None, then the weight array will be cosidered as the trained</span>
</span><span id="LinearSVC-474"><a href="#LinearSVC-474"><span class="linenos">474</span></a><span class="sd">            weight array</span>
</span><span id="LinearSVC-475"><a href="#LinearSVC-475"><span class="linenos">475</span></a>
</span><span id="LinearSVC-476"><a href="#LinearSVC-476"><span class="linenos">476</span></a><span class="sd">        `b` : np.float64, optional</span>
</span><span id="LinearSVC-477"><a href="#LinearSVC-477"><span class="linenos">477</span></a><span class="sd">        - The intercept, by default None</span>
</span><span id="LinearSVC-478"><a href="#LinearSVC-478"><span class="linenos">478</span></a><span class="sd">        - If None, then the intercept will be cosidered as the trained</span>
</span><span id="LinearSVC-479"><a href="#LinearSVC-479"><span class="linenos">479</span></a><span class="sd">            intercept</span>
</span><span id="LinearSVC-480"><a href="#LinearSVC-480"><span class="linenos">480</span></a>
</span><span id="LinearSVC-481"><a href="#LinearSVC-481"><span class="linenos">481</span></a>
</span><span id="LinearSVC-482"><a href="#LinearSVC-482"><span class="linenos">482</span></a><span class="sd">        Returns</span>
</span><span id="LinearSVC-483"><a href="#LinearSVC-483"><span class="linenos">483</span></a><span class="sd">        -------</span>
</span><span id="LinearSVC-484"><a href="#LinearSVC-484"><span class="linenos">484</span></a>
</span><span id="LinearSVC-485"><a href="#LinearSVC-485"><span class="linenos">485</span></a><span class="sd">        `np.float64`</span>
</span><span id="LinearSVC-486"><a href="#LinearSVC-486"><span class="linenos">486</span></a><span class="sd">        - The cost of the model</span>
</span><span id="LinearSVC-487"><a href="#LinearSVC-487"><span class="linenos">487</span></a>
</span><span id="LinearSVC-488"><a href="#LinearSVC-488"><span class="linenos">488</span></a><span class="sd">        ---</span>
</span><span id="LinearSVC-489"><a href="#LinearSVC-489"><span class="linenos">489</span></a><span class="sd">        &quot;&quot;&quot;</span>
</span><span id="LinearSVC-490"><a href="#LinearSVC-490"><span class="linenos">490</span></a>        <span class="n">X</span><span class="p">,</span> <span class="n">Y</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_validate_data</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">Y</span><span class="p">)</span>
</span><span id="LinearSVC-491"><a href="#LinearSVC-491"><span class="linenos">491</span></a>
</span><span id="LinearSVC-492"><a href="#LinearSVC-492"><span class="linenos">492</span></a>        <span class="n">W</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_weights</span> <span class="k">if</span> <span class="n">W</span> <span class="ow">is</span> <span class="kc">None</span> <span class="k">else</span> <span class="n">W</span>
</span><span id="LinearSVC-493"><a href="#LinearSVC-493"><span class="linenos">493</span></a>        <span class="n">b</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_intercept</span> <span class="k">if</span> <span class="n">b</span> <span class="ow">is</span> <span class="kc">None</span> <span class="k">else</span> <span class="n">b</span>
</span><span id="LinearSVC-494"><a href="#LinearSVC-494"><span class="linenos">494</span></a>
</span><span id="LinearSVC-495"><a href="#LinearSVC-495"><span class="linenos">495</span></a>        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_cost</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">Y</span><span class="p">,</span> <span class="n">W</span><span class="p">,</span> <span class="n">b</span><span class="p">)</span>
</span></pre></div>


            <div class="docstring"><h1 id="support-vector-classifier">Support Vector Classifier</h1>

<p><code><a href="#LinearSVC">LinearSVC</a></code> (Support Vector Classifier) is a classification algorithm that aims to separate data into two classes by finding a hyperplane that maximizes the margin between them. It's effective for high-dimensional data and can handle small sample sizes efficiently. This algorithm seeks to create a decision boundary by minimizing classification errors and maximizing the margin between the classes.</p>

<hr />

<h2 id="mathematical-approach">Mathematical Approach</h2>

<p><code><a href="#LinearSVC">LinearSVC</a></code> aims to find a hyperplane that best separates two classes by minimizing a loss function known as the hinge loss. The decision boundary is represented as a linear combination of input features, weights, and an intercept:</p>

<pre><code>z = X * W - b
</code></pre>

<p>Where:</p>

<ul>
<li><code>X</code> is the input data matrix of shape <code>(n_samples, n_features)</code>.</li>
<li><code>W</code> is the weight vector of shape <code>(n_features,)</code>.</li>
<li><code>b</code> is the intercept.</li>
</ul>

<p>The predicted class <code>y_hat</code> is determined based on the sign of <code>z</code>:</p>

<pre><code>y_hat = sign(z)
</code></pre>

<hr />

<h3 id="hinge-loss">Hinge Loss</h3>

<p>The hinge loss function measures the degree of violation of a sample's classification. For a sample <code>(x_i, y_i)</code>, where <code>x_i</code> is the input data and <code>y_i</code> is the true class label (-1 or 1), the hinge loss is defined as:</p>

<pre><code>loss_i = max(0, 1 - y_i * z_i)
</code></pre>

<p>Where <code>z_i</code> is the linear combination for the <code>i</code>-th sample. The overall hinge loss for the entire dataset is the sum of individual hinge losses:</p>

<pre><code>loss = sum(max(0, 1 - y_i * z_i)) for all samples i
</code></pre>

<hr />

<h3 id="margin-and-support-vectors">Margin and Support Vectors</h3>

<p>The margin is the distance between the decision boundary and the closest data points. The goal is to maximize this margin while minimizing the hinge loss. Support vectors are the data points that are closest to the decision boundary and play a crucial role in defining the hyperplane.</p>

<p>Maximizing the margin is equivalent to minimizing the norm of the weight vector <code>W</code>:</p>

<pre><code>min (||W|| / 2)
</code></pre>

<hr />

<h3 id="regularization">Regularization</h3>

<p>Regularization is used to prevent overfitting by penalizing large weights. The regularization term is added to the loss function and is defined as:</p>

<pre><code>lambda_ * ||W||^2
</code></pre>

<p>Where <code>lambda_</code> is the regularization parameter.</p>

<hr />

<h3 id="optimization">Optimization</h3>

<p>The goal is to minimize the hinge loss and the regularization term. This is achieved by using gradient descent to iteratively update the weights and intercept. The gradient of the hinge loss function with respect to the weights <code>W</code> and intercept <code>b</code> is calculated for each sample. For correctly classified samples (<code>y * z &gt;= 1</code>), only the regularization term contributes to the gradient. For misclassified samples (<code>y * z &lt; 1</code>), both the regularization term and the hinge loss gradient contribute.</p>

<p>The gradients are averaged over all samples and used to update the weights and intercept using the learning rate. This process is repeated for the specified number of iterations.</p>

<hr />

<h2 id="usage">Usage</h2>

<p>To use the <code><a href="#LinearSVC">LinearSVC</a></code> model, follow these steps:</p>

<ol>
<li>Import the <code><a href="#LinearSVC">LinearSVC</a></code> class from the appropriate module.</li>
<li>Create an instance of the <code><a href="#LinearSVC">LinearSVC</a></code> class, specifying hyperparameters.</li>
<li>Fit the model to your training data using the <code><a href="#LinearSVC.fit">fit</a></code> method.</li>
<li>Make predictions on new data using the <code><a href="#LinearSVC.predict">predict</a></code> method.</li>
<li>Evaluate the model's performance using the <code><a href="#LinearSVC.score">score</a></code> method.</li>
</ol>

<div class="pdoc-code codehilite">
<pre><span></span><code><span class="kn">from</span> <span class="nn"><a href="">learnML.classification</a></span> <span class="kn">import</span> <span class="n">LinearSVC</span>

<span class="c1"># Create an instance of LinearSVC</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">LinearSVC</span><span class="p">(</span><span class="n">learning_rate</span><span class="o">=</span><span class="mf">0.001</span><span class="p">,</span> <span class="n">lambda_</span><span class="o">=</span><span class="mf">0.01</span><span class="p">,</span> <span class="n">n_iterations</span><span class="o">=</span><span class="mi">1000</span><span class="p">)</span>

<span class="c1"># Fit the model to training data</span>
<span class="n">model</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">Y_train</span><span class="p">)</span>

<span class="c1"># Make predictions on new data</span>
<span class="n">predictions</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_test</span><span class="p">)</span>

<span class="c1"># Calculate the model&#39;s score</span>
<span class="n">model_score</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">score</span><span class="p">(</span><span class="n">X_test</span><span class="p">,</span> <span class="n">Y_test</span><span class="p">)</span>
</code></pre>
</div>

<hr />

<h2 id="advantages">Advantages</h2>

<ul>
<li>Effective in high dimensional spaces</li>
<li>Works well with small number of samples</li>
<li>Works efficiently when there is a clear margin of separation between classes</li>
<li>Uses a subset of training points in the decision function (called support vectors), so it is also memory efficient</li>
</ul>

<h2 id="disadvantages">Disadvantages</h2>

<ul>
<li>Not suitable for large number of samples (training time is higher)</li>
<li>Not suitable for noisy data with overlapping classes</li>
</ul>

<hr />
</div>


                            <div id="LinearSVC.__init__" class="classattr">
                                        <input id="LinearSVC.__init__-view-source" class="view-source-toggle-state" type="checkbox" aria-hidden="true" tabindex="-1">
<div class="attr function">
            
        <span class="name">LinearSVC</span><span class="signature pdoc-code multiline">(<span class="param">	<span class="n">learning_rate</span><span class="p">:</span> <span class="n">numpy</span><span class="o">.</span><span class="n">float64</span> <span class="o">=</span> <span class="mf">0.001</span>,</span><span class="param">	<span class="n">n_iterations</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">1000</span>,</span><span class="param">	<span class="n">lambda_</span><span class="p">:</span> <span class="n">numpy</span><span class="o">.</span><span class="n">float64</span> <span class="o">=</span> <span class="mf">0.01</span>,</span><span class="param">	<span class="n">x_scalar</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="n">learnML</span><span class="o">.</span><span class="n">interfaces</span><span class="o">.</span><span class="n">ifeature_engineering</span><span class="o">.</span><span class="n">IFeatureEngineering</span><span class="p">,</span> <span class="n">List</span><span class="p">[</span><span class="n">learnML</span><span class="o">.</span><span class="n">interfaces</span><span class="o">.</span><span class="n">ifeature_engineering</span><span class="o">.</span><span class="n">IFeatureEngineering</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span>,</span><span class="param">	<span class="n">debug</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span>,</span><span class="param">	<span class="n">copy_x</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span></span>)</span>

                <label class="view-source-button" for="LinearSVC.__init__-view-source"><span>View Source</span></label>

    </div>
    <a class="headerlink" href="#LinearSVC.__init__"></a>
            <div class="pdoc-code codehilite"><pre><span></span><span id="LinearSVC.__init__-129"><a href="#LinearSVC.__init__-129"><span class="linenos">129</span></a>    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span>
</span><span id="LinearSVC.__init__-130"><a href="#LinearSVC.__init__-130"><span class="linenos">130</span></a>        <span class="bp">self</span><span class="p">,</span>
</span><span id="LinearSVC.__init__-131"><a href="#LinearSVC.__init__-131"><span class="linenos">131</span></a>        <span class="n">learning_rate</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">float64</span> <span class="o">=</span> <span class="mf">0.001</span><span class="p">,</span>
</span><span id="LinearSVC.__init__-132"><a href="#LinearSVC.__init__-132"><span class="linenos">132</span></a>        <span class="n">n_iterations</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">1000</span><span class="p">,</span>
</span><span id="LinearSVC.__init__-133"><a href="#LinearSVC.__init__-133"><span class="linenos">133</span></a>        <span class="n">lambda_</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">float64</span> <span class="o">=</span> <span class="mf">0.01</span><span class="p">,</span>
</span><span id="LinearSVC.__init__-134"><a href="#LinearSVC.__init__-134"><span class="linenos">134</span></a>        <span class="n">x_scalar</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="n">IFeatureEngineering</span><span class="p">,</span> <span class="n">List</span><span class="p">[</span><span class="n">IFeatureEngineering</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
</span><span id="LinearSVC.__init__-135"><a href="#LinearSVC.__init__-135"><span class="linenos">135</span></a>        <span class="n">debug</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
</span><span id="LinearSVC.__init__-136"><a href="#LinearSVC.__init__-136"><span class="linenos">136</span></a>        <span class="n">copy_x</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
</span><span id="LinearSVC.__init__-137"><a href="#LinearSVC.__init__-137"><span class="linenos">137</span></a>    <span class="p">):</span>
</span><span id="LinearSVC.__init__-138"><a href="#LinearSVC.__init__-138"><span class="linenos">138</span></a><span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
</span><span id="LinearSVC.__init__-139"><a href="#LinearSVC.__init__-139"><span class="linenos">139</span></a><span class="sd">        Parameters</span>
</span><span id="LinearSVC.__init__-140"><a href="#LinearSVC.__init__-140"><span class="linenos">140</span></a><span class="sd">        ----------</span>
</span><span id="LinearSVC.__init__-141"><a href="#LinearSVC.__init__-141"><span class="linenos">141</span></a>
</span><span id="LinearSVC.__init__-142"><a href="#LinearSVC.__init__-142"><span class="linenos">142</span></a><span class="sd">        `learning_rate` : np.float64, optional</span>
</span><span id="LinearSVC.__init__-143"><a href="#LinearSVC.__init__-143"><span class="linenos">143</span></a><span class="sd">        - The learning rate, by default 0.001</span>
</span><span id="LinearSVC.__init__-144"><a href="#LinearSVC.__init__-144"><span class="linenos">144</span></a><span class="sd">        - The learning rate determines how much the weights are updated at each iteration</span>
</span><span id="LinearSVC.__init__-145"><a href="#LinearSVC.__init__-145"><span class="linenos">145</span></a><span class="sd">        - A low learning rate will take longer to converge, but a high learning rate may overshoot the optimal solution</span>
</span><span id="LinearSVC.__init__-146"><a href="#LinearSVC.__init__-146"><span class="linenos">146</span></a>
</span><span id="LinearSVC.__init__-147"><a href="#LinearSVC.__init__-147"><span class="linenos">147</span></a><span class="sd">        `n_iterations` : int, optional</span>
</span><span id="LinearSVC.__init__-148"><a href="#LinearSVC.__init__-148"><span class="linenos">148</span></a><span class="sd">        - The number of iterations, by default 1000</span>
</span><span id="LinearSVC.__init__-149"><a href="#LinearSVC.__init__-149"><span class="linenos">149</span></a><span class="sd">        - The number of iterations determines how many times the weights are updated</span>
</span><span id="LinearSVC.__init__-150"><a href="#LinearSVC.__init__-150"><span class="linenos">150</span></a><span class="sd">        - A higher number of iterations will take longer to converge, but a lower number of iterations may not be enough to converge</span>
</span><span id="LinearSVC.__init__-151"><a href="#LinearSVC.__init__-151"><span class="linenos">151</span></a>
</span><span id="LinearSVC.__init__-152"><a href="#LinearSVC.__init__-152"><span class="linenos">152</span></a><span class="sd">        `lambda_` : np.float64, optional</span>
</span><span id="LinearSVC.__init__-153"><a href="#LinearSVC.__init__-153"><span class="linenos">153</span></a><span class="sd">        - The regularization parameter, by default 0</span>
</span><span id="LinearSVC.__init__-154"><a href="#LinearSVC.__init__-154"><span class="linenos">154</span></a><span class="sd">        - The regularization parameter helps prevent overfitting by penalizing large weights</span>
</span><span id="LinearSVC.__init__-155"><a href="#LinearSVC.__init__-155"><span class="linenos">155</span></a><span class="sd">        - A higher regularization parameter will penalize large weights more, but a lower regularization parameter may not be enough to prevent overfitting</span>
</span><span id="LinearSVC.__init__-156"><a href="#LinearSVC.__init__-156"><span class="linenos">156</span></a>
</span><span id="LinearSVC.__init__-157"><a href="#LinearSVC.__init__-157"><span class="linenos">157</span></a><span class="sd">        `x_scalar` : Union[IFeatureEngineering, List[IFeatureEngineering]], optional</span>
</span><span id="LinearSVC.__init__-158"><a href="#LinearSVC.__init__-158"><span class="linenos">158</span></a><span class="sd">        - The feature engineering for the input data, by default None</span>
</span><span id="LinearSVC.__init__-159"><a href="#LinearSVC.__init__-159"><span class="linenos">159</span></a><span class="sd">        - If a list is provided, the feature engineering will be applied in the order provided</span>
</span><span id="LinearSVC.__init__-160"><a href="#LinearSVC.__init__-160"><span class="linenos">160</span></a><span class="sd">        - If a single feature engineering is provided, it will be applied to all input data</span>
</span><span id="LinearSVC.__init__-161"><a href="#LinearSVC.__init__-161"><span class="linenos">161</span></a>
</span><span id="LinearSVC.__init__-162"><a href="#LinearSVC.__init__-162"><span class="linenos">162</span></a><span class="sd">        `debug` : bool, optional</span>
</span><span id="LinearSVC.__init__-163"><a href="#LinearSVC.__init__-163"><span class="linenos">163</span></a><span class="sd">        - Whether to print debug messages, by default True</span>
</span><span id="LinearSVC.__init__-164"><a href="#LinearSVC.__init__-164"><span class="linenos">164</span></a><span class="sd">        - Debug messages include the cost at each iteration</span>
</span><span id="LinearSVC.__init__-165"><a href="#LinearSVC.__init__-165"><span class="linenos">165</span></a>
</span><span id="LinearSVC.__init__-166"><a href="#LinearSVC.__init__-166"><span class="linenos">166</span></a><span class="sd">        `copy_x` : bool, optional</span>
</span><span id="LinearSVC.__init__-167"><a href="#LinearSVC.__init__-167"><span class="linenos">167</span></a><span class="sd">        - Whether to copy the input array, by default True</span>
</span><span id="LinearSVC.__init__-168"><a href="#LinearSVC.__init__-168"><span class="linenos">168</span></a><span class="sd">        - If False, the input array will be overwritten</span>
</span><span id="LinearSVC.__init__-169"><a href="#LinearSVC.__init__-169"><span class="linenos">169</span></a>
</span><span id="LinearSVC.__init__-170"><a href="#LinearSVC.__init__-170"><span class="linenos">170</span></a><span class="sd">        ---</span>
</span><span id="LinearSVC.__init__-171"><a href="#LinearSVC.__init__-171"><span class="linenos">171</span></a><span class="sd">        &quot;&quot;&quot;</span>
</span><span id="LinearSVC.__init__-172"><a href="#LinearSVC.__init__-172"><span class="linenos">172</span></a>        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span>
</span><span id="LinearSVC.__init__-173"><a href="#LinearSVC.__init__-173"><span class="linenos">173</span></a>            <span class="n">learning_rate</span><span class="o">=</span><span class="n">learning_rate</span><span class="p">,</span>
</span><span id="LinearSVC.__init__-174"><a href="#LinearSVC.__init__-174"><span class="linenos">174</span></a>            <span class="n">n_iterations</span><span class="o">=</span><span class="n">n_iterations</span><span class="p">,</span>
</span><span id="LinearSVC.__init__-175"><a href="#LinearSVC.__init__-175"><span class="linenos">175</span></a>            <span class="n">debug</span><span class="o">=</span><span class="n">debug</span><span class="p">,</span>
</span><span id="LinearSVC.__init__-176"><a href="#LinearSVC.__init__-176"><span class="linenos">176</span></a>            <span class="n">copy_x</span><span class="o">=</span><span class="n">copy_x</span><span class="p">,</span>
</span><span id="LinearSVC.__init__-177"><a href="#LinearSVC.__init__-177"><span class="linenos">177</span></a>        <span class="p">)</span>
</span><span id="LinearSVC.__init__-178"><a href="#LinearSVC.__init__-178"><span class="linenos">178</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">_lambda</span> <span class="o">=</span> <span class="n">lambda_</span>
</span><span id="LinearSVC.__init__-179"><a href="#LinearSVC.__init__-179"><span class="linenos">179</span></a>
</span><span id="LinearSVC.__init__-180"><a href="#LinearSVC.__init__-180"><span class="linenos">180</span></a>        <span class="k">if</span> <span class="n">x_scalar</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
</span><span id="LinearSVC.__init__-181"><a href="#LinearSVC.__init__-181"><span class="linenos">181</span></a>            <span class="n">x_scalar</span> <span class="o">=</span> <span class="p">[]</span>
</span><span id="LinearSVC.__init__-182"><a href="#LinearSVC.__init__-182"><span class="linenos">182</span></a>        <span class="k">elif</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">x_scalar</span><span class="p">,</span> <span class="n">IFeatureEngineering</span><span class="p">):</span>
</span><span id="LinearSVC.__init__-183"><a href="#LinearSVC.__init__-183"><span class="linenos">183</span></a>            <span class="n">x_scalar</span> <span class="o">=</span> <span class="p">[</span><span class="n">x_scalar</span><span class="p">]</span>
</span><span id="LinearSVC.__init__-184"><a href="#LinearSVC.__init__-184"><span class="linenos">184</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">_x_scalar</span> <span class="o">=</span> <span class="n">x_scalar</span>
</span></pre></div>


            <div class="docstring"><h2 id="parameters">Parameters</h2>

<p><code>learning_rate</code> : np.float64, optional</p>

<ul>
<li>The learning rate, by default 0.001</li>
<li>The learning rate determines how much the weights are updated at each iteration</li>
<li>A low learning rate will take longer to converge, but a high learning rate may overshoot the optimal solution</li>
</ul>

<p><code>n_iterations</code> : int, optional</p>

<ul>
<li>The number of iterations, by default 1000</li>
<li>The number of iterations determines how many times the weights are updated</li>
<li>A higher number of iterations will take longer to converge, but a lower number of iterations may not be enough to converge</li>
</ul>

<p><code>lambda_</code> : np.float64, optional</p>

<ul>
<li>The regularization parameter, by default 0</li>
<li>The regularization parameter helps prevent overfitting by penalizing large weights</li>
<li>A higher regularization parameter will penalize large weights more, but a lower regularization parameter may not be enough to prevent overfitting</li>
</ul>

<p><code>x_scalar</code> : Union[IFeatureEngineering, List[IFeatureEngineering]], optional</p>

<ul>
<li>The feature engineering for the input data, by default None</li>
<li>If a list is provided, the feature engineering will be applied in the order provided</li>
<li>If a single feature engineering is provided, it will be applied to all input data</li>
</ul>

<p><code>debug</code> : bool, optional</p>

<ul>
<li>Whether to print debug messages, by default True</li>
<li>Debug messages include the cost at each iteration</li>
</ul>

<p><code>copy_x</code> : bool, optional</p>

<ul>
<li>Whether to copy the input array, by default True</li>
<li>If False, the input array will be overwritten</li>
</ul>

<hr />
</div>


                            </div>
                            <div id="LinearSVC.fit" class="classattr">
                                        <input id="LinearSVC.fit-view-source" class="view-source-toggle-state" type="checkbox" aria-hidden="true" tabindex="-1">
<div class="attr function">
            
        <span class="def">def</span>
        <span class="name">fit</span><span class="signature pdoc-code multiline">(<span class="param">	<span class="bp">self</span>,</span><span class="param">	<span class="n">X</span><span class="p">:</span> <span class="n">numpy</span><span class="o">.</span><span class="n">ndarray</span>,</span><span class="param">	<span class="n">Y</span><span class="p">:</span> <span class="n">numpy</span><span class="o">.</span><span class="n">ndarray</span>,</span><span class="param">	<span class="n">W</span><span class="p">:</span> <span class="n">numpy</span><span class="o">.</span><span class="n">ndarray</span> <span class="o">=</span> <span class="kc">None</span>,</span><span class="param">	<span class="n">b</span><span class="p">:</span> <span class="n">numpy</span><span class="o">.</span><span class="n">float64</span> <span class="o">=</span> <span class="mf">0.0</span></span><span class="return-annotation">) -> <span class="kc">None</span>:</span></span>

                <label class="view-source-button" for="LinearSVC.fit-view-source"><span>View Source</span></label>

    </div>
    <a class="headerlink" href="#LinearSVC.fit"></a>
            <div class="pdoc-code codehilite"><pre><span></span><span id="LinearSVC.fit-355"><a href="#LinearSVC.fit-355"><span class="linenos">355</span></a>    <span class="k">def</span> <span class="nf">fit</span><span class="p">(</span>
</span><span id="LinearSVC.fit-356"><a href="#LinearSVC.fit-356"><span class="linenos">356</span></a>        <span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">,</span> <span class="n">Y</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">,</span> <span class="n">W</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span> <span class="n">b</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">float64</span> <span class="o">=</span> <span class="mf">0.0</span>
</span><span id="LinearSVC.fit-357"><a href="#LinearSVC.fit-357"><span class="linenos">357</span></a>    <span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
</span><span id="LinearSVC.fit-358"><a href="#LinearSVC.fit-358"><span class="linenos">358</span></a><span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
</span><span id="LinearSVC.fit-359"><a href="#LinearSVC.fit-359"><span class="linenos">359</span></a><span class="sd">        ### Fit the model to the data</span>
</span><span id="LinearSVC.fit-360"><a href="#LinearSVC.fit-360"><span class="linenos">360</span></a>
</span><span id="LinearSVC.fit-361"><a href="#LinearSVC.fit-361"><span class="linenos">361</span></a><span class="sd">        Parameters</span>
</span><span id="LinearSVC.fit-362"><a href="#LinearSVC.fit-362"><span class="linenos">362</span></a><span class="sd">        ----------</span>
</span><span id="LinearSVC.fit-363"><a href="#LinearSVC.fit-363"><span class="linenos">363</span></a>
</span><span id="LinearSVC.fit-364"><a href="#LinearSVC.fit-364"><span class="linenos">364</span></a><span class="sd">        `X` : np.ndarray</span>
</span><span id="LinearSVC.fit-365"><a href="#LinearSVC.fit-365"><span class="linenos">365</span></a><span class="sd">        - The input data of shape (n_samples, n_features)</span>
</span><span id="LinearSVC.fit-366"><a href="#LinearSVC.fit-366"><span class="linenos">366</span></a>
</span><span id="LinearSVC.fit-367"><a href="#LinearSVC.fit-367"><span class="linenos">367</span></a><span class="sd">        `y` : np.ndarray</span>
</span><span id="LinearSVC.fit-368"><a href="#LinearSVC.fit-368"><span class="linenos">368</span></a><span class="sd">        - The target data of shape (n_samples, 1) or (n_samples,)</span>
</span><span id="LinearSVC.fit-369"><a href="#LinearSVC.fit-369"><span class="linenos">369</span></a>
</span><span id="LinearSVC.fit-370"><a href="#LinearSVC.fit-370"><span class="linenos">370</span></a><span class="sd">        `W` : np.ndarray, optional</span>
</span><span id="LinearSVC.fit-371"><a href="#LinearSVC.fit-371"><span class="linenos">371</span></a><span class="sd">        - The weights of shape (n_features,), by default None</span>
</span><span id="LinearSVC.fit-372"><a href="#LinearSVC.fit-372"><span class="linenos">372</span></a><span class="sd">        - If None, then the weight array will be initialized to an array of</span>
</span><span id="LinearSVC.fit-373"><a href="#LinearSVC.fit-373"><span class="linenos">373</span></a><span class="sd">            zeros of shape (n_features,)</span>
</span><span id="LinearSVC.fit-374"><a href="#LinearSVC.fit-374"><span class="linenos">374</span></a><span class="sd">        - If not None, then the weight array will be initialized to the given</span>
</span><span id="LinearSVC.fit-375"><a href="#LinearSVC.fit-375"><span class="linenos">375</span></a><span class="sd">            array</span>
</span><span id="LinearSVC.fit-376"><a href="#LinearSVC.fit-376"><span class="linenos">376</span></a>
</span><span id="LinearSVC.fit-377"><a href="#LinearSVC.fit-377"><span class="linenos">377</span></a><span class="sd">        `b` : np.float64, optional</span>
</span><span id="LinearSVC.fit-378"><a href="#LinearSVC.fit-378"><span class="linenos">378</span></a><span class="sd">        - The intercept, by default 0.0</span>
</span><span id="LinearSVC.fit-379"><a href="#LinearSVC.fit-379"><span class="linenos">379</span></a><span class="sd">        - If None, then the intercept will be initialized to 0.0</span>
</span><span id="LinearSVC.fit-380"><a href="#LinearSVC.fit-380"><span class="linenos">380</span></a><span class="sd">        - If not None, then the intercept will be initialized to the given</span>
</span><span id="LinearSVC.fit-381"><a href="#LinearSVC.fit-381"><span class="linenos">381</span></a><span class="sd">            value</span>
</span><span id="LinearSVC.fit-382"><a href="#LinearSVC.fit-382"><span class="linenos">382</span></a>
</span><span id="LinearSVC.fit-383"><a href="#LinearSVC.fit-383"><span class="linenos">383</span></a>
</span><span id="LinearSVC.fit-384"><a href="#LinearSVC.fit-384"><span class="linenos">384</span></a><span class="sd">        Returns</span>
</span><span id="LinearSVC.fit-385"><a href="#LinearSVC.fit-385"><span class="linenos">385</span></a><span class="sd">        -------</span>
</span><span id="LinearSVC.fit-386"><a href="#LinearSVC.fit-386"><span class="linenos">386</span></a>
</span><span id="LinearSVC.fit-387"><a href="#LinearSVC.fit-387"><span class="linenos">387</span></a><span class="sd">        None</span>
</span><span id="LinearSVC.fit-388"><a href="#LinearSVC.fit-388"><span class="linenos">388</span></a>
</span><span id="LinearSVC.fit-389"><a href="#LinearSVC.fit-389"><span class="linenos">389</span></a><span class="sd">        ---</span>
</span><span id="LinearSVC.fit-390"><a href="#LinearSVC.fit-390"><span class="linenos">390</span></a><span class="sd">        &quot;&quot;&quot;</span>
</span><span id="LinearSVC.fit-391"><a href="#LinearSVC.fit-391"><span class="linenos">391</span></a>        <span class="n">X</span><span class="p">,</span> <span class="n">Y</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_validate_data</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">Y</span><span class="p">)</span>
</span><span id="LinearSVC.fit-392"><a href="#LinearSVC.fit-392"><span class="linenos">392</span></a>
</span><span id="LinearSVC.fit-393"><a href="#LinearSVC.fit-393"><span class="linenos">393</span></a>        <span class="k">assert</span> <span class="n">X</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">==</span> <span class="n">Y</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="s2">&quot;The number of samples must be equal&quot;</span>
</span><span id="LinearSVC.fit-394"><a href="#LinearSVC.fit-394"><span class="linenos">394</span></a>
</span><span id="LinearSVC.fit-395"><a href="#LinearSVC.fit-395"><span class="linenos">395</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">_weights</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">X</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span> <span class="k">if</span> <span class="n">W</span> <span class="ow">is</span> <span class="kc">None</span> <span class="k">else</span> <span class="n">W</span>
</span><span id="LinearSVC.fit-396"><a href="#LinearSVC.fit-396"><span class="linenos">396</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">_intercept</span> <span class="o">=</span> <span class="n">b</span>
</span><span id="LinearSVC.fit-397"><a href="#LinearSVC.fit-397"><span class="linenos">397</span></a>
</span><span id="LinearSVC.fit-398"><a href="#LinearSVC.fit-398"><span class="linenos">398</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">_cost_history</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span>
</span><span id="LinearSVC.fit-399"><a href="#LinearSVC.fit-399"><span class="linenos">399</span></a>            <span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">_cost</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">Y</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">_weights</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">_intercept</span><span class="p">)]</span>
</span><span id="LinearSVC.fit-400"><a href="#LinearSVC.fit-400"><span class="linenos">400</span></a>        <span class="p">)</span>
</span><span id="LinearSVC.fit-401"><a href="#LinearSVC.fit-401"><span class="linenos">401</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">_params_history</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span>
</span><span id="LinearSVC.fit-402"><a href="#LinearSVC.fit-402"><span class="linenos">402</span></a>            <span class="p">[[</span><span class="bp">self</span><span class="o">.</span><span class="n">_weights</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">_intercept</span><span class="p">]],</span> <span class="n">dtype</span><span class="o">=</span><span class="nb">object</span>
</span><span id="LinearSVC.fit-403"><a href="#LinearSVC.fit-403"><span class="linenos">403</span></a>        <span class="p">)</span>
</span><span id="LinearSVC.fit-404"><a href="#LinearSVC.fit-404"><span class="linenos">404</span></a>
</span><span id="LinearSVC.fit-405"><a href="#LinearSVC.fit-405"><span class="linenos">405</span></a>        <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_n_iterations</span><span class="p">):</span>
</span><span id="LinearSVC.fit-406"><a href="#LinearSVC.fit-406"><span class="linenos">406</span></a>            <span class="n">dw</span><span class="p">,</span> <span class="n">db</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_gradient</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">Y</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">_weights</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">_intercept</span><span class="p">)</span>
</span><span id="LinearSVC.fit-407"><a href="#LinearSVC.fit-407"><span class="linenos">407</span></a>
</span><span id="LinearSVC.fit-408"><a href="#LinearSVC.fit-408"><span class="linenos">408</span></a>            <span class="bp">self</span><span class="o">.</span><span class="n">_weights</span> <span class="o">-=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_learning_rate</span> <span class="o">*</span> <span class="n">dw</span>
</span><span id="LinearSVC.fit-409"><a href="#LinearSVC.fit-409"><span class="linenos">409</span></a>            <span class="bp">self</span><span class="o">.</span><span class="n">_intercept</span> <span class="o">-=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_learning_rate</span> <span class="o">*</span> <span class="n">db</span>
</span><span id="LinearSVC.fit-410"><a href="#LinearSVC.fit-410"><span class="linenos">410</span></a>
</span><span id="LinearSVC.fit-411"><a href="#LinearSVC.fit-411"><span class="linenos">411</span></a>            <span class="c1"># Save cost and params history</span>
</span><span id="LinearSVC.fit-412"><a href="#LinearSVC.fit-412"><span class="linenos">412</span></a>            <span class="n">cost</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_cost</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">Y</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">_weights</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">_intercept</span><span class="p">)</span>
</span><span id="LinearSVC.fit-413"><a href="#LinearSVC.fit-413"><span class="linenos">413</span></a>
</span><span id="LinearSVC.fit-414"><a href="#LinearSVC.fit-414"><span class="linenos">414</span></a>            <span class="k">if</span> <span class="n">cost</span> <span class="o">==</span> <span class="n">np</span><span class="o">.</span><span class="n">nan</span> <span class="ow">or</span> <span class="n">cost</span> <span class="o">==</span> <span class="n">np</span><span class="o">.</span><span class="n">inf</span><span class="p">:</span>
</span><span id="LinearSVC.fit-415"><a href="#LinearSVC.fit-415"><span class="linenos">415</span></a>                <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
</span><span id="LinearSVC.fit-416"><a href="#LinearSVC.fit-416"><span class="linenos">416</span></a>                    <span class="s2">&quot;Gradient descent failed. Try normalizing the input array or reducing the learning rate. &quot;</span>
</span><span id="LinearSVC.fit-417"><a href="#LinearSVC.fit-417"><span class="linenos">417</span></a>                    <span class="s2">&quot;If the problem persists, try reducing the number of iterations.&quot;</span>
</span><span id="LinearSVC.fit-418"><a href="#LinearSVC.fit-418"><span class="linenos">418</span></a>                <span class="p">)</span>
</span><span id="LinearSVC.fit-419"><a href="#LinearSVC.fit-419"><span class="linenos">419</span></a>
</span><span id="LinearSVC.fit-420"><a href="#LinearSVC.fit-420"><span class="linenos">420</span></a>            <span class="bp">self</span><span class="o">.</span><span class="n">_cost_history</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_cost_history</span><span class="p">,</span> <span class="n">cost</span><span class="p">)</span>
</span><span id="LinearSVC.fit-421"><a href="#LinearSVC.fit-421"><span class="linenos">421</span></a>            <span class="bp">self</span><span class="o">.</span><span class="n">_params_history</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">append</span><span class="p">(</span>
</span><span id="LinearSVC.fit-422"><a href="#LinearSVC.fit-422"><span class="linenos">422</span></a>                <span class="bp">self</span><span class="o">.</span><span class="n">_params_history</span><span class="p">,</span>
</span><span id="LinearSVC.fit-423"><a href="#LinearSVC.fit-423"><span class="linenos">423</span></a>                <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([[</span><span class="bp">self</span><span class="o">.</span><span class="n">_weights</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">_intercept</span><span class="p">]],</span> <span class="n">dtype</span><span class="o">=</span><span class="nb">object</span><span class="p">),</span>
</span><span id="LinearSVC.fit-424"><a href="#LinearSVC.fit-424"><span class="linenos">424</span></a>                <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span>
</span><span id="LinearSVC.fit-425"><a href="#LinearSVC.fit-425"><span class="linenos">425</span></a>            <span class="p">)</span>
</span><span id="LinearSVC.fit-426"><a href="#LinearSVC.fit-426"><span class="linenos">426</span></a>
</span><span id="LinearSVC.fit-427"><a href="#LinearSVC.fit-427"><span class="linenos">427</span></a>            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_debug</span> <span class="ow">and</span> <span class="n">i</span> <span class="o">%</span> <span class="bp">self</span><span class="o">.</span><span class="n">_debug_freq</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
</span><span id="LinearSVC.fit-428"><a href="#LinearSVC.fit-428"><span class="linenos">428</span></a>                <span class="bp">self</span><span class="o">.</span><span class="n">_debug_print</span><span class="p">(</span><span class="n">i</span><span class="p">,</span> <span class="n">cost</span><span class="p">)</span>
</span><span id="LinearSVC.fit-429"><a href="#LinearSVC.fit-429"><span class="linenos">429</span></a>
</span><span id="LinearSVC.fit-430"><a href="#LinearSVC.fit-430"><span class="linenos">430</span></a>        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_debug</span><span class="p">:</span>
</span><span id="LinearSVC.fit-431"><a href="#LinearSVC.fit-431"><span class="linenos">431</span></a>            <span class="bp">self</span><span class="o">.</span><span class="n">_debug_print</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_n_iterations</span><span class="p">,</span> <span class="n">cost</span><span class="p">)</span>
</span></pre></div>


            <div class="docstring"><h3 id="fit-the-model-to-the-data">Fit the model to the data</h3>

<h2 id="parameters">Parameters</h2>

<p><code>X</code> : np.ndarray</p>

<ul>
<li>The input data of shape (n_samples, n_features)</li>
</ul>

<p><code>y</code> : np.ndarray</p>

<ul>
<li>The target data of shape (n_samples, 1) or (n_samples,)</li>
</ul>

<p><code>W</code> : np.ndarray, optional</p>

<ul>
<li>The weights of shape (n_features,), by default None</li>
<li>If None, then the weight array will be initialized to an array of
zeros of shape (n_features,)</li>
<li>If not None, then the weight array will be initialized to the given
array</li>
</ul>

<p><code>b</code> : np.float64, optional</p>

<ul>
<li>The intercept, by default 0.0</li>
<li>If None, then the intercept will be initialized to 0.0</li>
<li>If not None, then the intercept will be initialized to the given
value</li>
</ul>

<h2 id="returns">Returns</h2>

<p>None</p>

<hr />
</div>


                            </div>
                            <div id="LinearSVC.predict" class="classattr">
                                        <input id="LinearSVC.predict-view-source" class="view-source-toggle-state" type="checkbox" aria-hidden="true" tabindex="-1">
<div class="attr function">
            
        <span class="def">def</span>
        <span class="name">predict</span><span class="signature pdoc-code condensed">(<span class="param"><span class="bp">self</span>, </span><span class="param"><span class="n">X</span><span class="p">:</span> <span class="n">numpy</span><span class="o">.</span><span class="n">ndarray</span></span><span class="return-annotation">) -> <span class="n">numpy</span><span class="o">.</span><span class="n">ndarray</span>:</span></span>

                <label class="view-source-button" for="LinearSVC.predict-view-source"><span>View Source</span></label>

    </div>
    <a class="headerlink" href="#LinearSVC.predict"></a>
            <div class="pdoc-code codehilite"><pre><span></span><span id="LinearSVC.predict-433"><a href="#LinearSVC.predict-433"><span class="linenos">433</span></a>    <span class="k">def</span> <span class="nf">predict</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">:</span>
</span><span id="LinearSVC.predict-434"><a href="#LinearSVC.predict-434"><span class="linenos">434</span></a><span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
</span><span id="LinearSVC.predict-435"><a href="#LinearSVC.predict-435"><span class="linenos">435</span></a><span class="sd">        ### Predict the target data</span>
</span><span id="LinearSVC.predict-436"><a href="#LinearSVC.predict-436"><span class="linenos">436</span></a>
</span><span id="LinearSVC.predict-437"><a href="#LinearSVC.predict-437"><span class="linenos">437</span></a><span class="sd">        Parameters</span>
</span><span id="LinearSVC.predict-438"><a href="#LinearSVC.predict-438"><span class="linenos">438</span></a><span class="sd">        ----------</span>
</span><span id="LinearSVC.predict-439"><a href="#LinearSVC.predict-439"><span class="linenos">439</span></a>
</span><span id="LinearSVC.predict-440"><a href="#LinearSVC.predict-440"><span class="linenos">440</span></a><span class="sd">        `X` : np.ndarray</span>
</span><span id="LinearSVC.predict-441"><a href="#LinearSVC.predict-441"><span class="linenos">441</span></a><span class="sd">        - The input data of shape (n_samples, n_features)</span>
</span><span id="LinearSVC.predict-442"><a href="#LinearSVC.predict-442"><span class="linenos">442</span></a>
</span><span id="LinearSVC.predict-443"><a href="#LinearSVC.predict-443"><span class="linenos">443</span></a>
</span><span id="LinearSVC.predict-444"><a href="#LinearSVC.predict-444"><span class="linenos">444</span></a><span class="sd">        Returns</span>
</span><span id="LinearSVC.predict-445"><a href="#LinearSVC.predict-445"><span class="linenos">445</span></a><span class="sd">        -------</span>
</span><span id="LinearSVC.predict-446"><a href="#LinearSVC.predict-446"><span class="linenos">446</span></a>
</span><span id="LinearSVC.predict-447"><a href="#LinearSVC.predict-447"><span class="linenos">447</span></a><span class="sd">        `np.ndarray`</span>
</span><span id="LinearSVC.predict-448"><a href="#LinearSVC.predict-448"><span class="linenos">448</span></a><span class="sd">        - The predicted target data of shape (n_samples,)</span>
</span><span id="LinearSVC.predict-449"><a href="#LinearSVC.predict-449"><span class="linenos">449</span></a>
</span><span id="LinearSVC.predict-450"><a href="#LinearSVC.predict-450"><span class="linenos">450</span></a><span class="sd">        ---</span>
</span><span id="LinearSVC.predict-451"><a href="#LinearSVC.predict-451"><span class="linenos">451</span></a><span class="sd">        &quot;&quot;&quot;</span>
</span><span id="LinearSVC.predict-452"><a href="#LinearSVC.predict-452"><span class="linenos">452</span></a>        <span class="n">X</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_validate_data</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
</span><span id="LinearSVC.predict-453"><a href="#LinearSVC.predict-453"><span class="linenos">453</span></a>
</span><span id="LinearSVC.predict-454"><a href="#LinearSVC.predict-454"><span class="linenos">454</span></a>        <span class="c1"># Return 1 if the prediction is greater than or equal to 0, otherwise return -1</span>
</span><span id="LinearSVC.predict-455"><a href="#LinearSVC.predict-455"><span class="linenos">455</span></a>        <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">where</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_y_hat</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">_weights</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">_intercept</span><span class="p">)</span> <span class="o">&gt;=</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">)</span>
</span></pre></div>


            <div class="docstring"><h3 id="predict-the-target-data">Predict the target data</h3>

<h2 id="parameters">Parameters</h2>

<p><code>X</code> : np.ndarray</p>

<ul>
<li>The input data of shape (n_samples, n_features)</li>
</ul>

<h2 id="returns">Returns</h2>

<p><code>np.ndarray</code></p>

<ul>
<li>The predicted target data of shape (n_samples,)</li>
</ul>

<hr />
</div>


                            </div>
                            <div id="LinearSVC.score" class="classattr">
                                        <input id="LinearSVC.score-view-source" class="view-source-toggle-state" type="checkbox" aria-hidden="true" tabindex="-1">
<div class="attr function">
            
        <span class="def">def</span>
        <span class="name">score</span><span class="signature pdoc-code multiline">(<span class="param">	<span class="bp">self</span>,</span><span class="param">	<span class="n">X</span><span class="p">:</span> <span class="n">numpy</span><span class="o">.</span><span class="n">ndarray</span>,</span><span class="param">	<span class="n">Y</span><span class="p">:</span> <span class="n">numpy</span><span class="o">.</span><span class="n">ndarray</span>,</span><span class="param">	<span class="n">W</span><span class="p">:</span> <span class="n">numpy</span><span class="o">.</span><span class="n">ndarray</span> <span class="o">=</span> <span class="kc">None</span>,</span><span class="param">	<span class="n">b</span><span class="p">:</span> <span class="n">numpy</span><span class="o">.</span><span class="n">float64</span> <span class="o">=</span> <span class="kc">None</span></span><span class="return-annotation">) -> <span class="n">numpy</span><span class="o">.</span><span class="n">float64</span>:</span></span>

                <label class="view-source-button" for="LinearSVC.score-view-source"><span>View Source</span></label>

    </div>
    <a class="headerlink" href="#LinearSVC.score"></a>
            <div class="pdoc-code codehilite"><pre><span></span><span id="LinearSVC.score-457"><a href="#LinearSVC.score-457"><span class="linenos">457</span></a>    <span class="k">def</span> <span class="nf">score</span><span class="p">(</span>
</span><span id="LinearSVC.score-458"><a href="#LinearSVC.score-458"><span class="linenos">458</span></a>        <span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">,</span> <span class="n">Y</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">,</span> <span class="n">W</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span> <span class="n">b</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">float64</span> <span class="o">=</span> <span class="kc">None</span>
</span><span id="LinearSVC.score-459"><a href="#LinearSVC.score-459"><span class="linenos">459</span></a>    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">np</span><span class="o">.</span><span class="n">float64</span><span class="p">:</span>
</span><span id="LinearSVC.score-460"><a href="#LinearSVC.score-460"><span class="linenos">460</span></a><span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
</span><span id="LinearSVC.score-461"><a href="#LinearSVC.score-461"><span class="linenos">461</span></a><span class="sd">        ### Return the cost of the model given X, Y, W, and b.</span>
</span><span id="LinearSVC.score-462"><a href="#LinearSVC.score-462"><span class="linenos">462</span></a>
</span><span id="LinearSVC.score-463"><a href="#LinearSVC.score-463"><span class="linenos">463</span></a><span class="sd">        Parameters</span>
</span><span id="LinearSVC.score-464"><a href="#LinearSVC.score-464"><span class="linenos">464</span></a><span class="sd">        ----------</span>
</span><span id="LinearSVC.score-465"><a href="#LinearSVC.score-465"><span class="linenos">465</span></a>
</span><span id="LinearSVC.score-466"><a href="#LinearSVC.score-466"><span class="linenos">466</span></a><span class="sd">        `X` : np.ndarray</span>
</span><span id="LinearSVC.score-467"><a href="#LinearSVC.score-467"><span class="linenos">467</span></a><span class="sd">        - The input array of shape (n_samples, n_features)</span>
</span><span id="LinearSVC.score-468"><a href="#LinearSVC.score-468"><span class="linenos">468</span></a>
</span><span id="LinearSVC.score-469"><a href="#LinearSVC.score-469"><span class="linenos">469</span></a><span class="sd">        `Y` : np.ndarray</span>
</span><span id="LinearSVC.score-470"><a href="#LinearSVC.score-470"><span class="linenos">470</span></a><span class="sd">        - The output array of shape (n_samples,)</span>
</span><span id="LinearSVC.score-471"><a href="#LinearSVC.score-471"><span class="linenos">471</span></a>
</span><span id="LinearSVC.score-472"><a href="#LinearSVC.score-472"><span class="linenos">472</span></a><span class="sd">        `W` : np.ndarray, optional</span>
</span><span id="LinearSVC.score-473"><a href="#LinearSVC.score-473"><span class="linenos">473</span></a><span class="sd">        - If None, then the weight array will be cosidered as the trained</span>
</span><span id="LinearSVC.score-474"><a href="#LinearSVC.score-474"><span class="linenos">474</span></a><span class="sd">            weight array</span>
</span><span id="LinearSVC.score-475"><a href="#LinearSVC.score-475"><span class="linenos">475</span></a>
</span><span id="LinearSVC.score-476"><a href="#LinearSVC.score-476"><span class="linenos">476</span></a><span class="sd">        `b` : np.float64, optional</span>
</span><span id="LinearSVC.score-477"><a href="#LinearSVC.score-477"><span class="linenos">477</span></a><span class="sd">        - The intercept, by default None</span>
</span><span id="LinearSVC.score-478"><a href="#LinearSVC.score-478"><span class="linenos">478</span></a><span class="sd">        - If None, then the intercept will be cosidered as the trained</span>
</span><span id="LinearSVC.score-479"><a href="#LinearSVC.score-479"><span class="linenos">479</span></a><span class="sd">            intercept</span>
</span><span id="LinearSVC.score-480"><a href="#LinearSVC.score-480"><span class="linenos">480</span></a>
</span><span id="LinearSVC.score-481"><a href="#LinearSVC.score-481"><span class="linenos">481</span></a>
</span><span id="LinearSVC.score-482"><a href="#LinearSVC.score-482"><span class="linenos">482</span></a><span class="sd">        Returns</span>
</span><span id="LinearSVC.score-483"><a href="#LinearSVC.score-483"><span class="linenos">483</span></a><span class="sd">        -------</span>
</span><span id="LinearSVC.score-484"><a href="#LinearSVC.score-484"><span class="linenos">484</span></a>
</span><span id="LinearSVC.score-485"><a href="#LinearSVC.score-485"><span class="linenos">485</span></a><span class="sd">        `np.float64`</span>
</span><span id="LinearSVC.score-486"><a href="#LinearSVC.score-486"><span class="linenos">486</span></a><span class="sd">        - The cost of the model</span>
</span><span id="LinearSVC.score-487"><a href="#LinearSVC.score-487"><span class="linenos">487</span></a>
</span><span id="LinearSVC.score-488"><a href="#LinearSVC.score-488"><span class="linenos">488</span></a><span class="sd">        ---</span>
</span><span id="LinearSVC.score-489"><a href="#LinearSVC.score-489"><span class="linenos">489</span></a><span class="sd">        &quot;&quot;&quot;</span>
</span><span id="LinearSVC.score-490"><a href="#LinearSVC.score-490"><span class="linenos">490</span></a>        <span class="n">X</span><span class="p">,</span> <span class="n">Y</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_validate_data</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">Y</span><span class="p">)</span>
</span><span id="LinearSVC.score-491"><a href="#LinearSVC.score-491"><span class="linenos">491</span></a>
</span><span id="LinearSVC.score-492"><a href="#LinearSVC.score-492"><span class="linenos">492</span></a>        <span class="n">W</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_weights</span> <span class="k">if</span> <span class="n">W</span> <span class="ow">is</span> <span class="kc">None</span> <span class="k">else</span> <span class="n">W</span>
</span><span id="LinearSVC.score-493"><a href="#LinearSVC.score-493"><span class="linenos">493</span></a>        <span class="n">b</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_intercept</span> <span class="k">if</span> <span class="n">b</span> <span class="ow">is</span> <span class="kc">None</span> <span class="k">else</span> <span class="n">b</span>
</span><span id="LinearSVC.score-494"><a href="#LinearSVC.score-494"><span class="linenos">494</span></a>
</span><span id="LinearSVC.score-495"><a href="#LinearSVC.score-495"><span class="linenos">495</span></a>        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_cost</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">Y</span><span class="p">,</span> <span class="n">W</span><span class="p">,</span> <span class="n">b</span><span class="p">)</span>
</span></pre></div>


            <div class="docstring"><h3 id="return-the-cost-of-the-model-given-x-y-w-and-b">Return the cost of the model given X, Y, W, and b.</h3>

<h2 id="parameters">Parameters</h2>

<p><code>X</code> : np.ndarray</p>

<ul>
<li>The input array of shape (n_samples, n_features)</li>
</ul>

<p><code>Y</code> : np.ndarray</p>

<ul>
<li>The output array of shape (n_samples,)</li>
</ul>

<p><code>W</code> : np.ndarray, optional</p>

<ul>
<li>If None, then the weight array will be cosidered as the trained
weight array</li>
</ul>

<p><code>b</code> : np.float64, optional</p>

<ul>
<li>The intercept, by default None</li>
<li>If None, then the intercept will be cosidered as the trained
intercept</li>
</ul>

<h2 id="returns">Returns</h2>

<p><code>np.float64</code></p>

<ul>
<li>The cost of the model</li>
</ul>

<hr />
</div>


                            </div>
                            <div class="inherited">
                                <h5>Inherited Members</h5>
                                <dl>
                                    <div><dt>learnML.interfaces.iregression.IRegression</dt>
                                <dd id="LinearSVC.get_cost_history" class="function">get_cost_history</dd>
                <dd id="LinearSVC.get_parameter_history" class="function">get_parameter_history</dd>
                <dd id="LinearSVC.get_weights" class="function">get_weights</dd>
                <dd id="LinearSVC.get_intercept" class="function">get_intercept</dd>

            </div>
                                </dl>
                            </div>
                </section>
    </main>
<script>
    function escapeHTML(html) {
        return document.createElement('div').appendChild(document.createTextNode(html)).parentNode.innerHTML;
    }

    const originalContent = document.querySelector("main.pdoc");
    let currentContent = originalContent;

    function setContent(innerHTML) {
        let elem;
        if (innerHTML) {
            elem = document.createElement("main");
            elem.classList.add("pdoc");
            elem.innerHTML = innerHTML;
        } else {
            elem = originalContent;
        }
        if (currentContent !== elem) {
            currentContent.replaceWith(elem);
            currentContent = elem;
        }
    }

    function getSearchTerm() {
        return (new URL(window.location)).searchParams.get("search");
    }

    const searchBox = document.querySelector(".pdoc input[type=search]");
    searchBox.addEventListener("input", function () {
        let url = new URL(window.location);
        if (searchBox.value.trim()) {
            url.hash = "";
            url.searchParams.set("search", searchBox.value);
        } else {
            url.searchParams.delete("search");
        }
        history.replaceState("", "", url.toString());
        onInput();
    });
    window.addEventListener("popstate", onInput);


    let search, searchErr;

    async function initialize() {
        try {
            search = await new Promise((resolve, reject) => {
                const script = document.createElement("script");
                script.type = "text/javascript";
                script.async = true;
                script.onload = () => resolve(window.pdocSearch);
                script.onerror = (e) => reject(e);
                script.src = "../search.js";
                document.getElementsByTagName("head")[0].appendChild(script);
            });
        } catch (e) {
            console.error("Cannot fetch pdoc search index");
            searchErr = "Cannot fetch search index.";
        }
        onInput();

        document.querySelector("nav.pdoc").addEventListener("click", e => {
            if (e.target.hash) {
                searchBox.value = "";
                searchBox.dispatchEvent(new Event("input"));
            }
        });
    }

    function onInput() {
        setContent((() => {
            const term = getSearchTerm();
            if (!term) {
                return null
            }
            if (searchErr) {
                return `<h3>Error: ${searchErr}</h3>`
            }
            if (!search) {
                return "<h3>Searching...</h3>"
            }

            window.scrollTo({top: 0, left: 0, behavior: 'auto'});

            const results = search(term);

            let html;
            if (results.length === 0) {
                html = `No search results for '${escapeHTML(term)}'.`
            } else {
                html = `<h4>${results.length} search result${results.length > 1 ? "s" : ""} for '${escapeHTML(term)}'.</h4>`;
            }
            for (let result of results.slice(0, 10)) {
                let doc = result.doc;
                let url = `../${doc.modulename.replaceAll(".", "/")}.html`;
                if (doc.qualname) {
                    url += `#${doc.qualname}`;
                }

                let heading;
                switch (result.doc.kind) {
                    case "function":
                        if (doc.fullname.endsWith(".__init__")) {
                            heading = `<span class="name">${doc.fullname.replace(/\.__init__$/, "")}</span>${doc.signature}`;
                        } else {
                            heading = `<span class="def">${doc.funcdef}</span> <span class="name">${doc.fullname}</span>${doc.signature}`;
                        }
                        break;
                    case "class":
                        heading = `<span class="def">class</span> <span class="name">${doc.fullname}</span>`;
                        if (doc.bases)
                            heading += `<wbr>(<span class="base">${doc.bases}</span>)`;
                        heading += `:`;
                        break;
                    case "variable":
                        heading = `<span class="name">${doc.fullname}</span>`;
                        if (doc.annotation)
                            heading += `<span class="annotation">${doc.annotation}</span>`;
                        if (doc.default_value)
                            heading += `<span class="default_value"> = ${doc.default_value}</span>`;
                        break;
                    default:
                        heading = `<span class="name">${doc.fullname}</span>`;
                        break;
                }
                html += `
                        <section class="search-result">
                        <a href="${url}" class="attr ${doc.kind}">${heading}</a>
                        <div class="docstring">${doc.doc}</div>
                        </section>
                    `;

            }
            return html;
        })());
    }

    if (getSearchTerm()) {
        initialize();
        searchBox.value = getSearchTerm();
        onInput();
    } else {
        searchBox.addEventListener("focus", initialize, {once: true});
    }

    searchBox.addEventListener("keydown", e => {
        if (["ArrowDown", "ArrowUp", "Enter"].includes(e.key)) {
            let focused = currentContent.querySelector(".search-result.focused");
            if (!focused) {
                currentContent.querySelector(".search-result").classList.add("focused");
            } else if (
                e.key === "ArrowDown"
                && focused.nextElementSibling
                && focused.nextElementSibling.classList.contains("search-result")
            ) {
                focused.classList.remove("focused");
                focused.nextElementSibling.classList.add("focused");
                focused.nextElementSibling.scrollIntoView({
                    behavior: "smooth",
                    block: "nearest",
                    inline: "nearest"
                });
            } else if (
                e.key === "ArrowUp"
                && focused.previousElementSibling
                && focused.previousElementSibling.classList.contains("search-result")
            ) {
                focused.classList.remove("focused");
                focused.previousElementSibling.classList.add("focused");
                focused.previousElementSibling.scrollIntoView({
                    behavior: "smooth",
                    block: "nearest",
                    inline: "nearest"
                });
            } else if (
                e.key === "Enter"
            ) {
                focused.querySelector("a").click();
            }
        }
    });
</script></body>
</html>